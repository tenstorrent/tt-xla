#loc1 = loc("p0.3")
#loc2 = loc("p1.10")
#loc3 = loc("p2.18")
#loc4 = loc("p3.22")
#loc5 = loc("p4.26")
#loc6 = loc("p5.30")
#loc7 = loc("p6.34")
#loc8 = loc("p7.42")
#loc9 = loc("p8.46")
#loc10 = loc("p9.50")
#loc11 = loc("p10.54")
#loc12 = loc("p11.58")
#loc13 = loc("p12.64")
#loc14 = loc("p13.68")
#loc15 = loc("p14.72")
#loc16 = loc("p15.76")
#loc17 = loc("p16.80")
#loc18 = loc("p17.85")
#loc19 = loc("p18.89")
#loc20 = loc("p19.93")
#loc21 = loc("p20.97")
#loc22 = loc("p21.101")
#loc23 = loc("p22.103")
#loc24 = loc("p23.107")
#loc25 = loc("p24.111")
#loc26 = loc("p25.115")
#loc27 = loc("p26.119")
#loc28 = loc("p27.121")
#loc29 = loc("p28.310")
#loc30 = loc("p29.314")
#loc31 = loc("p30.318")
#loc32 = loc("p31.322")
#loc33 = loc("p32.326")
#loc34 = loc("p33.328")
#loc35 = loc("p34.332")
#loc36 = loc("p35.336")
#loc37 = loc("p36.340")
#loc38 = loc("p37.344")
#loc39 = loc("p38.346")
#loc40 = loc("p39.350")
#loc41 = loc("p40.354")
#loc42 = loc("p41.358")
#loc43 = loc("p42.362")
#loc44 = loc("p43.394")
#loc45 = loc("p44.398")
#loc46 = loc("p45.402")
#loc47 = loc("p46.406")
#loc48 = loc("p47.410")
#loc49 = loc("p48.412")
#loc50 = loc("p49.416")
#loc51 = loc("p50.420")
#loc52 = loc("p51.424")
#loc53 = loc("p52.428")
#loc54 = loc("p53.430")
#loc55 = loc("p54.434")
#loc56 = loc("p55.438")
#loc57 = loc("p56.442")
#loc58 = loc("p57.446")
#loc59 = loc("p58.478")
#loc60 = loc("p59.482")
#loc61 = loc("p60.486")
#loc62 = loc("p61.490")
#loc63 = loc("p62.494")
#loc64 = loc("p63.496")
#loc65 = loc("p64.500")
#loc66 = loc("p65.504")
#loc67 = loc("p66.508")
#loc68 = loc("p67.512")
#loc69 = loc("p68.514")
#loc70 = loc("p69.518")
#loc71 = loc("p70.522")
#loc72 = loc("p71.526")
#loc73 = loc("p72.530")
#loc74 = loc("p73.568")
#loc75 = loc("p74.572")
#loc76 = loc("p75.576")
#loc77 = loc("p76.580")
#loc78 = loc("p77.584")
#loc79 = loc("p78.586")
#loc80 = loc("p79.590")
#loc81 = loc("p80.594")
#loc82 = loc("p81.598")
#loc83 = loc("p82.602")
#loc84 = loc("p83.604")
#loc85 = loc("p84.608")
#loc86 = loc("p85.612")
#loc87 = loc("p86.616")
#loc88 = loc("p87.620")
#loc89 = loc("p88.652")
#loc90 = loc("p89.656")
#loc91 = loc("p90.660")
#loc92 = loc("p91.664")
#loc93 = loc("p92.668")
#loc94 = loc("p93.670")
#loc95 = loc("p94.674")
#loc96 = loc("p95.678")
#loc97 = loc("p96.682")
#loc98 = loc("p97.686")
#loc99 = loc("p98.688")
#loc100 = loc("p99.692")
#loc101 = loc("p100.696")
#loc102 = loc("p101.700")
#loc103 = loc("p102.704")
#loc104 = loc("p103.736")
#loc105 = loc("p104.740")
#loc106 = loc("p105.744")
#loc107 = loc("p106.748")
#loc108 = loc("p107.752")
#loc109 = loc("p108.754")
#loc110 = loc("p109.758")
#loc111 = loc("p110.762")
#loc112 = loc("p111.766")
#loc113 = loc("p112.770")
#loc114 = loc("p113.772")
#loc115 = loc("p114.776")
#loc116 = loc("p115.780")
#loc117 = loc("p116.784")
#loc118 = loc("p117.788")
#loc119 = loc("p118.820")
#loc120 = loc("p119.824")
#loc121 = loc("p120.828")
#loc122 = loc("p121.832")
#loc123 = loc("p122.836")
#loc124 = loc("p123.838")
#loc125 = loc("p124.842")
#loc126 = loc("p125.846")
#loc127 = loc("p126.850")
#loc128 = loc("p127.854")
#loc129 = loc("p128.856")
#loc130 = loc("p129.860")
#loc131 = loc("p130.864")
#loc132 = loc("p131.868")
#loc133 = loc("p132.872")
#loc134 = loc("p133.910")
#loc135 = loc("p134.914")
#loc136 = loc("p135.918")
#loc137 = loc("p136.922")
#loc138 = loc("p137.926")
#loc139 = loc("p138.928")
#loc140 = loc("p139.932")
#loc141 = loc("p140.936")
#loc142 = loc("p141.940")
#loc143 = loc("p142.944")
#loc144 = loc("p143.946")
#loc145 = loc("p144.950")
#loc146 = loc("p145.954")
#loc147 = loc("p146.958")
#loc148 = loc("p147.962")
#loc149 = loc("p148.994")
#loc150 = loc("p149.998")
#loc151 = loc("p150.1002")
#loc152 = loc("p151.1006")
#loc153 = loc("p152.1010")
#loc154 = loc("p153.1012")
#loc155 = loc("p154.1016")
#loc156 = loc("p155.1020")
#loc157 = loc("p156.1024")
#loc158 = loc("p157.1028")
#loc159 = loc("p158.1030")
#loc160 = loc("p159.1034")
#loc161 = loc("p160.1038")
#loc162 = loc("p161.1042")
#loc163 = loc("p162.1046")
#loc164 = loc("p163.1078")
#loc165 = loc("p164.1082")
#loc166 = loc("p165.1086")
#loc167 = loc("p166.1090")
#loc168 = loc("p167.1094")
#loc169 = loc("p168.1096")
#loc170 = loc("p169.1100")
#loc171 = loc("p170.1104")
#loc172 = loc("p171.1108")
#loc173 = loc("p172.1112")
#loc174 = loc("p173.1114")
#loc175 = loc("p174.1118")
#loc176 = loc("p175.1122")
#loc177 = loc("p176.1126")
#loc178 = loc("p177.1130")
#loc179 = loc("p178.1162")
#loc180 = loc("p179.1166")
#loc181 = loc("p180.1170")
#loc182 = loc("p181.1174")
#loc183 = loc("p182.1178")
#loc184 = loc("p183.1180")
#loc185 = loc("p184.1184")
#loc186 = loc("p185.1188")
#loc187 = loc("p186.1192")
#loc188 = loc("p187.1196")
#loc189 = loc("p188.1198")
#loc190 = loc("p189.1202")
#loc191 = loc("p190.1206")
#loc192 = loc("p191.1210")
#loc193 = loc("p192.1214")
#loc194 = loc("p193.1246")
#loc195 = loc("p194.1250")
#loc196 = loc("p195.1254")
#loc197 = loc("p196.1258")
#loc198 = loc("p197.1262")
#loc199 = loc("p198.1264")
#loc200 = loc("p199.1268")
#loc201 = loc("p200.1272")
#loc202 = loc("p201.1276")
#loc203 = loc("p202.1280")
#loc204 = loc("p203.1282")
#loc205 = loc("p204.1286")
#loc206 = loc("p205.1290")
#loc207 = loc("p206.1294")
#loc208 = loc("p207.1298")
#loc209 = loc("p208.1330")
#loc210 = loc("p209.1334")
#loc211 = loc("p210.1338")
#loc212 = loc("p211.1342")
#loc213 = loc("p212.1346")
#loc214 = loc("p213.1348")
#loc215 = loc("p214.1352")
#loc216 = loc("p215.1356")
#loc217 = loc("p216.1360")
#loc218 = loc("p217.1364")
#loc219 = loc("p218.1366")
#loc220 = loc("p219.1370")
#loc221 = loc("p220.1374")
#loc222 = loc("p221.1378")
#loc223 = loc("p222.1382")
#loc224 = loc("p223.1420")
#loc225 = loc("p224.1424")
#loc226 = loc("p225.1428")
#loc227 = loc("p226.1432")
#loc228 = loc("p227.1436")
#loc229 = loc("p228.1438")
#loc230 = loc("p229.1442")
#loc231 = loc("p230.1446")
#loc232 = loc("p231.1450")
#loc233 = loc("p232.1454")
#loc234 = loc("p233.1456")
#loc235 = loc("p234.1460")
#loc236 = loc("p235.1464")
#loc237 = loc("p236.1468")
#loc238 = loc("p237.1472")
#loc239 = loc("p238.1504")
#loc240 = loc("p239.1508")
#loc241 = loc("p240.1512")
#loc242 = loc("p241.1516")
#loc243 = loc("p242.1520")
#loc244 = loc("p243.1522")
#loc245 = loc("p244.1526")
#loc246 = loc("p245.1530")
#loc247 = loc("p246.1534")
#loc248 = loc("p247.1538")
#loc249 = loc("p248.1540")
#loc250 = loc("p249.1544")
#loc251 = loc("p250.1548")
#loc252 = loc("p251.1552")
#loc253 = loc("p252.1556")
#loc254 = loc("p253.1588")
#loc255 = loc("p254.1592")
#loc256 = loc("p255.1596")
#loc257 = loc("p256.1600")
#loc258 = loc("p257.1604")
#loc259 = loc("p258.1606")
#loc260 = loc("p259.1610")
#loc261 = loc("p260.1614")
#loc262 = loc("p261.1618")
#loc263 = loc("p262.1622")
#loc264 = loc("p263.1624")
#loc265 = loc("p264.1628")
#loc266 = loc("p265.1632")
#loc267 = loc("p266.1636")
#loc268 = loc("p267.1640")
module @SyncTensorsGraph.1698 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.1698 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
      func.func @main(%arg0: tensor<1000xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___fc_bias"} loc("p0.3"), %arg1: tensor<1000x2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___fc_weight"} loc("p1.10"), %arg2: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___downsample_1_running_var"} loc("p2.18"), %arg3: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___downsample_1_running_mean"} loc("p3.22"), %arg4: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___downsample_1_bias"} loc("p4.26"), %arg5: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___downsample_1_weight"} loc("p5.30"), %arg6: tensor<2048x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___downsample_0_weight"} loc("p6.34"), %arg7: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___downsample_1_running_var"} loc("p7.42"), %arg8: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___downsample_1_running_mean"} loc("p8.46"), %arg9: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___downsample_1_bias"} loc("p9.50"), %arg10: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___downsample_1_weight"} loc("p10.54"), %arg11: tensor<1024x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___downsample_0_weight"} loc("p11.58"), %arg12: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___downsample_1_running_var"} loc("p12.64"), %arg13: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___downsample_1_running_mean"} loc("p13.68"), %arg14: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___downsample_1_bias"} loc("p14.72"), %arg15: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___downsample_1_weight"} loc("p15.76"), %arg16: tensor<512x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___downsample_0_weight"} loc("p16.80"), %arg17: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___downsample_1_running_var"} loc("p17.85"), %arg18: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___downsample_1_running_mean"} loc("p18.89"), %arg19: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___downsample_1_bias"} loc("p19.93"), %arg20: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___downsample_1_weight"} loc("p20.97"), %arg21: tensor<256x64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___downsample_0_weight"} loc("p21.101"), %arg22: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___bn1_running_var"} loc("p22.103"), %arg23: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___bn1_running_mean"} loc("p23.107"), %arg24: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___bn1_bias"} loc("p24.111"), %arg25: tensor<64xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___bn1_weight"} loc("p25.115"), %arg26: tensor<64x3x7x7xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "l__self___conv1_weight"} loc("p26.119"), %arg27: tensor<1x3x224x224xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "args_0"} loc("p27.121"), %arg28: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn3_running_var"} loc("p28.310"), %arg29: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn3_running_mean"} loc("p29.314"), %arg30: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn3_bias"} loc("p30.318"), %arg31: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn3_weight"} loc("p31.322"), %arg32: tensor<256x128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___conv3_weight"} loc("p32.326"), %arg33: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn2_running_var"} loc("p33.328"), %arg34: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn2_running_mean"} loc("p34.332"), %arg35: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn2_bias"} loc("p35.336"), %arg36: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn2_weight"} loc("p36.340"), %arg37: tensor<128x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___conv2_weight"} loc("p37.344"), %arg38: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn1_running_var"} loc("p38.346"), %arg39: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn1_running_mean"} loc("p39.350"), %arg40: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn1_bias"} loc("p40.354"), %arg41: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___bn1_weight"} loc("p41.358"), %arg42: tensor<128x64x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___0___conv1_weight"} loc("p42.362"), %arg43: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn3_running_var"} loc("p43.394"), %arg44: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn3_running_mean"} loc("p44.398"), %arg45: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn3_bias"} loc("p45.402"), %arg46: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn3_weight"} loc("p46.406"), %arg47: tensor<256x128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___conv3_weight"} loc("p47.410"), %arg48: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn2_running_var"} loc("p48.412"), %arg49: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn2_running_mean"} loc("p49.416"), %arg50: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn2_bias"} loc("p50.420"), %arg51: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn2_weight"} loc("p51.424"), %arg52: tensor<128x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___conv2_weight"} loc("p52.428"), %arg53: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn1_running_var"} loc("p53.430"), %arg54: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn1_running_mean"} loc("p54.434"), %arg55: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn1_bias"} loc("p55.438"), %arg56: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___bn1_weight"} loc("p56.442"), %arg57: tensor<128x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___1___conv1_weight"} loc("p57.446"), %arg58: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn3_running_var"} loc("p58.478"), %arg59: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn3_running_mean"} loc("p59.482"), %arg60: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn3_bias"} loc("p60.486"), %arg61: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn3_weight"} loc("p61.490"), %arg62: tensor<256x128x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___conv3_weight"} loc("p62.494"), %arg63: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn2_running_var"} loc("p63.496"), %arg64: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn2_running_mean"} loc("p64.500"), %arg65: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn2_bias"} loc("p65.504"), %arg66: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn2_weight"} loc("p66.508"), %arg67: tensor<128x128x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___conv2_weight"} loc("p67.512"), %arg68: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn1_running_var"} loc("p68.514"), %arg69: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn1_running_mean"} loc("p69.518"), %arg70: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn1_bias"} loc("p70.522"), %arg71: tensor<128xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___bn1_weight"} loc("p71.526"), %arg72: tensor<128x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer1___2___conv1_weight"} loc("p72.530"), %arg73: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn3_running_var"} loc("p73.568"), %arg74: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn3_running_mean"} loc("p74.572"), %arg75: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn3_bias"} loc("p75.576"), %arg76: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn3_weight"} loc("p76.580"), %arg77: tensor<512x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___conv3_weight"} loc("p77.584"), %arg78: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn2_running_var"} loc("p78.586"), %arg79: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn2_running_mean"} loc("p79.590"), %arg80: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn2_bias"} loc("p80.594"), %arg81: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn2_weight"} loc("p81.598"), %arg82: tensor<256x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___conv2_weight"} loc("p82.602"), %arg83: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn1_running_var"} loc("p83.604"), %arg84: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn1_running_mean"} loc("p84.608"), %arg85: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn1_bias"} loc("p85.612"), %arg86: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___bn1_weight"} loc("p86.616"), %arg87: tensor<256x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___0___conv1_weight"} loc("p87.620"), %arg88: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn3_running_var"} loc("p88.652"), %arg89: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn3_running_mean"} loc("p89.656"), %arg90: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn3_bias"} loc("p90.660"), %arg91: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn3_weight"} loc("p91.664"), %arg92: tensor<512x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___conv3_weight"} loc("p92.668"), %arg93: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn2_running_var"} loc("p93.670"), %arg94: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn2_running_mean"} loc("p94.674"), %arg95: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn2_bias"} loc("p95.678"), %arg96: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn2_weight"} loc("p96.682"), %arg97: tensor<256x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___conv2_weight"} loc("p97.686"), %arg98: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn1_running_var"} loc("p98.688"), %arg99: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn1_running_mean"} loc("p99.692"), %arg100: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn1_bias"} loc("p100.696"), %arg101: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___bn1_weight"} loc("p101.700"), %arg102: tensor<256x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___1___conv1_weight"} loc("p102.704"), %arg103: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn3_running_var"} loc("p103.736"), %arg104: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn3_running_mean"} loc("p104.740"), %arg105: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn3_bias"} loc("p105.744"), %arg106: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn3_weight"} loc("p106.748"), %arg107: tensor<512x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___conv3_weight"} loc("p107.752"), %arg108: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn2_running_var"} loc("p108.754"), %arg109: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn2_running_mean"} loc("p109.758"), %arg110: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn2_bias"} loc("p110.762"), %arg111: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn2_weight"} loc("p111.766"), %arg112: tensor<256x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___conv2_weight"} loc("p112.770"), %arg113: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn1_running_var"} loc("p113.772"), %arg114: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn1_running_mean"} loc("p114.776"), %arg115: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn1_bias"} loc("p115.780"), %arg116: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___bn1_weight"} loc("p116.784"), %arg117: tensor<256x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___2___conv1_weight"} loc("p117.788"), %arg118: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn3_running_var"} loc("p118.820"), %arg119: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn3_running_mean"} loc("p119.824"), %arg120: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn3_bias"} loc("p120.828"), %arg121: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn3_weight"} loc("p121.832"), %arg122: tensor<512x256x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___conv3_weight"} loc("p122.836"), %arg123: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn2_running_var"} loc("p123.838"), %arg124: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn2_running_mean"} loc("p124.842"), %arg125: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn2_bias"} loc("p125.846"), %arg126: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn2_weight"} loc("p126.850"), %arg127: tensor<256x256x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___conv2_weight"} loc("p127.854"), %arg128: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn1_running_var"} loc("p128.856"), %arg129: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn1_running_mean"} loc("p129.860"), %arg130: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn1_bias"} loc("p130.864"), %arg131: tensor<256xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___bn1_weight"} loc("p131.868"), %arg132: tensor<256x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer2___3___conv1_weight"} loc("p132.872"), %arg133: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn3_running_var"} loc("p133.910"), %arg134: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn3_running_mean"} loc("p134.914"), %arg135: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn3_bias"} loc("p135.918"), %arg136: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn3_weight"} loc("p136.922"), %arg137: tensor<1024x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___conv3_weight"} loc("p137.926"), %arg138: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn2_running_var"} loc("p138.928"), %arg139: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn2_running_mean"} loc("p139.932"), %arg140: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn2_bias"} loc("p140.936"), %arg141: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn2_weight"} loc("p141.940"), %arg142: tensor<512x512x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___conv2_weight"} loc("p142.944"), %arg143: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn1_running_var"} loc("p143.946"), %arg144: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn1_running_mean"} loc("p144.950"), %arg145: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn1_bias"} loc("p145.954"), %arg146: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___bn1_weight"} loc("p146.958"), %arg147: tensor<512x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___0___conv1_weight"} loc("p147.962"), %arg148: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn3_running_var"} loc("p148.994"), %arg149: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn3_running_mean"} loc("p149.998"), %arg150: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn3_bias"} loc("p150.1002"), %arg151: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn3_weight"} loc("p151.1006"), %arg152: tensor<1024x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___conv3_weight"} loc("p152.1010"), %arg153: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn2_running_var"} loc("p153.1012"), %arg154: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn2_running_mean"} loc("p154.1016"), %arg155: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn2_bias"} loc("p155.1020"), %arg156: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn2_weight"} loc("p156.1024"), %arg157: tensor<512x512x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___conv2_weight"} loc("p157.1028"), %arg158: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn1_running_var"} loc("p158.1030"), %arg159: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn1_running_mean"} loc("p159.1034"), %arg160: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn1_bias"} loc("p160.1038"), %arg161: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___bn1_weight"} loc("p161.1042"), %arg162: tensor<512x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___1___conv1_weight"} loc("p162.1046"), %arg163: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn3_running_var"} loc("p163.1078"), %arg164: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn3_running_mean"} loc("p164.1082"), %arg165: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn3_bias"} loc("p165.1086"), %arg166: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn3_weight"} loc("p166.1090"), %arg167: tensor<1024x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___conv3_weight"} loc("p167.1094"), %arg168: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn2_running_var"} loc("p168.1096"), %arg169: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn2_running_mean"} loc("p169.1100"), %arg170: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn2_bias"} loc("p170.1104"), %arg171: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn2_weight"} loc("p171.1108"), %arg172: tensor<512x512x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___conv2_weight"} loc("p172.1112"), %arg173: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn1_running_var"} loc("p173.1114"), %arg174: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn1_running_mean"} loc("p174.1118"), %arg175: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn1_bias"} loc("p175.1122"), %arg176: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___bn1_weight"} loc("p176.1126"), %arg177: tensor<512x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___2___conv1_weight"} loc("p177.1130"), %arg178: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn3_running_var"} loc("p178.1162"), %arg179: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn3_running_mean"} loc("p179.1166"), %arg180: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn3_bias"} loc("p180.1170"), %arg181: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn3_weight"} loc("p181.1174"), %arg182: tensor<1024x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___conv3_weight"} loc("p182.1178"), %arg183: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn2_running_var"} loc("p183.1180"), %arg184: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn2_running_mean"} loc("p184.1184"), %arg185: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn2_bias"} loc("p185.1188"), %arg186: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn2_weight"} loc("p186.1192"), %arg187: tensor<512x512x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___conv2_weight"} loc("p187.1196"), %arg188: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn1_running_var"} loc("p188.1198"), %arg189: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn1_running_mean"} loc("p189.1202"), %arg190: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn1_bias"} loc("p190.1206"), %arg191: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___bn1_weight"} loc("p191.1210"), %arg192: tensor<512x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___3___conv1_weight"} loc("p192.1214"), %arg193: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn3_running_var"} loc("p193.1246"), %arg194: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn3_running_mean"} loc("p194.1250"), %arg195: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn3_bias"} loc("p195.1254"), %arg196: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn3_weight"} loc("p196.1258"), %arg197: tensor<1024x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___conv3_weight"} loc("p197.1262"), %arg198: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn2_running_var"} loc("p198.1264"), %arg199: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn2_running_mean"} loc("p199.1268"), %arg200: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn2_bias"} loc("p200.1272"), %arg201: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn2_weight"} loc("p201.1276"), %arg202: tensor<512x512x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___conv2_weight"} loc("p202.1280"), %arg203: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn1_running_var"} loc("p203.1282"), %arg204: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn1_running_mean"} loc("p204.1286"), %arg205: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn1_bias"} loc("p205.1290"), %arg206: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___bn1_weight"} loc("p206.1294"), %arg207: tensor<512x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___4___conv1_weight"} loc("p207.1298"), %arg208: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn3_running_var"} loc("p208.1330"), %arg209: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn3_running_mean"} loc("p209.1334"), %arg210: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn3_bias"} loc("p210.1338"), %arg211: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn3_weight"} loc("p211.1342"), %arg212: tensor<1024x512x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___conv3_weight"} loc("p212.1346"), %arg213: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn2_running_var"} loc("p213.1348"), %arg214: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn2_running_mean"} loc("p214.1352"), %arg215: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn2_bias"} loc("p215.1356"), %arg216: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn2_weight"} loc("p216.1360"), %arg217: tensor<512x512x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___conv2_weight"} loc("p217.1364"), %arg218: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn1_running_var"} loc("p218.1366"), %arg219: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn1_running_mean"} loc("p219.1370"), %arg220: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn1_bias"} loc("p220.1374"), %arg221: tensor<512xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___bn1_weight"} loc("p221.1378"), %arg222: tensor<512x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer3___5___conv1_weight"} loc("p222.1382"), %arg223: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn3_running_var"} loc("p223.1420"), %arg224: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn3_running_mean"} loc("p224.1424"), %arg225: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn3_bias"} loc("p225.1428"), %arg226: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn3_weight"} loc("p226.1432"), %arg227: tensor<2048x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___conv3_weight"} loc("p227.1436"), %arg228: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn2_running_var"} loc("p228.1438"), %arg229: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn2_running_mean"} loc("p229.1442"), %arg230: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn2_bias"} loc("p230.1446"), %arg231: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn2_weight"} loc("p231.1450"), %arg232: tensor<1024x1024x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___conv2_weight"} loc("p232.1454"), %arg233: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn1_running_var"} loc("p233.1456"), %arg234: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn1_running_mean"} loc("p234.1460"), %arg235: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn1_bias"} loc("p235.1464"), %arg236: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___bn1_weight"} loc("p236.1468"), %arg237: tensor<1024x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___0___conv1_weight"} loc("p237.1472"), %arg238: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn3_running_var"} loc("p238.1504"), %arg239: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn3_running_mean"} loc("p239.1508"), %arg240: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn3_bias"} loc("p240.1512"), %arg241: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn3_weight"} loc("p241.1516"), %arg242: tensor<2048x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___conv3_weight"} loc("p242.1520"), %arg243: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn2_running_var"} loc("p243.1522"), %arg244: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn2_running_mean"} loc("p244.1526"), %arg245: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn2_bias"} loc("p245.1530"), %arg246: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn2_weight"} loc("p246.1534"), %arg247: tensor<1024x1024x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___conv2_weight"} loc("p247.1538"), %arg248: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn1_running_var"} loc("p248.1540"), %arg249: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn1_running_mean"} loc("p249.1544"), %arg250: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn1_bias"} loc("p250.1548"), %arg251: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___bn1_weight"} loc("p251.1552"), %arg252: tensor<1024x2048x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___1___conv1_weight"} loc("p252.1556"), %arg253: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn3_running_var"} loc("p253.1588"), %arg254: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn3_running_mean"} loc("p254.1592"), %arg255: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn3_bias"} loc("p255.1596"), %arg256: tensor<2048xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn3_weight"} loc("p256.1600"), %arg257: tensor<2048x1024x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___conv3_weight"} loc("p257.1604"), %arg258: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn2_running_var"} loc("p258.1606"), %arg259: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn2_running_mean"} loc("p259.1610"), %arg260: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn2_bias"} loc("p260.1614"), %arg261: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn2_weight"} loc("p261.1618"), %arg262: tensor<1024x1024x3x3xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___conv2_weight"} loc("p262.1622"), %arg263: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn1_running_var"} loc("p263.1624"), %arg264: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn1_running_mean"} loc("p264.1628"), %arg265: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn1_bias"} loc("p265.1632"), %arg266: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___bn1_weight"} loc("p266.1636"), %arg267: tensor<1024x2048x1x1xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<unsharded>, ttir.name = "getattr_l__self___layer4___2___conv1_weight"} loc("p267.1640")) -> (tensor<1x1000xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = "ttir.constant"() <{value = dense<2.038570e-02> : tensor<1x2048xbf16>}> : () -> tensor<1x2048xbf16> loc(#loc)
        %1 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x2048x7x7xbf16>}> : () -> tensor<1x2048x7x7xbf16> loc(#loc)
        %2 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x1024x7x7xbf16>}> : () -> tensor<1x1024x7x7xbf16> loc(#loc)
        %3 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x1024x14x14xbf16>}> : () -> tensor<1x1024x14x14xbf16> loc(#loc)
        %4 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x512x14x14xbf16>}> : () -> tensor<1x512x14x14xbf16> loc(#loc)
        %5 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x512x28x28xbf16>}> : () -> tensor<1x512x28x28xbf16> loc(#loc)
        %6 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x256x28x28xbf16>}> : () -> tensor<1x256x28x28xbf16> loc(#loc)
        %7 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x256x56x56xbf16>}> : () -> tensor<1x256x56x56xbf16> loc(#loc)
        %8 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x128x56x56xbf16>}> : () -> tensor<1x128x56x56xbf16> loc(#loc)
        %9 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<1x64x112x112xbf16>}> : () -> tensor<1x64x112x112xbf16> loc(#loc)
        %10 = "ttir.constant"() <{value = dense<0xFF80> : tensor<bf16>}> : () -> tensor<bf16> loc(#loc)
        %11 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<bf16>}> : () -> tensor<bf16> loc(#loc)
        %12 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc269)
        %13 = "ttir.reshape"(%arg22, %12) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc269)
        %14 = ttir.empty() : tensor<64xbf16> loc(#loc270)
        %15 = "ttir.reshape"(%13, %14) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc270)
        %16 = ttir.empty() : tensor<1x64x112x112xbf16> loc(#loc271)
        %17 = "ttir.convolution"(%arg27, %arg26, %16) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 3, 3, 3, 3>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 2, 2>}> : (tensor<1x3x224x224xbf16>, tensor<64x3x7x7xbf16>, tensor<1x64x112x112xbf16>) -> tensor<1x64x112x112xbf16> loc(#loc271)
        %18 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc272)
        %19 = "ttir.reshape"(%arg25, %18) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc272)
        %20 = ttir.empty() : tensor<64xbf16> loc(#loc273)
        %21 = "ttir.reshape"(%19, %20) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc273)
        %22 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc274)
        %23 = "ttir.reshape"(%arg24, %22) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc274)
        %24 = ttir.empty() : tensor<64xbf16> loc(#loc275)
        %25 = "ttir.reshape"(%23, %24) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc275)
        %26 = ttir.empty() : tensor<1x1x64xbf16> loc(#loc276)
        %27 = "ttir.reshape"(%arg23, %26) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<64xbf16>, tensor<1x1x64xbf16>) -> tensor<1x1x64xbf16> loc(#loc276)
        %28 = ttir.empty() : tensor<64xbf16> loc(#loc277)
        %29 = "ttir.reshape"(%27, %28) <{shape = [64 : i32]}> : (tensor<1x1x64xbf16>, tensor<64xbf16>) -> tensor<64xbf16> loc(#loc277)
        %30 = ttir.empty() : tensor<1x64x112x112xbf16> loc(#loc278)
        %31 = "ttir.batch_norm_inference"(%17, %21, %25, %29, %15, %30) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x64x112x112xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<64xbf16>, tensor<1x64x112x112xbf16>) -> tensor<1x64x112x112xbf16> loc(#loc278)
        %32 = ttir.empty() : tensor<1x64x112x112xbf16> loc(#loc279)
        %33 = "ttir.maximum"(%31, %9, %32) : (tensor<1x64x112x112xbf16>, tensor<1x64x112x112xbf16>, tensor<1x64x112x112xbf16>) -> tensor<1x64x112x112xbf16> loc(#loc279)
        %34 = ttir.empty() : tensor<1x64x114x114xbf16> loc(#loc280)
        %35 = "ttir.pad"(%33, %34) <{padding = array<i32: 0, 0, 0, 0, 1, 1, 1, 1>, value = 0xFF800000 : f32}> : (tensor<1x64x112x112xbf16>, tensor<1x64x114x114xbf16>) -> tensor<1x64x114x114xbf16> loc(#loc280)
        %36 = ttir.empty() : tensor<1x64x56x56xbf16> loc(#loc281)
        %37 = "ttir.pooling"(%35, %36) <{base_dilations = array<i64: 1, 1, 1, 1>, operandSegmentSizes = array<i32: 1, 1>, padding = array<i64: 0, 0, 0, 0, 0, 0, 0, 0>, pooling_method = #ttir<pooling_method Max>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 1, 3, 3>, window_strides = array<i64: 1, 1, 2, 2>}> : (tensor<1x64x114x114xbf16>, tensor<1x64x56x56xbf16>) -> tensor<1x64x56x56xbf16> loc(#loc281)
        %38 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc282)
        %39 = "ttir.reshape"(%arg17, %38) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc282)
        %40 = ttir.empty() : tensor<256xbf16> loc(#loc283)
        %41 = "ttir.reshape"(%39, %40) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc283)
        %42 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc284)
        %43 = "ttir.reshape"(%arg38, %42) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc284)
        %44 = ttir.empty() : tensor<128xbf16> loc(#loc285)
        %45 = "ttir.reshape"(%43, %44) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc285)
        %46 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc286)
        %47 = "ttir.reshape"(%arg33, %46) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc286)
        %48 = ttir.empty() : tensor<128xbf16> loc(#loc287)
        %49 = "ttir.reshape"(%47, %48) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc287)
        %50 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc288)
        %51 = "ttir.reshape"(%arg28, %50) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc288)
        %52 = ttir.empty() : tensor<256xbf16> loc(#loc289)
        %53 = "ttir.reshape"(%51, %52) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc289)
        %54 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc290)
        %55 = "ttir.reshape"(%arg53, %54) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc290)
        %56 = ttir.empty() : tensor<128xbf16> loc(#loc291)
        %57 = "ttir.reshape"(%55, %56) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc291)
        %58 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc292)
        %59 = "ttir.reshape"(%arg48, %58) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc292)
        %60 = ttir.empty() : tensor<128xbf16> loc(#loc293)
        %61 = "ttir.reshape"(%59, %60) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc293)
        %62 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc294)
        %63 = "ttir.reshape"(%arg43, %62) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc294)
        %64 = ttir.empty() : tensor<256xbf16> loc(#loc295)
        %65 = "ttir.reshape"(%63, %64) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc295)
        %66 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc296)
        %67 = "ttir.reshape"(%arg68, %66) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc296)
        %68 = ttir.empty() : tensor<128xbf16> loc(#loc297)
        %69 = "ttir.reshape"(%67, %68) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc297)
        %70 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc298)
        %71 = "ttir.reshape"(%arg63, %70) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc298)
        %72 = ttir.empty() : tensor<128xbf16> loc(#loc299)
        %73 = "ttir.reshape"(%71, %72) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc299)
        %74 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc300)
        %75 = "ttir.reshape"(%arg58, %74) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc300)
        %76 = ttir.empty() : tensor<256xbf16> loc(#loc301)
        %77 = "ttir.reshape"(%75, %76) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc301)
        %78 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc302)
        %79 = "ttir.reshape"(%arg12, %78) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc302)
        %80 = ttir.empty() : tensor<512xbf16> loc(#loc303)
        %81 = "ttir.reshape"(%79, %80) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc303)
        %82 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc304)
        %83 = "ttir.reshape"(%arg83, %82) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc304)
        %84 = ttir.empty() : tensor<256xbf16> loc(#loc305)
        %85 = "ttir.reshape"(%83, %84) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc305)
        %86 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc306)
        %87 = "ttir.reshape"(%arg78, %86) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc306)
        %88 = ttir.empty() : tensor<256xbf16> loc(#loc307)
        %89 = "ttir.reshape"(%87, %88) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc307)
        %90 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc308)
        %91 = "ttir.reshape"(%arg73, %90) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc308)
        %92 = ttir.empty() : tensor<512xbf16> loc(#loc309)
        %93 = "ttir.reshape"(%91, %92) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc309)
        %94 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc310)
        %95 = "ttir.reshape"(%arg98, %94) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc310)
        %96 = ttir.empty() : tensor<256xbf16> loc(#loc311)
        %97 = "ttir.reshape"(%95, %96) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc311)
        %98 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc312)
        %99 = "ttir.reshape"(%arg93, %98) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc312)
        %100 = ttir.empty() : tensor<256xbf16> loc(#loc313)
        %101 = "ttir.reshape"(%99, %100) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc313)
        %102 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc314)
        %103 = "ttir.reshape"(%arg88, %102) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc314)
        %104 = ttir.empty() : tensor<512xbf16> loc(#loc315)
        %105 = "ttir.reshape"(%103, %104) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc315)
        %106 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc316)
        %107 = "ttir.reshape"(%arg113, %106) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc316)
        %108 = ttir.empty() : tensor<256xbf16> loc(#loc317)
        %109 = "ttir.reshape"(%107, %108) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc317)
        %110 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc318)
        %111 = "ttir.reshape"(%arg108, %110) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc318)
        %112 = ttir.empty() : tensor<256xbf16> loc(#loc319)
        %113 = "ttir.reshape"(%111, %112) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc319)
        %114 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc320)
        %115 = "ttir.reshape"(%arg103, %114) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc320)
        %116 = ttir.empty() : tensor<512xbf16> loc(#loc321)
        %117 = "ttir.reshape"(%115, %116) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc321)
        %118 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc322)
        %119 = "ttir.reshape"(%arg128, %118) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc322)
        %120 = ttir.empty() : tensor<256xbf16> loc(#loc323)
        %121 = "ttir.reshape"(%119, %120) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc323)
        %122 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc324)
        %123 = "ttir.reshape"(%arg123, %122) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc324)
        %124 = ttir.empty() : tensor<256xbf16> loc(#loc325)
        %125 = "ttir.reshape"(%123, %124) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc325)
        %126 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc326)
        %127 = "ttir.reshape"(%arg118, %126) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc326)
        %128 = ttir.empty() : tensor<512xbf16> loc(#loc327)
        %129 = "ttir.reshape"(%127, %128) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc327)
        %130 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc328)
        %131 = "ttir.reshape"(%arg7, %130) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc328)
        %132 = ttir.empty() : tensor<1024xbf16> loc(#loc329)
        %133 = "ttir.reshape"(%131, %132) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc329)
        %134 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc330)
        %135 = "ttir.reshape"(%arg143, %134) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc330)
        %136 = ttir.empty() : tensor<512xbf16> loc(#loc331)
        %137 = "ttir.reshape"(%135, %136) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc331)
        %138 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc332)
        %139 = "ttir.reshape"(%arg138, %138) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc332)
        %140 = ttir.empty() : tensor<512xbf16> loc(#loc333)
        %141 = "ttir.reshape"(%139, %140) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc333)
        %142 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc334)
        %143 = "ttir.reshape"(%arg133, %142) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc334)
        %144 = ttir.empty() : tensor<1024xbf16> loc(#loc335)
        %145 = "ttir.reshape"(%143, %144) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc335)
        %146 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc336)
        %147 = "ttir.reshape"(%arg158, %146) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc336)
        %148 = ttir.empty() : tensor<512xbf16> loc(#loc337)
        %149 = "ttir.reshape"(%147, %148) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc337)
        %150 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc338)
        %151 = "ttir.reshape"(%arg153, %150) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc338)
        %152 = ttir.empty() : tensor<512xbf16> loc(#loc339)
        %153 = "ttir.reshape"(%151, %152) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc339)
        %154 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc340)
        %155 = "ttir.reshape"(%arg148, %154) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc340)
        %156 = ttir.empty() : tensor<1024xbf16> loc(#loc341)
        %157 = "ttir.reshape"(%155, %156) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc341)
        %158 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc342)
        %159 = "ttir.reshape"(%arg173, %158) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc342)
        %160 = ttir.empty() : tensor<512xbf16> loc(#loc343)
        %161 = "ttir.reshape"(%159, %160) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc343)
        %162 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc344)
        %163 = "ttir.reshape"(%arg168, %162) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc344)
        %164 = ttir.empty() : tensor<512xbf16> loc(#loc345)
        %165 = "ttir.reshape"(%163, %164) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc345)
        %166 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc346)
        %167 = "ttir.reshape"(%arg163, %166) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc346)
        %168 = ttir.empty() : tensor<1024xbf16> loc(#loc347)
        %169 = "ttir.reshape"(%167, %168) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc347)
        %170 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc348)
        %171 = "ttir.reshape"(%arg188, %170) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc348)
        %172 = ttir.empty() : tensor<512xbf16> loc(#loc349)
        %173 = "ttir.reshape"(%171, %172) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc349)
        %174 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc350)
        %175 = "ttir.reshape"(%arg183, %174) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc350)
        %176 = ttir.empty() : tensor<512xbf16> loc(#loc351)
        %177 = "ttir.reshape"(%175, %176) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc351)
        %178 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc352)
        %179 = "ttir.reshape"(%arg178, %178) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc352)
        %180 = ttir.empty() : tensor<1024xbf16> loc(#loc353)
        %181 = "ttir.reshape"(%179, %180) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc353)
        %182 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc354)
        %183 = "ttir.reshape"(%arg203, %182) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc354)
        %184 = ttir.empty() : tensor<512xbf16> loc(#loc355)
        %185 = "ttir.reshape"(%183, %184) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc355)
        %186 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc356)
        %187 = "ttir.reshape"(%arg198, %186) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc356)
        %188 = ttir.empty() : tensor<512xbf16> loc(#loc357)
        %189 = "ttir.reshape"(%187, %188) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc357)
        %190 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc358)
        %191 = "ttir.reshape"(%arg193, %190) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc358)
        %192 = ttir.empty() : tensor<1024xbf16> loc(#loc359)
        %193 = "ttir.reshape"(%191, %192) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc359)
        %194 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc360)
        %195 = "ttir.reshape"(%arg218, %194) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc360)
        %196 = ttir.empty() : tensor<512xbf16> loc(#loc361)
        %197 = "ttir.reshape"(%195, %196) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc361)
        %198 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc362)
        %199 = "ttir.reshape"(%arg213, %198) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc362)
        %200 = ttir.empty() : tensor<512xbf16> loc(#loc363)
        %201 = "ttir.reshape"(%199, %200) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc363)
        %202 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc364)
        %203 = "ttir.reshape"(%arg208, %202) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc364)
        %204 = ttir.empty() : tensor<1024xbf16> loc(#loc365)
        %205 = "ttir.reshape"(%203, %204) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc365)
        %206 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc366)
        %207 = "ttir.reshape"(%arg2, %206) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc366)
        %208 = ttir.empty() : tensor<2048xbf16> loc(#loc367)
        %209 = "ttir.reshape"(%207, %208) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc367)
        %210 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc368)
        %211 = "ttir.reshape"(%arg233, %210) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc368)
        %212 = ttir.empty() : tensor<1024xbf16> loc(#loc369)
        %213 = "ttir.reshape"(%211, %212) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc369)
        %214 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc370)
        %215 = "ttir.reshape"(%arg228, %214) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc370)
        %216 = ttir.empty() : tensor<1024xbf16> loc(#loc371)
        %217 = "ttir.reshape"(%215, %216) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc371)
        %218 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc372)
        %219 = "ttir.reshape"(%arg223, %218) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc372)
        %220 = ttir.empty() : tensor<2048xbf16> loc(#loc373)
        %221 = "ttir.reshape"(%219, %220) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc373)
        %222 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc374)
        %223 = "ttir.reshape"(%arg248, %222) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc374)
        %224 = ttir.empty() : tensor<1024xbf16> loc(#loc375)
        %225 = "ttir.reshape"(%223, %224) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc375)
        %226 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc376)
        %227 = "ttir.reshape"(%arg243, %226) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc376)
        %228 = ttir.empty() : tensor<1024xbf16> loc(#loc377)
        %229 = "ttir.reshape"(%227, %228) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc377)
        %230 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc378)
        %231 = "ttir.reshape"(%arg238, %230) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc378)
        %232 = ttir.empty() : tensor<2048xbf16> loc(#loc379)
        %233 = "ttir.reshape"(%231, %232) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc379)
        %234 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc380)
        %235 = "ttir.reshape"(%arg263, %234) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc380)
        %236 = ttir.empty() : tensor<1024xbf16> loc(#loc381)
        %237 = "ttir.reshape"(%235, %236) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc381)
        %238 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc382)
        %239 = "ttir.reshape"(%arg258, %238) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc382)
        %240 = ttir.empty() : tensor<1024xbf16> loc(#loc383)
        %241 = "ttir.reshape"(%239, %240) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc383)
        %242 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc384)
        %243 = "ttir.reshape"(%arg253, %242) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc384)
        %244 = ttir.empty() : tensor<2048xbf16> loc(#loc385)
        %245 = "ttir.reshape"(%243, %244) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc385)
        %246 = ttir.empty() : tensor<1x128x56x56xbf16> loc(#loc386)
        %247 = "ttir.convolution"(%37, %arg42, %246) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x64x56x56xbf16>, tensor<128x64x1x1xbf16>, tensor<1x128x56x56xbf16>) -> tensor<1x128x56x56xbf16> loc(#loc386)
        %248 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc387)
        %249 = "ttir.reshape"(%arg41, %248) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc387)
        %250 = ttir.empty() : tensor<128xbf16> loc(#loc388)
        %251 = "ttir.reshape"(%249, %250) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc388)
        %252 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc389)
        %253 = "ttir.reshape"(%arg40, %252) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc389)
        %254 = ttir.empty() : tensor<128xbf16> loc(#loc390)
        %255 = "ttir.reshape"(%253, %254) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc390)
        %256 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc391)
        %257 = "ttir.reshape"(%arg39, %256) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc391)
        %258 = ttir.empty() : tensor<128xbf16> loc(#loc392)
        %259 = "ttir.reshape"(%257, %258) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc392)
        %260 = ttir.empty() : tensor<1x128x56x56xbf16> loc(#loc393)
        %261 = "ttir.batch_norm_inference"(%247, %251, %255, %259, %45, %260) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x128x56x56xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<1x128x56x56xbf16>) -> tensor<1x128x56x56xbf16> loc(#loc393)
        %262 = ttir.empty() : tensor<1x128x56x56xbf16> loc(#loc394)
        %263 = "ttir.maximum"(%261, %8, %262) : (tensor<1x128x56x56xbf16>, tensor<1x128x56x56xbf16>, tensor<1x128x56x56xbf16>) -> tensor<1x128x56x56xbf16> loc(#loc394)
        %264 = ttir.empty() : tensor<1x128x56x56xbf16> loc(#loc395)
        %265 = "ttir.convolution"(%263, %arg37, %264) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x128x56x56xbf16>, tensor<128x128x3x3xbf16>, tensor<1x128x56x56xbf16>) -> tensor<1x128x56x56xbf16> loc(#loc395)
        %266 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc396)
        %267 = "ttir.reshape"(%arg36, %266) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc396)
        %268 = ttir.empty() : tensor<128xbf16> loc(#loc397)
        %269 = "ttir.reshape"(%267, %268) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc397)
        %270 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc398)
        %271 = "ttir.reshape"(%arg35, %270) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc398)
        %272 = ttir.empty() : tensor<128xbf16> loc(#loc399)
        %273 = "ttir.reshape"(%271, %272) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc399)
        %274 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc400)
        %275 = "ttir.reshape"(%arg34, %274) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc400)
        %276 = ttir.empty() : tensor<128xbf16> loc(#loc401)
        %277 = "ttir.reshape"(%275, %276) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc401)
        %278 = ttir.empty() : tensor<1x128x56x56xbf16> loc(#loc402)
        %279 = "ttir.batch_norm_inference"(%265, %269, %273, %277, %49, %278) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x128x56x56xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<1x128x56x56xbf16>) -> tensor<1x128x56x56xbf16> loc(#loc402)
        %280 = ttir.empty() : tensor<1x128x56x56xbf16> loc(#loc403)
        %281 = "ttir.maximum"(%279, %8, %280) : (tensor<1x128x56x56xbf16>, tensor<1x128x56x56xbf16>, tensor<1x128x56x56xbf16>) -> tensor<1x128x56x56xbf16> loc(#loc403)
        %282 = ttir.empty() : tensor<1x256x56x56xbf16> loc(#loc404)
        %283 = "ttir.convolution"(%281, %arg32, %282) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x128x56x56xbf16>, tensor<256x128x1x1xbf16>, tensor<1x256x56x56xbf16>) -> tensor<1x256x56x56xbf16> loc(#loc404)
        %284 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc405)
        %285 = "ttir.reshape"(%arg31, %284) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc405)
        %286 = ttir.empty() : tensor<256xbf16> loc(#loc406)
        %287 = "ttir.reshape"(%285, %286) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc406)
        %288 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc407)
        %289 = "ttir.reshape"(%arg30, %288) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc407)
        %290 = ttir.empty() : tensor<256xbf16> loc(#loc408)
        %291 = "ttir.reshape"(%289, %290) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc408)
        %292 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc409)
        %293 = "ttir.reshape"(%arg29, %292) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc409)
        %294 = ttir.empty() : tensor<256xbf16> loc(#loc410)
        %295 = "ttir.reshape"(%293, %294) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc410)
        %296 = ttir.empty() : tensor<1x256x56x56xbf16> loc(#loc411)
        %297 = "ttir.batch_norm_inference"(%283, %287, %291, %295, %53, %296) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x256x56x56xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<1x256x56x56xbf16>) -> tensor<1x256x56x56xbf16> loc(#loc411)
        %298 = ttir.empty() : tensor<1x256x56x56xbf16> loc(#loc412)
        %299 = "ttir.convolution"(%37, %arg21, %298) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x64x56x56xbf16>, tensor<256x64x1x1xbf16>, tensor<1x256x56x56xbf16>) -> tensor<1x256x56x56xbf16> loc(#loc412)
        %300 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc413)
        %301 = "ttir.reshape"(%arg20, %300) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc413)
        %302 = ttir.empty() : tensor<256xbf16> loc(#loc414)
        %303 = "ttir.reshape"(%301, %302) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc414)
        %304 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc415)
        %305 = "ttir.reshape"(%arg19, %304) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc415)
        %306 = ttir.empty() : tensor<256xbf16> loc(#loc416)
        %307 = "ttir.reshape"(%305, %306) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc416)
        %308 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc417)
        %309 = "ttir.reshape"(%arg18, %308) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc417)
        %310 = ttir.empty() : tensor<256xbf16> loc(#loc418)
        %311 = "ttir.reshape"(%309, %310) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc418)
        %312 = ttir.empty() : tensor<1x256x56x56xbf16> loc(#loc419)
        %313 = "ttir.batch_norm_inference"(%299, %303, %307, %311, %41, %312) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x256x56x56xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<1x256x56x56xbf16>) -> tensor<1x256x56x56xbf16> loc(#loc419)
        %314 = ttir.empty() : tensor<1x256x56x56xbf16> loc(#loc420)
        %315 = "ttir.add"(%297, %313, %314) : (tensor<1x256x56x56xbf16>, tensor<1x256x56x56xbf16>, tensor<1x256x56x56xbf16>) -> tensor<1x256x56x56xbf16> loc(#loc420)
        %316 = ttir.empty() : tensor<1x256x56x56xbf16> loc(#loc421)
        %317 = "ttir.maximum"(%315, %7, %316) : (tensor<1x256x56x56xbf16>, tensor<1x256x56x56xbf16>, tensor<1x256x56x56xbf16>) -> tensor<1x256x56x56xbf16> loc(#loc421)
        %318 = ttir.empty() : tensor<1x128x56x56xbf16> loc(#loc422)
        %319 = "ttir.convolution"(%317, %arg57, %318) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x256x56x56xbf16>, tensor<128x256x1x1xbf16>, tensor<1x128x56x56xbf16>) -> tensor<1x128x56x56xbf16> loc(#loc422)
        %320 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc423)
        %321 = "ttir.reshape"(%arg56, %320) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc423)
        %322 = ttir.empty() : tensor<128xbf16> loc(#loc424)
        %323 = "ttir.reshape"(%321, %322) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc424)
        %324 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc425)
        %325 = "ttir.reshape"(%arg55, %324) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc425)
        %326 = ttir.empty() : tensor<128xbf16> loc(#loc426)
        %327 = "ttir.reshape"(%325, %326) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc426)
        %328 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc427)
        %329 = "ttir.reshape"(%arg54, %328) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc427)
        %330 = ttir.empty() : tensor<128xbf16> loc(#loc428)
        %331 = "ttir.reshape"(%329, %330) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc428)
        %332 = ttir.empty() : tensor<1x128x56x56xbf16> loc(#loc429)
        %333 = "ttir.batch_norm_inference"(%319, %323, %327, %331, %57, %332) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x128x56x56xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<1x128x56x56xbf16>) -> tensor<1x128x56x56xbf16> loc(#loc429)
        %334 = ttir.empty() : tensor<1x128x56x56xbf16> loc(#loc430)
        %335 = "ttir.maximum"(%333, %8, %334) : (tensor<1x128x56x56xbf16>, tensor<1x128x56x56xbf16>, tensor<1x128x56x56xbf16>) -> tensor<1x128x56x56xbf16> loc(#loc430)
        %336 = ttir.empty() : tensor<1x128x56x56xbf16> loc(#loc431)
        %337 = "ttir.convolution"(%335, %arg52, %336) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x128x56x56xbf16>, tensor<128x128x3x3xbf16>, tensor<1x128x56x56xbf16>) -> tensor<1x128x56x56xbf16> loc(#loc431)
        %338 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc432)
        %339 = "ttir.reshape"(%arg51, %338) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc432)
        %340 = ttir.empty() : tensor<128xbf16> loc(#loc433)
        %341 = "ttir.reshape"(%339, %340) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc433)
        %342 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc434)
        %343 = "ttir.reshape"(%arg50, %342) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc434)
        %344 = ttir.empty() : tensor<128xbf16> loc(#loc435)
        %345 = "ttir.reshape"(%343, %344) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc435)
        %346 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc436)
        %347 = "ttir.reshape"(%arg49, %346) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc436)
        %348 = ttir.empty() : tensor<128xbf16> loc(#loc437)
        %349 = "ttir.reshape"(%347, %348) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc437)
        %350 = ttir.empty() : tensor<1x128x56x56xbf16> loc(#loc438)
        %351 = "ttir.batch_norm_inference"(%337, %341, %345, %349, %61, %350) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x128x56x56xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<1x128x56x56xbf16>) -> tensor<1x128x56x56xbf16> loc(#loc438)
        %352 = ttir.empty() : tensor<1x128x56x56xbf16> loc(#loc439)
        %353 = "ttir.maximum"(%351, %8, %352) : (tensor<1x128x56x56xbf16>, tensor<1x128x56x56xbf16>, tensor<1x128x56x56xbf16>) -> tensor<1x128x56x56xbf16> loc(#loc439)
        %354 = ttir.empty() : tensor<1x256x56x56xbf16> loc(#loc440)
        %355 = "ttir.convolution"(%353, %arg47, %354) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x128x56x56xbf16>, tensor<256x128x1x1xbf16>, tensor<1x256x56x56xbf16>) -> tensor<1x256x56x56xbf16> loc(#loc440)
        %356 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc441)
        %357 = "ttir.reshape"(%arg46, %356) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc441)
        %358 = ttir.empty() : tensor<256xbf16> loc(#loc442)
        %359 = "ttir.reshape"(%357, %358) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc442)
        %360 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc443)
        %361 = "ttir.reshape"(%arg45, %360) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc443)
        %362 = ttir.empty() : tensor<256xbf16> loc(#loc444)
        %363 = "ttir.reshape"(%361, %362) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc444)
        %364 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc445)
        %365 = "ttir.reshape"(%arg44, %364) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc445)
        %366 = ttir.empty() : tensor<256xbf16> loc(#loc446)
        %367 = "ttir.reshape"(%365, %366) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc446)
        %368 = ttir.empty() : tensor<1x256x56x56xbf16> loc(#loc447)
        %369 = "ttir.batch_norm_inference"(%355, %359, %363, %367, %65, %368) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x256x56x56xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<1x256x56x56xbf16>) -> tensor<1x256x56x56xbf16> loc(#loc447)
        %370 = ttir.empty() : tensor<1x256x56x56xbf16> loc(#loc448)
        %371 = "ttir.add"(%369, %317, %370) : (tensor<1x256x56x56xbf16>, tensor<1x256x56x56xbf16>, tensor<1x256x56x56xbf16>) -> tensor<1x256x56x56xbf16> loc(#loc448)
        %372 = ttir.empty() : tensor<1x256x56x56xbf16> loc(#loc449)
        %373 = "ttir.maximum"(%371, %7, %372) : (tensor<1x256x56x56xbf16>, tensor<1x256x56x56xbf16>, tensor<1x256x56x56xbf16>) -> tensor<1x256x56x56xbf16> loc(#loc449)
        %374 = ttir.empty() : tensor<1x128x56x56xbf16> loc(#loc450)
        %375 = "ttir.convolution"(%373, %arg72, %374) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x256x56x56xbf16>, tensor<128x256x1x1xbf16>, tensor<1x128x56x56xbf16>) -> tensor<1x128x56x56xbf16> loc(#loc450)
        %376 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc451)
        %377 = "ttir.reshape"(%arg71, %376) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc451)
        %378 = ttir.empty() : tensor<128xbf16> loc(#loc452)
        %379 = "ttir.reshape"(%377, %378) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc452)
        %380 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc453)
        %381 = "ttir.reshape"(%arg70, %380) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc453)
        %382 = ttir.empty() : tensor<128xbf16> loc(#loc454)
        %383 = "ttir.reshape"(%381, %382) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc454)
        %384 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc455)
        %385 = "ttir.reshape"(%arg69, %384) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc455)
        %386 = ttir.empty() : tensor<128xbf16> loc(#loc456)
        %387 = "ttir.reshape"(%385, %386) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc456)
        %388 = ttir.empty() : tensor<1x128x56x56xbf16> loc(#loc457)
        %389 = "ttir.batch_norm_inference"(%375, %379, %383, %387, %69, %388) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x128x56x56xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<1x128x56x56xbf16>) -> tensor<1x128x56x56xbf16> loc(#loc457)
        %390 = ttir.empty() : tensor<1x128x56x56xbf16> loc(#loc458)
        %391 = "ttir.maximum"(%389, %8, %390) : (tensor<1x128x56x56xbf16>, tensor<1x128x56x56xbf16>, tensor<1x128x56x56xbf16>) -> tensor<1x128x56x56xbf16> loc(#loc458)
        %392 = ttir.empty() : tensor<1x128x56x56xbf16> loc(#loc459)
        %393 = "ttir.convolution"(%391, %arg67, %392) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x128x56x56xbf16>, tensor<128x128x3x3xbf16>, tensor<1x128x56x56xbf16>) -> tensor<1x128x56x56xbf16> loc(#loc459)
        %394 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc460)
        %395 = "ttir.reshape"(%arg66, %394) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc460)
        %396 = ttir.empty() : tensor<128xbf16> loc(#loc461)
        %397 = "ttir.reshape"(%395, %396) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc461)
        %398 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc462)
        %399 = "ttir.reshape"(%arg65, %398) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc462)
        %400 = ttir.empty() : tensor<128xbf16> loc(#loc463)
        %401 = "ttir.reshape"(%399, %400) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc463)
        %402 = ttir.empty() : tensor<1x1x128xbf16> loc(#loc464)
        %403 = "ttir.reshape"(%arg64, %402) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16> loc(#loc464)
        %404 = ttir.empty() : tensor<128xbf16> loc(#loc465)
        %405 = "ttir.reshape"(%403, %404) <{shape = [128 : i32]}> : (tensor<1x1x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16> loc(#loc465)
        %406 = ttir.empty() : tensor<1x128x56x56xbf16> loc(#loc466)
        %407 = "ttir.batch_norm_inference"(%393, %397, %401, %405, %73, %406) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x128x56x56xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>, tensor<1x128x56x56xbf16>) -> tensor<1x128x56x56xbf16> loc(#loc466)
        %408 = ttir.empty() : tensor<1x128x56x56xbf16> loc(#loc467)
        %409 = "ttir.maximum"(%407, %8, %408) : (tensor<1x128x56x56xbf16>, tensor<1x128x56x56xbf16>, tensor<1x128x56x56xbf16>) -> tensor<1x128x56x56xbf16> loc(#loc467)
        %410 = ttir.empty() : tensor<1x256x56x56xbf16> loc(#loc468)
        %411 = "ttir.convolution"(%409, %arg62, %410) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x128x56x56xbf16>, tensor<256x128x1x1xbf16>, tensor<1x256x56x56xbf16>) -> tensor<1x256x56x56xbf16> loc(#loc468)
        %412 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc469)
        %413 = "ttir.reshape"(%arg61, %412) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc469)
        %414 = ttir.empty() : tensor<256xbf16> loc(#loc470)
        %415 = "ttir.reshape"(%413, %414) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc470)
        %416 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc471)
        %417 = "ttir.reshape"(%arg60, %416) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc471)
        %418 = ttir.empty() : tensor<256xbf16> loc(#loc472)
        %419 = "ttir.reshape"(%417, %418) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc472)
        %420 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc473)
        %421 = "ttir.reshape"(%arg59, %420) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc473)
        %422 = ttir.empty() : tensor<256xbf16> loc(#loc474)
        %423 = "ttir.reshape"(%421, %422) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc474)
        %424 = ttir.empty() : tensor<1x256x56x56xbf16> loc(#loc475)
        %425 = "ttir.batch_norm_inference"(%411, %415, %419, %423, %77, %424) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x256x56x56xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<1x256x56x56xbf16>) -> tensor<1x256x56x56xbf16> loc(#loc475)
        %426 = ttir.empty() : tensor<1x256x56x56xbf16> loc(#loc476)
        %427 = "ttir.add"(%425, %373, %426) : (tensor<1x256x56x56xbf16>, tensor<1x256x56x56xbf16>, tensor<1x256x56x56xbf16>) -> tensor<1x256x56x56xbf16> loc(#loc476)
        %428 = ttir.empty() : tensor<1x256x56x56xbf16> loc(#loc477)
        %429 = "ttir.maximum"(%427, %7, %428) : (tensor<1x256x56x56xbf16>, tensor<1x256x56x56xbf16>, tensor<1x256x56x56xbf16>) -> tensor<1x256x56x56xbf16> loc(#loc477)
        %430 = ttir.empty() : tensor<1x256x56x56xbf16> loc(#loc478)
        %431 = "ttir.convolution"(%429, %arg87, %430) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x256x56x56xbf16>, tensor<256x256x1x1xbf16>, tensor<1x256x56x56xbf16>) -> tensor<1x256x56x56xbf16> loc(#loc478)
        %432 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc479)
        %433 = "ttir.reshape"(%arg86, %432) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc479)
        %434 = ttir.empty() : tensor<256xbf16> loc(#loc480)
        %435 = "ttir.reshape"(%433, %434) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc480)
        %436 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc481)
        %437 = "ttir.reshape"(%arg85, %436) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc481)
        %438 = ttir.empty() : tensor<256xbf16> loc(#loc482)
        %439 = "ttir.reshape"(%437, %438) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc482)
        %440 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc483)
        %441 = "ttir.reshape"(%arg84, %440) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc483)
        %442 = ttir.empty() : tensor<256xbf16> loc(#loc484)
        %443 = "ttir.reshape"(%441, %442) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc484)
        %444 = ttir.empty() : tensor<1x256x56x56xbf16> loc(#loc485)
        %445 = "ttir.batch_norm_inference"(%431, %435, %439, %443, %85, %444) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x256x56x56xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<1x256x56x56xbf16>) -> tensor<1x256x56x56xbf16> loc(#loc485)
        %446 = ttir.empty() : tensor<1x256x56x56xbf16> loc(#loc486)
        %447 = "ttir.maximum"(%445, %7, %446) : (tensor<1x256x56x56xbf16>, tensor<1x256x56x56xbf16>, tensor<1x256x56x56xbf16>) -> tensor<1x256x56x56xbf16> loc(#loc486)
        %448 = ttir.empty() : tensor<1x256x28x28xbf16> loc(#loc487)
        %449 = "ttir.convolution"(%447, %arg82, %448) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 2, 2>}> : (tensor<1x256x56x56xbf16>, tensor<256x256x3x3xbf16>, tensor<1x256x28x28xbf16>) -> tensor<1x256x28x28xbf16> loc(#loc487)
        %450 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc488)
        %451 = "ttir.reshape"(%arg81, %450) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc488)
        %452 = ttir.empty() : tensor<256xbf16> loc(#loc489)
        %453 = "ttir.reshape"(%451, %452) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc489)
        %454 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc490)
        %455 = "ttir.reshape"(%arg80, %454) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc490)
        %456 = ttir.empty() : tensor<256xbf16> loc(#loc491)
        %457 = "ttir.reshape"(%455, %456) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc491)
        %458 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc492)
        %459 = "ttir.reshape"(%arg79, %458) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc492)
        %460 = ttir.empty() : tensor<256xbf16> loc(#loc493)
        %461 = "ttir.reshape"(%459, %460) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc493)
        %462 = ttir.empty() : tensor<1x256x28x28xbf16> loc(#loc494)
        %463 = "ttir.batch_norm_inference"(%449, %453, %457, %461, %89, %462) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x256x28x28xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<1x256x28x28xbf16>) -> tensor<1x256x28x28xbf16> loc(#loc494)
        %464 = ttir.empty() : tensor<1x256x28x28xbf16> loc(#loc495)
        %465 = "ttir.maximum"(%463, %6, %464) : (tensor<1x256x28x28xbf16>, tensor<1x256x28x28xbf16>, tensor<1x256x28x28xbf16>) -> tensor<1x256x28x28xbf16> loc(#loc495)
        %466 = ttir.empty() : tensor<1x512x28x28xbf16> loc(#loc496)
        %467 = "ttir.convolution"(%465, %arg77, %466) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x256x28x28xbf16>, tensor<512x256x1x1xbf16>, tensor<1x512x28x28xbf16>) -> tensor<1x512x28x28xbf16> loc(#loc496)
        %468 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc497)
        %469 = "ttir.reshape"(%arg76, %468) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc497)
        %470 = ttir.empty() : tensor<512xbf16> loc(#loc498)
        %471 = "ttir.reshape"(%469, %470) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc498)
        %472 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc499)
        %473 = "ttir.reshape"(%arg75, %472) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc499)
        %474 = ttir.empty() : tensor<512xbf16> loc(#loc500)
        %475 = "ttir.reshape"(%473, %474) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc500)
        %476 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc501)
        %477 = "ttir.reshape"(%arg74, %476) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc501)
        %478 = ttir.empty() : tensor<512xbf16> loc(#loc502)
        %479 = "ttir.reshape"(%477, %478) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc502)
        %480 = ttir.empty() : tensor<1x512x28x28xbf16> loc(#loc503)
        %481 = "ttir.batch_norm_inference"(%467, %471, %475, %479, %93, %480) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x512x28x28xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<1x512x28x28xbf16>) -> tensor<1x512x28x28xbf16> loc(#loc503)
        %482 = ttir.empty() : tensor<1x512x28x28xbf16> loc(#loc504)
        %483 = "ttir.convolution"(%429, %arg16, %482) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 2, 2>}> : (tensor<1x256x56x56xbf16>, tensor<512x256x1x1xbf16>, tensor<1x512x28x28xbf16>) -> tensor<1x512x28x28xbf16> loc(#loc504)
        %484 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc505)
        %485 = "ttir.reshape"(%arg15, %484) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc505)
        %486 = ttir.empty() : tensor<512xbf16> loc(#loc506)
        %487 = "ttir.reshape"(%485, %486) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc506)
        %488 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc507)
        %489 = "ttir.reshape"(%arg14, %488) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc507)
        %490 = ttir.empty() : tensor<512xbf16> loc(#loc508)
        %491 = "ttir.reshape"(%489, %490) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc508)
        %492 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc509)
        %493 = "ttir.reshape"(%arg13, %492) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc509)
        %494 = ttir.empty() : tensor<512xbf16> loc(#loc510)
        %495 = "ttir.reshape"(%493, %494) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc510)
        %496 = ttir.empty() : tensor<1x512x28x28xbf16> loc(#loc511)
        %497 = "ttir.batch_norm_inference"(%483, %487, %491, %495, %81, %496) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x512x28x28xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<1x512x28x28xbf16>) -> tensor<1x512x28x28xbf16> loc(#loc511)
        %498 = ttir.empty() : tensor<1x512x28x28xbf16> loc(#loc512)
        %499 = "ttir.add"(%481, %497, %498) : (tensor<1x512x28x28xbf16>, tensor<1x512x28x28xbf16>, tensor<1x512x28x28xbf16>) -> tensor<1x512x28x28xbf16> loc(#loc512)
        %500 = ttir.empty() : tensor<1x512x28x28xbf16> loc(#loc513)
        %501 = "ttir.maximum"(%499, %5, %500) : (tensor<1x512x28x28xbf16>, tensor<1x512x28x28xbf16>, tensor<1x512x28x28xbf16>) -> tensor<1x512x28x28xbf16> loc(#loc513)
        %502 = ttir.empty() : tensor<1x256x28x28xbf16> loc(#loc514)
        %503 = "ttir.convolution"(%501, %arg102, %502) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x512x28x28xbf16>, tensor<256x512x1x1xbf16>, tensor<1x256x28x28xbf16>) -> tensor<1x256x28x28xbf16> loc(#loc514)
        %504 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc515)
        %505 = "ttir.reshape"(%arg101, %504) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc515)
        %506 = ttir.empty() : tensor<256xbf16> loc(#loc516)
        %507 = "ttir.reshape"(%505, %506) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc516)
        %508 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc517)
        %509 = "ttir.reshape"(%arg100, %508) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc517)
        %510 = ttir.empty() : tensor<256xbf16> loc(#loc518)
        %511 = "ttir.reshape"(%509, %510) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc518)
        %512 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc519)
        %513 = "ttir.reshape"(%arg99, %512) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc519)
        %514 = ttir.empty() : tensor<256xbf16> loc(#loc520)
        %515 = "ttir.reshape"(%513, %514) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc520)
        %516 = ttir.empty() : tensor<1x256x28x28xbf16> loc(#loc521)
        %517 = "ttir.batch_norm_inference"(%503, %507, %511, %515, %97, %516) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x256x28x28xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<1x256x28x28xbf16>) -> tensor<1x256x28x28xbf16> loc(#loc521)
        %518 = ttir.empty() : tensor<1x256x28x28xbf16> loc(#loc522)
        %519 = "ttir.maximum"(%517, %6, %518) : (tensor<1x256x28x28xbf16>, tensor<1x256x28x28xbf16>, tensor<1x256x28x28xbf16>) -> tensor<1x256x28x28xbf16> loc(#loc522)
        %520 = ttir.empty() : tensor<1x256x28x28xbf16> loc(#loc523)
        %521 = "ttir.convolution"(%519, %arg97, %520) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x256x28x28xbf16>, tensor<256x256x3x3xbf16>, tensor<1x256x28x28xbf16>) -> tensor<1x256x28x28xbf16> loc(#loc523)
        %522 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc524)
        %523 = "ttir.reshape"(%arg96, %522) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc524)
        %524 = ttir.empty() : tensor<256xbf16> loc(#loc525)
        %525 = "ttir.reshape"(%523, %524) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc525)
        %526 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc526)
        %527 = "ttir.reshape"(%arg95, %526) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc526)
        %528 = ttir.empty() : tensor<256xbf16> loc(#loc527)
        %529 = "ttir.reshape"(%527, %528) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc527)
        %530 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc528)
        %531 = "ttir.reshape"(%arg94, %530) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc528)
        %532 = ttir.empty() : tensor<256xbf16> loc(#loc529)
        %533 = "ttir.reshape"(%531, %532) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc529)
        %534 = ttir.empty() : tensor<1x256x28x28xbf16> loc(#loc530)
        %535 = "ttir.batch_norm_inference"(%521, %525, %529, %533, %101, %534) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x256x28x28xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<1x256x28x28xbf16>) -> tensor<1x256x28x28xbf16> loc(#loc530)
        %536 = ttir.empty() : tensor<1x256x28x28xbf16> loc(#loc531)
        %537 = "ttir.maximum"(%535, %6, %536) : (tensor<1x256x28x28xbf16>, tensor<1x256x28x28xbf16>, tensor<1x256x28x28xbf16>) -> tensor<1x256x28x28xbf16> loc(#loc531)
        %538 = ttir.empty() : tensor<1x512x28x28xbf16> loc(#loc532)
        %539 = "ttir.convolution"(%537, %arg92, %538) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x256x28x28xbf16>, tensor<512x256x1x1xbf16>, tensor<1x512x28x28xbf16>) -> tensor<1x512x28x28xbf16> loc(#loc532)
        %540 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc533)
        %541 = "ttir.reshape"(%arg91, %540) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc533)
        %542 = ttir.empty() : tensor<512xbf16> loc(#loc534)
        %543 = "ttir.reshape"(%541, %542) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc534)
        %544 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc535)
        %545 = "ttir.reshape"(%arg90, %544) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc535)
        %546 = ttir.empty() : tensor<512xbf16> loc(#loc536)
        %547 = "ttir.reshape"(%545, %546) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc536)
        %548 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc537)
        %549 = "ttir.reshape"(%arg89, %548) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc537)
        %550 = ttir.empty() : tensor<512xbf16> loc(#loc538)
        %551 = "ttir.reshape"(%549, %550) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc538)
        %552 = ttir.empty() : tensor<1x512x28x28xbf16> loc(#loc539)
        %553 = "ttir.batch_norm_inference"(%539, %543, %547, %551, %105, %552) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x512x28x28xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<1x512x28x28xbf16>) -> tensor<1x512x28x28xbf16> loc(#loc539)
        %554 = ttir.empty() : tensor<1x512x28x28xbf16> loc(#loc540)
        %555 = "ttir.add"(%553, %501, %554) : (tensor<1x512x28x28xbf16>, tensor<1x512x28x28xbf16>, tensor<1x512x28x28xbf16>) -> tensor<1x512x28x28xbf16> loc(#loc540)
        %556 = ttir.empty() : tensor<1x512x28x28xbf16> loc(#loc541)
        %557 = "ttir.maximum"(%555, %5, %556) : (tensor<1x512x28x28xbf16>, tensor<1x512x28x28xbf16>, tensor<1x512x28x28xbf16>) -> tensor<1x512x28x28xbf16> loc(#loc541)
        %558 = ttir.empty() : tensor<1x256x28x28xbf16> loc(#loc542)
        %559 = "ttir.convolution"(%557, %arg117, %558) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x512x28x28xbf16>, tensor<256x512x1x1xbf16>, tensor<1x256x28x28xbf16>) -> tensor<1x256x28x28xbf16> loc(#loc542)
        %560 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc543)
        %561 = "ttir.reshape"(%arg116, %560) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc543)
        %562 = ttir.empty() : tensor<256xbf16> loc(#loc544)
        %563 = "ttir.reshape"(%561, %562) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc544)
        %564 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc545)
        %565 = "ttir.reshape"(%arg115, %564) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc545)
        %566 = ttir.empty() : tensor<256xbf16> loc(#loc546)
        %567 = "ttir.reshape"(%565, %566) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc546)
        %568 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc547)
        %569 = "ttir.reshape"(%arg114, %568) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc547)
        %570 = ttir.empty() : tensor<256xbf16> loc(#loc548)
        %571 = "ttir.reshape"(%569, %570) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc548)
        %572 = ttir.empty() : tensor<1x256x28x28xbf16> loc(#loc549)
        %573 = "ttir.batch_norm_inference"(%559, %563, %567, %571, %109, %572) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x256x28x28xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<1x256x28x28xbf16>) -> tensor<1x256x28x28xbf16> loc(#loc549)
        %574 = ttir.empty() : tensor<1x256x28x28xbf16> loc(#loc550)
        %575 = "ttir.maximum"(%573, %6, %574) : (tensor<1x256x28x28xbf16>, tensor<1x256x28x28xbf16>, tensor<1x256x28x28xbf16>) -> tensor<1x256x28x28xbf16> loc(#loc550)
        %576 = ttir.empty() : tensor<1x256x28x28xbf16> loc(#loc551)
        %577 = "ttir.convolution"(%575, %arg112, %576) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x256x28x28xbf16>, tensor<256x256x3x3xbf16>, tensor<1x256x28x28xbf16>) -> tensor<1x256x28x28xbf16> loc(#loc551)
        %578 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc552)
        %579 = "ttir.reshape"(%arg111, %578) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc552)
        %580 = ttir.empty() : tensor<256xbf16> loc(#loc553)
        %581 = "ttir.reshape"(%579, %580) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc553)
        %582 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc554)
        %583 = "ttir.reshape"(%arg110, %582) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc554)
        %584 = ttir.empty() : tensor<256xbf16> loc(#loc555)
        %585 = "ttir.reshape"(%583, %584) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc555)
        %586 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc556)
        %587 = "ttir.reshape"(%arg109, %586) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc556)
        %588 = ttir.empty() : tensor<256xbf16> loc(#loc557)
        %589 = "ttir.reshape"(%587, %588) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc557)
        %590 = ttir.empty() : tensor<1x256x28x28xbf16> loc(#loc558)
        %591 = "ttir.batch_norm_inference"(%577, %581, %585, %589, %113, %590) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x256x28x28xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<1x256x28x28xbf16>) -> tensor<1x256x28x28xbf16> loc(#loc558)
        %592 = ttir.empty() : tensor<1x256x28x28xbf16> loc(#loc559)
        %593 = "ttir.maximum"(%591, %6, %592) : (tensor<1x256x28x28xbf16>, tensor<1x256x28x28xbf16>, tensor<1x256x28x28xbf16>) -> tensor<1x256x28x28xbf16> loc(#loc559)
        %594 = ttir.empty() : tensor<1x512x28x28xbf16> loc(#loc560)
        %595 = "ttir.convolution"(%593, %arg107, %594) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x256x28x28xbf16>, tensor<512x256x1x1xbf16>, tensor<1x512x28x28xbf16>) -> tensor<1x512x28x28xbf16> loc(#loc560)
        %596 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc561)
        %597 = "ttir.reshape"(%arg106, %596) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc561)
        %598 = ttir.empty() : tensor<512xbf16> loc(#loc562)
        %599 = "ttir.reshape"(%597, %598) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc562)
        %600 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc563)
        %601 = "ttir.reshape"(%arg105, %600) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc563)
        %602 = ttir.empty() : tensor<512xbf16> loc(#loc564)
        %603 = "ttir.reshape"(%601, %602) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc564)
        %604 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc565)
        %605 = "ttir.reshape"(%arg104, %604) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc565)
        %606 = ttir.empty() : tensor<512xbf16> loc(#loc566)
        %607 = "ttir.reshape"(%605, %606) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc566)
        %608 = ttir.empty() : tensor<1x512x28x28xbf16> loc(#loc567)
        %609 = "ttir.batch_norm_inference"(%595, %599, %603, %607, %117, %608) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x512x28x28xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<1x512x28x28xbf16>) -> tensor<1x512x28x28xbf16> loc(#loc567)
        %610 = ttir.empty() : tensor<1x512x28x28xbf16> loc(#loc568)
        %611 = "ttir.add"(%609, %557, %610) : (tensor<1x512x28x28xbf16>, tensor<1x512x28x28xbf16>, tensor<1x512x28x28xbf16>) -> tensor<1x512x28x28xbf16> loc(#loc568)
        %612 = ttir.empty() : tensor<1x512x28x28xbf16> loc(#loc569)
        %613 = "ttir.maximum"(%611, %5, %612) : (tensor<1x512x28x28xbf16>, tensor<1x512x28x28xbf16>, tensor<1x512x28x28xbf16>) -> tensor<1x512x28x28xbf16> loc(#loc569)
        %614 = ttir.empty() : tensor<1x256x28x28xbf16> loc(#loc570)
        %615 = "ttir.convolution"(%613, %arg132, %614) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x512x28x28xbf16>, tensor<256x512x1x1xbf16>, tensor<1x256x28x28xbf16>) -> tensor<1x256x28x28xbf16> loc(#loc570)
        %616 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc571)
        %617 = "ttir.reshape"(%arg131, %616) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc571)
        %618 = ttir.empty() : tensor<256xbf16> loc(#loc572)
        %619 = "ttir.reshape"(%617, %618) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc572)
        %620 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc573)
        %621 = "ttir.reshape"(%arg130, %620) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc573)
        %622 = ttir.empty() : tensor<256xbf16> loc(#loc574)
        %623 = "ttir.reshape"(%621, %622) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc574)
        %624 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc575)
        %625 = "ttir.reshape"(%arg129, %624) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc575)
        %626 = ttir.empty() : tensor<256xbf16> loc(#loc576)
        %627 = "ttir.reshape"(%625, %626) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc576)
        %628 = ttir.empty() : tensor<1x256x28x28xbf16> loc(#loc577)
        %629 = "ttir.batch_norm_inference"(%615, %619, %623, %627, %121, %628) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x256x28x28xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<1x256x28x28xbf16>) -> tensor<1x256x28x28xbf16> loc(#loc577)
        %630 = ttir.empty() : tensor<1x256x28x28xbf16> loc(#loc578)
        %631 = "ttir.maximum"(%629, %6, %630) : (tensor<1x256x28x28xbf16>, tensor<1x256x28x28xbf16>, tensor<1x256x28x28xbf16>) -> tensor<1x256x28x28xbf16> loc(#loc578)
        %632 = ttir.empty() : tensor<1x256x28x28xbf16> loc(#loc579)
        %633 = "ttir.convolution"(%631, %arg127, %632) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x256x28x28xbf16>, tensor<256x256x3x3xbf16>, tensor<1x256x28x28xbf16>) -> tensor<1x256x28x28xbf16> loc(#loc579)
        %634 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc580)
        %635 = "ttir.reshape"(%arg126, %634) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc580)
        %636 = ttir.empty() : tensor<256xbf16> loc(#loc581)
        %637 = "ttir.reshape"(%635, %636) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc581)
        %638 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc582)
        %639 = "ttir.reshape"(%arg125, %638) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc582)
        %640 = ttir.empty() : tensor<256xbf16> loc(#loc583)
        %641 = "ttir.reshape"(%639, %640) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc583)
        %642 = ttir.empty() : tensor<1x1x256xbf16> loc(#loc584)
        %643 = "ttir.reshape"(%arg124, %642) <{shape = [1 : i32, 1 : i32, 256 : i32]}> : (tensor<256xbf16>, tensor<1x1x256xbf16>) -> tensor<1x1x256xbf16> loc(#loc584)
        %644 = ttir.empty() : tensor<256xbf16> loc(#loc585)
        %645 = "ttir.reshape"(%643, %644) <{shape = [256 : i32]}> : (tensor<1x1x256xbf16>, tensor<256xbf16>) -> tensor<256xbf16> loc(#loc585)
        %646 = ttir.empty() : tensor<1x256x28x28xbf16> loc(#loc586)
        %647 = "ttir.batch_norm_inference"(%633, %637, %641, %645, %125, %646) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x256x28x28xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<256xbf16>, tensor<1x256x28x28xbf16>) -> tensor<1x256x28x28xbf16> loc(#loc586)
        %648 = ttir.empty() : tensor<1x256x28x28xbf16> loc(#loc587)
        %649 = "ttir.maximum"(%647, %6, %648) : (tensor<1x256x28x28xbf16>, tensor<1x256x28x28xbf16>, tensor<1x256x28x28xbf16>) -> tensor<1x256x28x28xbf16> loc(#loc587)
        %650 = ttir.empty() : tensor<1x512x28x28xbf16> loc(#loc588)
        %651 = "ttir.convolution"(%649, %arg122, %650) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x256x28x28xbf16>, tensor<512x256x1x1xbf16>, tensor<1x512x28x28xbf16>) -> tensor<1x512x28x28xbf16> loc(#loc588)
        %652 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc589)
        %653 = "ttir.reshape"(%arg121, %652) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc589)
        %654 = ttir.empty() : tensor<512xbf16> loc(#loc590)
        %655 = "ttir.reshape"(%653, %654) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc590)
        %656 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc591)
        %657 = "ttir.reshape"(%arg120, %656) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc591)
        %658 = ttir.empty() : tensor<512xbf16> loc(#loc592)
        %659 = "ttir.reshape"(%657, %658) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc592)
        %660 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc593)
        %661 = "ttir.reshape"(%arg119, %660) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc593)
        %662 = ttir.empty() : tensor<512xbf16> loc(#loc594)
        %663 = "ttir.reshape"(%661, %662) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc594)
        %664 = ttir.empty() : tensor<1x512x28x28xbf16> loc(#loc595)
        %665 = "ttir.batch_norm_inference"(%651, %655, %659, %663, %129, %664) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x512x28x28xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<1x512x28x28xbf16>) -> tensor<1x512x28x28xbf16> loc(#loc595)
        %666 = ttir.empty() : tensor<1x512x28x28xbf16> loc(#loc596)
        %667 = "ttir.add"(%665, %613, %666) : (tensor<1x512x28x28xbf16>, tensor<1x512x28x28xbf16>, tensor<1x512x28x28xbf16>) -> tensor<1x512x28x28xbf16> loc(#loc596)
        %668 = ttir.empty() : tensor<1x512x28x28xbf16> loc(#loc597)
        %669 = "ttir.maximum"(%667, %5, %668) : (tensor<1x512x28x28xbf16>, tensor<1x512x28x28xbf16>, tensor<1x512x28x28xbf16>) -> tensor<1x512x28x28xbf16> loc(#loc597)
        %670 = ttir.empty() : tensor<1x512x28x28xbf16> loc(#loc598)
        %671 = "ttir.convolution"(%669, %arg147, %670) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x512x28x28xbf16>, tensor<512x512x1x1xbf16>, tensor<1x512x28x28xbf16>) -> tensor<1x512x28x28xbf16> loc(#loc598)
        %672 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc599)
        %673 = "ttir.reshape"(%arg146, %672) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc599)
        %674 = ttir.empty() : tensor<512xbf16> loc(#loc600)
        %675 = "ttir.reshape"(%673, %674) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc600)
        %676 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc601)
        %677 = "ttir.reshape"(%arg145, %676) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc601)
        %678 = ttir.empty() : tensor<512xbf16> loc(#loc602)
        %679 = "ttir.reshape"(%677, %678) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc602)
        %680 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc603)
        %681 = "ttir.reshape"(%arg144, %680) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc603)
        %682 = ttir.empty() : tensor<512xbf16> loc(#loc604)
        %683 = "ttir.reshape"(%681, %682) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc604)
        %684 = ttir.empty() : tensor<1x512x28x28xbf16> loc(#loc605)
        %685 = "ttir.batch_norm_inference"(%671, %675, %679, %683, %137, %684) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x512x28x28xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<1x512x28x28xbf16>) -> tensor<1x512x28x28xbf16> loc(#loc605)
        %686 = ttir.empty() : tensor<1x512x28x28xbf16> loc(#loc606)
        %687 = "ttir.maximum"(%685, %5, %686) : (tensor<1x512x28x28xbf16>, tensor<1x512x28x28xbf16>, tensor<1x512x28x28xbf16>) -> tensor<1x512x28x28xbf16> loc(#loc606)
        %688 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc607)
        %689 = "ttir.convolution"(%687, %arg142, %688) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 2, 2>}> : (tensor<1x512x28x28xbf16>, tensor<512x512x3x3xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc607)
        %690 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc608)
        %691 = "ttir.reshape"(%arg141, %690) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc608)
        %692 = ttir.empty() : tensor<512xbf16> loc(#loc609)
        %693 = "ttir.reshape"(%691, %692) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc609)
        %694 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc610)
        %695 = "ttir.reshape"(%arg140, %694) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc610)
        %696 = ttir.empty() : tensor<512xbf16> loc(#loc611)
        %697 = "ttir.reshape"(%695, %696) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc611)
        %698 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc612)
        %699 = "ttir.reshape"(%arg139, %698) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc612)
        %700 = ttir.empty() : tensor<512xbf16> loc(#loc613)
        %701 = "ttir.reshape"(%699, %700) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc613)
        %702 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc614)
        %703 = "ttir.batch_norm_inference"(%689, %693, %697, %701, %141, %702) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x512x14x14xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc614)
        %704 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc615)
        %705 = "ttir.maximum"(%703, %4, %704) : (tensor<1x512x14x14xbf16>, tensor<1x512x14x14xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc615)
        %706 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc616)
        %707 = "ttir.convolution"(%705, %arg137, %706) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x512x14x14xbf16>, tensor<1024x512x1x1xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc616)
        %708 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc617)
        %709 = "ttir.reshape"(%arg136, %708) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc617)
        %710 = ttir.empty() : tensor<1024xbf16> loc(#loc618)
        %711 = "ttir.reshape"(%709, %710) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc618)
        %712 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc619)
        %713 = "ttir.reshape"(%arg135, %712) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc619)
        %714 = ttir.empty() : tensor<1024xbf16> loc(#loc620)
        %715 = "ttir.reshape"(%713, %714) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc620)
        %716 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc621)
        %717 = "ttir.reshape"(%arg134, %716) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc621)
        %718 = ttir.empty() : tensor<1024xbf16> loc(#loc622)
        %719 = "ttir.reshape"(%717, %718) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc622)
        %720 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc623)
        %721 = "ttir.batch_norm_inference"(%707, %711, %715, %719, %145, %720) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x1024x14x14xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc623)
        %722 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc624)
        %723 = "ttir.convolution"(%669, %arg11, %722) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 2, 2>}> : (tensor<1x512x28x28xbf16>, tensor<1024x512x1x1xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc624)
        %724 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc625)
        %725 = "ttir.reshape"(%arg10, %724) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc625)
        %726 = ttir.empty() : tensor<1024xbf16> loc(#loc626)
        %727 = "ttir.reshape"(%725, %726) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc626)
        %728 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc627)
        %729 = "ttir.reshape"(%arg9, %728) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc627)
        %730 = ttir.empty() : tensor<1024xbf16> loc(#loc628)
        %731 = "ttir.reshape"(%729, %730) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc628)
        %732 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc629)
        %733 = "ttir.reshape"(%arg8, %732) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc629)
        %734 = ttir.empty() : tensor<1024xbf16> loc(#loc630)
        %735 = "ttir.reshape"(%733, %734) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc630)
        %736 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc631)
        %737 = "ttir.batch_norm_inference"(%723, %727, %731, %735, %133, %736) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x1024x14x14xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc631)
        %738 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc632)
        %739 = "ttir.add"(%721, %737, %738) : (tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc632)
        %740 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc633)
        %741 = "ttir.maximum"(%739, %3, %740) : (tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc633)
        %742 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc634)
        %743 = "ttir.convolution"(%741, %arg162, %742) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1024x14x14xbf16>, tensor<512x1024x1x1xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc634)
        %744 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc635)
        %745 = "ttir.reshape"(%arg161, %744) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc635)
        %746 = ttir.empty() : tensor<512xbf16> loc(#loc636)
        %747 = "ttir.reshape"(%745, %746) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc636)
        %748 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc637)
        %749 = "ttir.reshape"(%arg160, %748) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc637)
        %750 = ttir.empty() : tensor<512xbf16> loc(#loc638)
        %751 = "ttir.reshape"(%749, %750) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc638)
        %752 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc639)
        %753 = "ttir.reshape"(%arg159, %752) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc639)
        %754 = ttir.empty() : tensor<512xbf16> loc(#loc640)
        %755 = "ttir.reshape"(%753, %754) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc640)
        %756 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc641)
        %757 = "ttir.batch_norm_inference"(%743, %747, %751, %755, %149, %756) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x512x14x14xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc641)
        %758 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc642)
        %759 = "ttir.maximum"(%757, %4, %758) : (tensor<1x512x14x14xbf16>, tensor<1x512x14x14xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc642)
        %760 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc643)
        %761 = "ttir.convolution"(%759, %arg157, %760) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x512x14x14xbf16>, tensor<512x512x3x3xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc643)
        %762 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc644)
        %763 = "ttir.reshape"(%arg156, %762) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc644)
        %764 = ttir.empty() : tensor<512xbf16> loc(#loc645)
        %765 = "ttir.reshape"(%763, %764) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc645)
        %766 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc646)
        %767 = "ttir.reshape"(%arg155, %766) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc646)
        %768 = ttir.empty() : tensor<512xbf16> loc(#loc647)
        %769 = "ttir.reshape"(%767, %768) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc647)
        %770 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc648)
        %771 = "ttir.reshape"(%arg154, %770) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc648)
        %772 = ttir.empty() : tensor<512xbf16> loc(#loc649)
        %773 = "ttir.reshape"(%771, %772) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc649)
        %774 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc650)
        %775 = "ttir.batch_norm_inference"(%761, %765, %769, %773, %153, %774) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x512x14x14xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc650)
        %776 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc651)
        %777 = "ttir.maximum"(%775, %4, %776) : (tensor<1x512x14x14xbf16>, tensor<1x512x14x14xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc651)
        %778 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc652)
        %779 = "ttir.convolution"(%777, %arg152, %778) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x512x14x14xbf16>, tensor<1024x512x1x1xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc652)
        %780 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc653)
        %781 = "ttir.reshape"(%arg151, %780) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc653)
        %782 = ttir.empty() : tensor<1024xbf16> loc(#loc654)
        %783 = "ttir.reshape"(%781, %782) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc654)
        %784 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc655)
        %785 = "ttir.reshape"(%arg150, %784) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc655)
        %786 = ttir.empty() : tensor<1024xbf16> loc(#loc656)
        %787 = "ttir.reshape"(%785, %786) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc656)
        %788 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc657)
        %789 = "ttir.reshape"(%arg149, %788) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc657)
        %790 = ttir.empty() : tensor<1024xbf16> loc(#loc658)
        %791 = "ttir.reshape"(%789, %790) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc658)
        %792 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc659)
        %793 = "ttir.batch_norm_inference"(%779, %783, %787, %791, %157, %792) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x1024x14x14xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc659)
        %794 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc660)
        %795 = "ttir.add"(%793, %741, %794) : (tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc660)
        %796 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc661)
        %797 = "ttir.maximum"(%795, %3, %796) : (tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc661)
        %798 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc662)
        %799 = "ttir.convolution"(%797, %arg177, %798) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1024x14x14xbf16>, tensor<512x1024x1x1xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc662)
        %800 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc663)
        %801 = "ttir.reshape"(%arg176, %800) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc663)
        %802 = ttir.empty() : tensor<512xbf16> loc(#loc664)
        %803 = "ttir.reshape"(%801, %802) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc664)
        %804 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc665)
        %805 = "ttir.reshape"(%arg175, %804) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc665)
        %806 = ttir.empty() : tensor<512xbf16> loc(#loc666)
        %807 = "ttir.reshape"(%805, %806) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc666)
        %808 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc667)
        %809 = "ttir.reshape"(%arg174, %808) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc667)
        %810 = ttir.empty() : tensor<512xbf16> loc(#loc668)
        %811 = "ttir.reshape"(%809, %810) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc668)
        %812 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc669)
        %813 = "ttir.batch_norm_inference"(%799, %803, %807, %811, %161, %812) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x512x14x14xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc669)
        %814 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc670)
        %815 = "ttir.maximum"(%813, %4, %814) : (tensor<1x512x14x14xbf16>, tensor<1x512x14x14xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc670)
        %816 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc671)
        %817 = "ttir.convolution"(%815, %arg172, %816) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x512x14x14xbf16>, tensor<512x512x3x3xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc671)
        %818 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc672)
        %819 = "ttir.reshape"(%arg171, %818) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc672)
        %820 = ttir.empty() : tensor<512xbf16> loc(#loc673)
        %821 = "ttir.reshape"(%819, %820) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc673)
        %822 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc674)
        %823 = "ttir.reshape"(%arg170, %822) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc674)
        %824 = ttir.empty() : tensor<512xbf16> loc(#loc675)
        %825 = "ttir.reshape"(%823, %824) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc675)
        %826 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc676)
        %827 = "ttir.reshape"(%arg169, %826) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc676)
        %828 = ttir.empty() : tensor<512xbf16> loc(#loc677)
        %829 = "ttir.reshape"(%827, %828) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc677)
        %830 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc678)
        %831 = "ttir.batch_norm_inference"(%817, %821, %825, %829, %165, %830) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x512x14x14xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc678)
        %832 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc679)
        %833 = "ttir.maximum"(%831, %4, %832) : (tensor<1x512x14x14xbf16>, tensor<1x512x14x14xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc679)
        %834 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc680)
        %835 = "ttir.convolution"(%833, %arg167, %834) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x512x14x14xbf16>, tensor<1024x512x1x1xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc680)
        %836 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc681)
        %837 = "ttir.reshape"(%arg166, %836) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc681)
        %838 = ttir.empty() : tensor<1024xbf16> loc(#loc682)
        %839 = "ttir.reshape"(%837, %838) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc682)
        %840 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc683)
        %841 = "ttir.reshape"(%arg165, %840) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc683)
        %842 = ttir.empty() : tensor<1024xbf16> loc(#loc684)
        %843 = "ttir.reshape"(%841, %842) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc684)
        %844 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc685)
        %845 = "ttir.reshape"(%arg164, %844) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc685)
        %846 = ttir.empty() : tensor<1024xbf16> loc(#loc686)
        %847 = "ttir.reshape"(%845, %846) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc686)
        %848 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc687)
        %849 = "ttir.batch_norm_inference"(%835, %839, %843, %847, %169, %848) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x1024x14x14xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc687)
        %850 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc688)
        %851 = "ttir.add"(%849, %797, %850) : (tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc688)
        %852 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc689)
        %853 = "ttir.maximum"(%851, %3, %852) : (tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc689)
        %854 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc690)
        %855 = "ttir.convolution"(%853, %arg192, %854) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1024x14x14xbf16>, tensor<512x1024x1x1xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc690)
        %856 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc691)
        %857 = "ttir.reshape"(%arg191, %856) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc691)
        %858 = ttir.empty() : tensor<512xbf16> loc(#loc692)
        %859 = "ttir.reshape"(%857, %858) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc692)
        %860 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc693)
        %861 = "ttir.reshape"(%arg190, %860) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc693)
        %862 = ttir.empty() : tensor<512xbf16> loc(#loc694)
        %863 = "ttir.reshape"(%861, %862) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc694)
        %864 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc695)
        %865 = "ttir.reshape"(%arg189, %864) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc695)
        %866 = ttir.empty() : tensor<512xbf16> loc(#loc696)
        %867 = "ttir.reshape"(%865, %866) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc696)
        %868 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc697)
        %869 = "ttir.batch_norm_inference"(%855, %859, %863, %867, %173, %868) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x512x14x14xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc697)
        %870 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc698)
        %871 = "ttir.maximum"(%869, %4, %870) : (tensor<1x512x14x14xbf16>, tensor<1x512x14x14xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc698)
        %872 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc699)
        %873 = "ttir.convolution"(%871, %arg187, %872) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x512x14x14xbf16>, tensor<512x512x3x3xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc699)
        %874 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc700)
        %875 = "ttir.reshape"(%arg186, %874) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc700)
        %876 = ttir.empty() : tensor<512xbf16> loc(#loc701)
        %877 = "ttir.reshape"(%875, %876) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc701)
        %878 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc702)
        %879 = "ttir.reshape"(%arg185, %878) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc702)
        %880 = ttir.empty() : tensor<512xbf16> loc(#loc703)
        %881 = "ttir.reshape"(%879, %880) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc703)
        %882 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc704)
        %883 = "ttir.reshape"(%arg184, %882) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc704)
        %884 = ttir.empty() : tensor<512xbf16> loc(#loc705)
        %885 = "ttir.reshape"(%883, %884) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc705)
        %886 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc706)
        %887 = "ttir.batch_norm_inference"(%873, %877, %881, %885, %177, %886) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x512x14x14xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc706)
        %888 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc707)
        %889 = "ttir.maximum"(%887, %4, %888) : (tensor<1x512x14x14xbf16>, tensor<1x512x14x14xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc707)
        %890 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc708)
        %891 = "ttir.convolution"(%889, %arg182, %890) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x512x14x14xbf16>, tensor<1024x512x1x1xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc708)
        %892 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc709)
        %893 = "ttir.reshape"(%arg181, %892) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc709)
        %894 = ttir.empty() : tensor<1024xbf16> loc(#loc710)
        %895 = "ttir.reshape"(%893, %894) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc710)
        %896 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc711)
        %897 = "ttir.reshape"(%arg180, %896) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc711)
        %898 = ttir.empty() : tensor<1024xbf16> loc(#loc712)
        %899 = "ttir.reshape"(%897, %898) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc712)
        %900 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc713)
        %901 = "ttir.reshape"(%arg179, %900) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc713)
        %902 = ttir.empty() : tensor<1024xbf16> loc(#loc714)
        %903 = "ttir.reshape"(%901, %902) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc714)
        %904 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc715)
        %905 = "ttir.batch_norm_inference"(%891, %895, %899, %903, %181, %904) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x1024x14x14xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc715)
        %906 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc716)
        %907 = "ttir.add"(%905, %853, %906) : (tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc716)
        %908 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc717)
        %909 = "ttir.maximum"(%907, %3, %908) : (tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc717)
        %910 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc718)
        %911 = "ttir.convolution"(%909, %arg207, %910) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1024x14x14xbf16>, tensor<512x1024x1x1xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc718)
        %912 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc719)
        %913 = "ttir.reshape"(%arg206, %912) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc719)
        %914 = ttir.empty() : tensor<512xbf16> loc(#loc720)
        %915 = "ttir.reshape"(%913, %914) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc720)
        %916 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc721)
        %917 = "ttir.reshape"(%arg205, %916) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc721)
        %918 = ttir.empty() : tensor<512xbf16> loc(#loc722)
        %919 = "ttir.reshape"(%917, %918) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc722)
        %920 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc723)
        %921 = "ttir.reshape"(%arg204, %920) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc723)
        %922 = ttir.empty() : tensor<512xbf16> loc(#loc724)
        %923 = "ttir.reshape"(%921, %922) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc724)
        %924 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc725)
        %925 = "ttir.batch_norm_inference"(%911, %915, %919, %923, %185, %924) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x512x14x14xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc725)
        %926 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc726)
        %927 = "ttir.maximum"(%925, %4, %926) : (tensor<1x512x14x14xbf16>, tensor<1x512x14x14xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc726)
        %928 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc727)
        %929 = "ttir.convolution"(%927, %arg202, %928) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x512x14x14xbf16>, tensor<512x512x3x3xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc727)
        %930 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc728)
        %931 = "ttir.reshape"(%arg201, %930) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc728)
        %932 = ttir.empty() : tensor<512xbf16> loc(#loc729)
        %933 = "ttir.reshape"(%931, %932) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc729)
        %934 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc730)
        %935 = "ttir.reshape"(%arg200, %934) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc730)
        %936 = ttir.empty() : tensor<512xbf16> loc(#loc731)
        %937 = "ttir.reshape"(%935, %936) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc731)
        %938 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc732)
        %939 = "ttir.reshape"(%arg199, %938) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc732)
        %940 = ttir.empty() : tensor<512xbf16> loc(#loc733)
        %941 = "ttir.reshape"(%939, %940) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc733)
        %942 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc734)
        %943 = "ttir.batch_norm_inference"(%929, %933, %937, %941, %189, %942) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x512x14x14xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc734)
        %944 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc735)
        %945 = "ttir.maximum"(%943, %4, %944) : (tensor<1x512x14x14xbf16>, tensor<1x512x14x14xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc735)
        %946 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc736)
        %947 = "ttir.convolution"(%945, %arg197, %946) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x512x14x14xbf16>, tensor<1024x512x1x1xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc736)
        %948 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc737)
        %949 = "ttir.reshape"(%arg196, %948) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc737)
        %950 = ttir.empty() : tensor<1024xbf16> loc(#loc738)
        %951 = "ttir.reshape"(%949, %950) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc738)
        %952 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc739)
        %953 = "ttir.reshape"(%arg195, %952) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc739)
        %954 = ttir.empty() : tensor<1024xbf16> loc(#loc740)
        %955 = "ttir.reshape"(%953, %954) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc740)
        %956 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc741)
        %957 = "ttir.reshape"(%arg194, %956) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc741)
        %958 = ttir.empty() : tensor<1024xbf16> loc(#loc742)
        %959 = "ttir.reshape"(%957, %958) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc742)
        %960 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc743)
        %961 = "ttir.batch_norm_inference"(%947, %951, %955, %959, %193, %960) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x1024x14x14xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc743)
        %962 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc744)
        %963 = "ttir.add"(%961, %909, %962) : (tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc744)
        %964 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc745)
        %965 = "ttir.maximum"(%963, %3, %964) : (tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc745)
        %966 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc746)
        %967 = "ttir.convolution"(%965, %arg222, %966) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1024x14x14xbf16>, tensor<512x1024x1x1xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc746)
        %968 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc747)
        %969 = "ttir.reshape"(%arg221, %968) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc747)
        %970 = ttir.empty() : tensor<512xbf16> loc(#loc748)
        %971 = "ttir.reshape"(%969, %970) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc748)
        %972 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc749)
        %973 = "ttir.reshape"(%arg220, %972) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc749)
        %974 = ttir.empty() : tensor<512xbf16> loc(#loc750)
        %975 = "ttir.reshape"(%973, %974) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc750)
        %976 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc751)
        %977 = "ttir.reshape"(%arg219, %976) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc751)
        %978 = ttir.empty() : tensor<512xbf16> loc(#loc752)
        %979 = "ttir.reshape"(%977, %978) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc752)
        %980 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc753)
        %981 = "ttir.batch_norm_inference"(%967, %971, %975, %979, %197, %980) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x512x14x14xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc753)
        %982 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc754)
        %983 = "ttir.maximum"(%981, %4, %982) : (tensor<1x512x14x14xbf16>, tensor<1x512x14x14xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc754)
        %984 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc755)
        %985 = "ttir.convolution"(%983, %arg217, %984) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x512x14x14xbf16>, tensor<512x512x3x3xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc755)
        %986 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc756)
        %987 = "ttir.reshape"(%arg216, %986) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc756)
        %988 = ttir.empty() : tensor<512xbf16> loc(#loc757)
        %989 = "ttir.reshape"(%987, %988) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc757)
        %990 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc758)
        %991 = "ttir.reshape"(%arg215, %990) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc758)
        %992 = ttir.empty() : tensor<512xbf16> loc(#loc759)
        %993 = "ttir.reshape"(%991, %992) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc759)
        %994 = ttir.empty() : tensor<1x1x512xbf16> loc(#loc760)
        %995 = "ttir.reshape"(%arg214, %994) <{shape = [1 : i32, 1 : i32, 512 : i32]}> : (tensor<512xbf16>, tensor<1x1x512xbf16>) -> tensor<1x1x512xbf16> loc(#loc760)
        %996 = ttir.empty() : tensor<512xbf16> loc(#loc761)
        %997 = "ttir.reshape"(%995, %996) <{shape = [512 : i32]}> : (tensor<1x1x512xbf16>, tensor<512xbf16>) -> tensor<512xbf16> loc(#loc761)
        %998 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc762)
        %999 = "ttir.batch_norm_inference"(%985, %989, %993, %997, %201, %998) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x512x14x14xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<512xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc762)
        %1000 = ttir.empty() : tensor<1x512x14x14xbf16> loc(#loc763)
        %1001 = "ttir.maximum"(%999, %4, %1000) : (tensor<1x512x14x14xbf16>, tensor<1x512x14x14xbf16>, tensor<1x512x14x14xbf16>) -> tensor<1x512x14x14xbf16> loc(#loc763)
        %1002 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc764)
        %1003 = "ttir.convolution"(%1001, %arg212, %1002) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x512x14x14xbf16>, tensor<1024x512x1x1xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc764)
        %1004 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc765)
        %1005 = "ttir.reshape"(%arg211, %1004) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc765)
        %1006 = ttir.empty() : tensor<1024xbf16> loc(#loc766)
        %1007 = "ttir.reshape"(%1005, %1006) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc766)
        %1008 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc767)
        %1009 = "ttir.reshape"(%arg210, %1008) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc767)
        %1010 = ttir.empty() : tensor<1024xbf16> loc(#loc768)
        %1011 = "ttir.reshape"(%1009, %1010) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc768)
        %1012 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc769)
        %1013 = "ttir.reshape"(%arg209, %1012) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc769)
        %1014 = ttir.empty() : tensor<1024xbf16> loc(#loc770)
        %1015 = "ttir.reshape"(%1013, %1014) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc770)
        %1016 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc771)
        %1017 = "ttir.batch_norm_inference"(%1003, %1007, %1011, %1015, %205, %1016) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x1024x14x14xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc771)
        %1018 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc772)
        %1019 = "ttir.add"(%1017, %965, %1018) : (tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc772)
        %1020 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc773)
        %1021 = "ttir.maximum"(%1019, %3, %1020) : (tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc773)
        %1022 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc774)
        %1023 = "ttir.convolution"(%1021, %arg237, %1022) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1024x14x14xbf16>, tensor<1024x1024x1x1xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc774)
        %1024 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc775)
        %1025 = "ttir.reshape"(%arg236, %1024) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc775)
        %1026 = ttir.empty() : tensor<1024xbf16> loc(#loc776)
        %1027 = "ttir.reshape"(%1025, %1026) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc776)
        %1028 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc777)
        %1029 = "ttir.reshape"(%arg235, %1028) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc777)
        %1030 = ttir.empty() : tensor<1024xbf16> loc(#loc778)
        %1031 = "ttir.reshape"(%1029, %1030) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc778)
        %1032 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc779)
        %1033 = "ttir.reshape"(%arg234, %1032) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc779)
        %1034 = ttir.empty() : tensor<1024xbf16> loc(#loc780)
        %1035 = "ttir.reshape"(%1033, %1034) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc780)
        %1036 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc781)
        %1037 = "ttir.batch_norm_inference"(%1023, %1027, %1031, %1035, %213, %1036) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x1024x14x14xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc781)
        %1038 = ttir.empty() : tensor<1x1024x14x14xbf16> loc(#loc782)
        %1039 = "ttir.maximum"(%1037, %3, %1038) : (tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>, tensor<1x1024x14x14xbf16>) -> tensor<1x1024x14x14xbf16> loc(#loc782)
        %1040 = ttir.empty() : tensor<1x1024x7x7xbf16> loc(#loc783)
        %1041 = "ttir.convolution"(%1039, %arg232, %1040) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 2, 2>}> : (tensor<1x1024x14x14xbf16>, tensor<1024x1024x3x3xbf16>, tensor<1x1024x7x7xbf16>) -> tensor<1x1024x7x7xbf16> loc(#loc783)
        %1042 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc784)
        %1043 = "ttir.reshape"(%arg231, %1042) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc784)
        %1044 = ttir.empty() : tensor<1024xbf16> loc(#loc785)
        %1045 = "ttir.reshape"(%1043, %1044) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc785)
        %1046 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc786)
        %1047 = "ttir.reshape"(%arg230, %1046) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc786)
        %1048 = ttir.empty() : tensor<1024xbf16> loc(#loc787)
        %1049 = "ttir.reshape"(%1047, %1048) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc787)
        %1050 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc788)
        %1051 = "ttir.reshape"(%arg229, %1050) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc788)
        %1052 = ttir.empty() : tensor<1024xbf16> loc(#loc789)
        %1053 = "ttir.reshape"(%1051, %1052) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc789)
        %1054 = ttir.empty() : tensor<1x1024x7x7xbf16> loc(#loc790)
        %1055 = "ttir.batch_norm_inference"(%1041, %1045, %1049, %1053, %217, %1054) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x1024x7x7xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1x1024x7x7xbf16>) -> tensor<1x1024x7x7xbf16> loc(#loc790)
        %1056 = ttir.empty() : tensor<1x1024x7x7xbf16> loc(#loc791)
        %1057 = "ttir.maximum"(%1055, %2, %1056) : (tensor<1x1024x7x7xbf16>, tensor<1x1024x7x7xbf16>, tensor<1x1024x7x7xbf16>) -> tensor<1x1024x7x7xbf16> loc(#loc791)
        %1058 = ttir.empty() : tensor<1x2048x7x7xbf16> loc(#loc792)
        %1059 = "ttir.convolution"(%1057, %arg227, %1058) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1024x7x7xbf16>, tensor<2048x1024x1x1xbf16>, tensor<1x2048x7x7xbf16>) -> tensor<1x2048x7x7xbf16> loc(#loc792)
        %1060 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc793)
        %1061 = "ttir.reshape"(%arg226, %1060) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc793)
        %1062 = ttir.empty() : tensor<2048xbf16> loc(#loc794)
        %1063 = "ttir.reshape"(%1061, %1062) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc794)
        %1064 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc795)
        %1065 = "ttir.reshape"(%arg225, %1064) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc795)
        %1066 = ttir.empty() : tensor<2048xbf16> loc(#loc796)
        %1067 = "ttir.reshape"(%1065, %1066) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc796)
        %1068 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc797)
        %1069 = "ttir.reshape"(%arg224, %1068) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc797)
        %1070 = ttir.empty() : tensor<2048xbf16> loc(#loc798)
        %1071 = "ttir.reshape"(%1069, %1070) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc798)
        %1072 = ttir.empty() : tensor<1x2048x7x7xbf16> loc(#loc799)
        %1073 = "ttir.batch_norm_inference"(%1059, %1063, %1067, %1071, %221, %1072) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x2048x7x7xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<1x2048x7x7xbf16>) -> tensor<1x2048x7x7xbf16> loc(#loc799)
        %1074 = ttir.empty() : tensor<1x2048x7x7xbf16> loc(#loc800)
        %1075 = "ttir.convolution"(%1021, %arg6, %1074) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 2, 2>}> : (tensor<1x1024x14x14xbf16>, tensor<2048x1024x1x1xbf16>, tensor<1x2048x7x7xbf16>) -> tensor<1x2048x7x7xbf16> loc(#loc800)
        %1076 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc801)
        %1077 = "ttir.reshape"(%arg5, %1076) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc801)
        %1078 = ttir.empty() : tensor<2048xbf16> loc(#loc802)
        %1079 = "ttir.reshape"(%1077, %1078) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc802)
        %1080 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc803)
        %1081 = "ttir.reshape"(%arg4, %1080) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc803)
        %1082 = ttir.empty() : tensor<2048xbf16> loc(#loc804)
        %1083 = "ttir.reshape"(%1081, %1082) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc804)
        %1084 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc805)
        %1085 = "ttir.reshape"(%arg3, %1084) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc805)
        %1086 = ttir.empty() : tensor<2048xbf16> loc(#loc806)
        %1087 = "ttir.reshape"(%1085, %1086) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc806)
        %1088 = ttir.empty() : tensor<1x2048x7x7xbf16> loc(#loc807)
        %1089 = "ttir.batch_norm_inference"(%1075, %1079, %1083, %1087, %209, %1088) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x2048x7x7xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<1x2048x7x7xbf16>) -> tensor<1x2048x7x7xbf16> loc(#loc807)
        %1090 = ttir.empty() : tensor<1x2048x7x7xbf16> loc(#loc808)
        %1091 = "ttir.add"(%1073, %1089, %1090) : (tensor<1x2048x7x7xbf16>, tensor<1x2048x7x7xbf16>, tensor<1x2048x7x7xbf16>) -> tensor<1x2048x7x7xbf16> loc(#loc808)
        %1092 = ttir.empty() : tensor<1x2048x7x7xbf16> loc(#loc809)
        %1093 = "ttir.maximum"(%1091, %1, %1092) : (tensor<1x2048x7x7xbf16>, tensor<1x2048x7x7xbf16>, tensor<1x2048x7x7xbf16>) -> tensor<1x2048x7x7xbf16> loc(#loc809)
        %1094 = ttir.empty() : tensor<1x1024x7x7xbf16> loc(#loc810)
        %1095 = "ttir.convolution"(%1093, %arg252, %1094) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x2048x7x7xbf16>, tensor<1024x2048x1x1xbf16>, tensor<1x1024x7x7xbf16>) -> tensor<1x1024x7x7xbf16> loc(#loc810)
        %1096 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc811)
        %1097 = "ttir.reshape"(%arg251, %1096) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc811)
        %1098 = ttir.empty() : tensor<1024xbf16> loc(#loc812)
        %1099 = "ttir.reshape"(%1097, %1098) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc812)
        %1100 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc813)
        %1101 = "ttir.reshape"(%arg250, %1100) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc813)
        %1102 = ttir.empty() : tensor<1024xbf16> loc(#loc814)
        %1103 = "ttir.reshape"(%1101, %1102) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc814)
        %1104 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc815)
        %1105 = "ttir.reshape"(%arg249, %1104) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc815)
        %1106 = ttir.empty() : tensor<1024xbf16> loc(#loc816)
        %1107 = "ttir.reshape"(%1105, %1106) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc816)
        %1108 = ttir.empty() : tensor<1x1024x7x7xbf16> loc(#loc817)
        %1109 = "ttir.batch_norm_inference"(%1095, %1099, %1103, %1107, %225, %1108) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x1024x7x7xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1x1024x7x7xbf16>) -> tensor<1x1024x7x7xbf16> loc(#loc817)
        %1110 = ttir.empty() : tensor<1x1024x7x7xbf16> loc(#loc818)
        %1111 = "ttir.maximum"(%1109, %2, %1110) : (tensor<1x1024x7x7xbf16>, tensor<1x1024x7x7xbf16>, tensor<1x1024x7x7xbf16>) -> tensor<1x1024x7x7xbf16> loc(#loc818)
        %1112 = ttir.empty() : tensor<1x1024x7x7xbf16> loc(#loc819)
        %1113 = "ttir.convolution"(%1111, %arg247, %1112) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1024x7x7xbf16>, tensor<1024x1024x3x3xbf16>, tensor<1x1024x7x7xbf16>) -> tensor<1x1024x7x7xbf16> loc(#loc819)
        %1114 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc820)
        %1115 = "ttir.reshape"(%arg246, %1114) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc820)
        %1116 = ttir.empty() : tensor<1024xbf16> loc(#loc821)
        %1117 = "ttir.reshape"(%1115, %1116) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc821)
        %1118 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc822)
        %1119 = "ttir.reshape"(%arg245, %1118) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc822)
        %1120 = ttir.empty() : tensor<1024xbf16> loc(#loc823)
        %1121 = "ttir.reshape"(%1119, %1120) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc823)
        %1122 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc824)
        %1123 = "ttir.reshape"(%arg244, %1122) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc824)
        %1124 = ttir.empty() : tensor<1024xbf16> loc(#loc825)
        %1125 = "ttir.reshape"(%1123, %1124) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc825)
        %1126 = ttir.empty() : tensor<1x1024x7x7xbf16> loc(#loc826)
        %1127 = "ttir.batch_norm_inference"(%1113, %1117, %1121, %1125, %229, %1126) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x1024x7x7xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1x1024x7x7xbf16>) -> tensor<1x1024x7x7xbf16> loc(#loc826)
        %1128 = ttir.empty() : tensor<1x1024x7x7xbf16> loc(#loc827)
        %1129 = "ttir.maximum"(%1127, %2, %1128) : (tensor<1x1024x7x7xbf16>, tensor<1x1024x7x7xbf16>, tensor<1x1024x7x7xbf16>) -> tensor<1x1024x7x7xbf16> loc(#loc827)
        %1130 = ttir.empty() : tensor<1x2048x7x7xbf16> loc(#loc828)
        %1131 = "ttir.convolution"(%1129, %arg242, %1130) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1024x7x7xbf16>, tensor<2048x1024x1x1xbf16>, tensor<1x2048x7x7xbf16>) -> tensor<1x2048x7x7xbf16> loc(#loc828)
        %1132 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc829)
        %1133 = "ttir.reshape"(%arg241, %1132) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc829)
        %1134 = ttir.empty() : tensor<2048xbf16> loc(#loc830)
        %1135 = "ttir.reshape"(%1133, %1134) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc830)
        %1136 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc831)
        %1137 = "ttir.reshape"(%arg240, %1136) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc831)
        %1138 = ttir.empty() : tensor<2048xbf16> loc(#loc832)
        %1139 = "ttir.reshape"(%1137, %1138) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc832)
        %1140 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc833)
        %1141 = "ttir.reshape"(%arg239, %1140) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc833)
        %1142 = ttir.empty() : tensor<2048xbf16> loc(#loc834)
        %1143 = "ttir.reshape"(%1141, %1142) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc834)
        %1144 = ttir.empty() : tensor<1x2048x7x7xbf16> loc(#loc835)
        %1145 = "ttir.batch_norm_inference"(%1131, %1135, %1139, %1143, %233, %1144) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x2048x7x7xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<1x2048x7x7xbf16>) -> tensor<1x2048x7x7xbf16> loc(#loc835)
        %1146 = ttir.empty() : tensor<1x2048x7x7xbf16> loc(#loc836)
        %1147 = "ttir.add"(%1145, %1093, %1146) : (tensor<1x2048x7x7xbf16>, tensor<1x2048x7x7xbf16>, tensor<1x2048x7x7xbf16>) -> tensor<1x2048x7x7xbf16> loc(#loc836)
        %1148 = ttir.empty() : tensor<1x2048x7x7xbf16> loc(#loc837)
        %1149 = "ttir.maximum"(%1147, %1, %1148) : (tensor<1x2048x7x7xbf16>, tensor<1x2048x7x7xbf16>, tensor<1x2048x7x7xbf16>) -> tensor<1x2048x7x7xbf16> loc(#loc837)
        %1150 = ttir.empty() : tensor<1x1024x7x7xbf16> loc(#loc838)
        %1151 = "ttir.convolution"(%1149, %arg267, %1150) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x2048x7x7xbf16>, tensor<1024x2048x1x1xbf16>, tensor<1x1024x7x7xbf16>) -> tensor<1x1024x7x7xbf16> loc(#loc838)
        %1152 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc839)
        %1153 = "ttir.reshape"(%arg266, %1152) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc839)
        %1154 = ttir.empty() : tensor<1024xbf16> loc(#loc840)
        %1155 = "ttir.reshape"(%1153, %1154) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc840)
        %1156 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc841)
        %1157 = "ttir.reshape"(%arg265, %1156) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc841)
        %1158 = ttir.empty() : tensor<1024xbf16> loc(#loc842)
        %1159 = "ttir.reshape"(%1157, %1158) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc842)
        %1160 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc843)
        %1161 = "ttir.reshape"(%arg264, %1160) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc843)
        %1162 = ttir.empty() : tensor<1024xbf16> loc(#loc844)
        %1163 = "ttir.reshape"(%1161, %1162) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc844)
        %1164 = ttir.empty() : tensor<1x1024x7x7xbf16> loc(#loc845)
        %1165 = "ttir.batch_norm_inference"(%1151, %1155, %1159, %1163, %237, %1164) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x1024x7x7xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1x1024x7x7xbf16>) -> tensor<1x1024x7x7xbf16> loc(#loc845)
        %1166 = ttir.empty() : tensor<1x1024x7x7xbf16> loc(#loc846)
        %1167 = "ttir.maximum"(%1165, %2, %1166) : (tensor<1x1024x7x7xbf16>, tensor<1x1024x7x7xbf16>, tensor<1x1024x7x7xbf16>) -> tensor<1x1024x7x7xbf16> loc(#loc846)
        %1168 = ttir.empty() : tensor<1x1024x7x7xbf16> loc(#loc847)
        %1169 = "ttir.convolution"(%1167, %arg262, %1168) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 1, 1, 1, 1>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1024x7x7xbf16>, tensor<1024x1024x3x3xbf16>, tensor<1x1024x7x7xbf16>) -> tensor<1x1024x7x7xbf16> loc(#loc847)
        %1170 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc848)
        %1171 = "ttir.reshape"(%arg261, %1170) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc848)
        %1172 = ttir.empty() : tensor<1024xbf16> loc(#loc849)
        %1173 = "ttir.reshape"(%1171, %1172) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc849)
        %1174 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc850)
        %1175 = "ttir.reshape"(%arg260, %1174) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc850)
        %1176 = ttir.empty() : tensor<1024xbf16> loc(#loc851)
        %1177 = "ttir.reshape"(%1175, %1176) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc851)
        %1178 = ttir.empty() : tensor<1x1x1024xbf16> loc(#loc852)
        %1179 = "ttir.reshape"(%arg259, %1178) <{shape = [1 : i32, 1 : i32, 1024 : i32]}> : (tensor<1024xbf16>, tensor<1x1x1024xbf16>) -> tensor<1x1x1024xbf16> loc(#loc852)
        %1180 = ttir.empty() : tensor<1024xbf16> loc(#loc853)
        %1181 = "ttir.reshape"(%1179, %1180) <{shape = [1024 : i32]}> : (tensor<1x1x1024xbf16>, tensor<1024xbf16>) -> tensor<1024xbf16> loc(#loc853)
        %1182 = ttir.empty() : tensor<1x1024x7x7xbf16> loc(#loc854)
        %1183 = "ttir.batch_norm_inference"(%1169, %1173, %1177, %1181, %241, %1182) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x1024x7x7xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1024xbf16>, tensor<1x1024x7x7xbf16>) -> tensor<1x1024x7x7xbf16> loc(#loc854)
        %1184 = ttir.empty() : tensor<1x1024x7x7xbf16> loc(#loc855)
        %1185 = "ttir.maximum"(%1183, %2, %1184) : (tensor<1x1024x7x7xbf16>, tensor<1x1024x7x7xbf16>, tensor<1x1024x7x7xbf16>) -> tensor<1x1024x7x7xbf16> loc(#loc855)
        %1186 = ttir.empty() : tensor<1x2048x7x7xbf16> loc(#loc856)
        %1187 = "ttir.convolution"(%1185, %arg257, %1186) <{batch_group_count = 1 : i64, convolution_layout = #ttir<convolution_layout input_batch = 0, input_feature = 1, input_spatial_dimensions = 2x3, kernel_output_feature = 0, kernel_input_feature = 1, kernel_spatial_dimensions = 2x3, output_batch = 0, output_feature = 1, output_spatial_dimensions = 2x3>, feature_group_count = 1 : i64, input_dilation = array<i64: 1, 1>, padding = array<i64: 0, 0, 0, 0>, weight_dilation = array<i64: 1, 1>, window_reversal = array<i1: false, false>, window_strides = array<i64: 1, 1>}> : (tensor<1x1024x7x7xbf16>, tensor<2048x1024x1x1xbf16>, tensor<1x2048x7x7xbf16>) -> tensor<1x2048x7x7xbf16> loc(#loc856)
        %1188 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc857)
        %1189 = "ttir.reshape"(%arg256, %1188) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc857)
        %1190 = ttir.empty() : tensor<2048xbf16> loc(#loc858)
        %1191 = "ttir.reshape"(%1189, %1190) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc858)
        %1192 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc859)
        %1193 = "ttir.reshape"(%arg255, %1192) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc859)
        %1194 = ttir.empty() : tensor<2048xbf16> loc(#loc860)
        %1195 = "ttir.reshape"(%1193, %1194) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc860)
        %1196 = ttir.empty() : tensor<1x1x2048xbf16> loc(#loc861)
        %1197 = "ttir.reshape"(%arg254, %1196) <{shape = [1 : i32, 1 : i32, 2048 : i32]}> : (tensor<2048xbf16>, tensor<1x1x2048xbf16>) -> tensor<1x1x2048xbf16> loc(#loc861)
        %1198 = ttir.empty() : tensor<2048xbf16> loc(#loc862)
        %1199 = "ttir.reshape"(%1197, %1198) <{shape = [2048 : i32]}> : (tensor<1x1x2048xbf16>, tensor<2048xbf16>) -> tensor<2048xbf16> loc(#loc862)
        %1200 = ttir.empty() : tensor<1x2048x7x7xbf16> loc(#loc863)
        %1201 = "ttir.batch_norm_inference"(%1187, %1191, %1195, %1199, %245, %1200) <{dimension = 1 : i32, epsilon = 9.99999974E-6 : f32}> : (tensor<1x2048x7x7xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<2048xbf16>, tensor<1x2048x7x7xbf16>) -> tensor<1x2048x7x7xbf16> loc(#loc863)
        %1202 = ttir.empty() : tensor<1x2048x7x7xbf16> loc(#loc864)
        %1203 = "ttir.add"(%1201, %1149, %1202) : (tensor<1x2048x7x7xbf16>, tensor<1x2048x7x7xbf16>, tensor<1x2048x7x7xbf16>) -> tensor<1x2048x7x7xbf16> loc(#loc864)
        %1204 = ttir.empty() : tensor<1x2048x7x7xbf16> loc(#loc865)
        %1205 = "ttir.maximum"(%1203, %1, %1204) : (tensor<1x2048x7x7xbf16>, tensor<1x2048x7x7xbf16>, tensor<1x2048x7x7xbf16>) -> tensor<1x2048x7x7xbf16> loc(#loc865)
        %1206 = ttir.empty() : tensor<1x2048xbf16> loc(#loc866)
        %1207 = "ttir.sum"(%1205, %1206) <{dim_arg = [2 : i32, 3 : i32], keep_dim = false}> : (tensor<1x2048x7x7xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16> loc(#loc866)
        %1208 = ttir.empty() : tensor<1x2048xbf16> loc(#loc867)
        %1209 = "ttir.multiply"(%1207, %0, %1208) : (tensor<1x2048xbf16>, tensor<1x2048xbf16>, tensor<1x2048xbf16>) -> tensor<1x2048xbf16> loc(#loc867)
        %1210 = ttir.empty() : tensor<1x1000x2048xbf16> loc(#loc868)
        %1211 = "ttir.reshape"(%arg1, %1210) <{shape = [1 : i32, 1000 : i32, 2048 : i32]}> : (tensor<1000x2048xbf16>, tensor<1x1000x2048xbf16>) -> tensor<1x1000x2048xbf16> loc(#loc868)
        %1212 = ttir.empty() : tensor<1000x2048xbf16> loc(#loc869)
        %1213 = "ttir.reshape"(%1211, %1212) <{shape = [1000 : i32, 2048 : i32]}> : (tensor<1x1000x2048xbf16>, tensor<1000x2048xbf16>) -> tensor<1000x2048xbf16> loc(#loc869)
        %1214 = ttir.empty() : tensor<2048x1000xbf16> loc(#loc870)
        %1215 = "ttir.permute"(%1213, %1214) <{permutation = array<i64: 1, 0>}> : (tensor<1000x2048xbf16>, tensor<2048x1000xbf16>) -> tensor<2048x1000xbf16> loc(#loc870)
        %1216 = "ttir.dot_general"(%1209, %1215) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x2048xbf16>, tensor<2048x1000xbf16>) -> tensor<1x1000xbf16> loc(#loc871)
        %1217 = ttir.empty() : tensor<1x1x1000xbf16> loc(#loc872)
        %1218 = "ttir.reshape"(%arg0, %1217) <{shape = [1 : i32, 1 : i32, 1000 : i32]}> : (tensor<1000xbf16>, tensor<1x1x1000xbf16>) -> tensor<1x1x1000xbf16> loc(#loc872)
        %1219 = ttir.empty() : tensor<1x1000xbf16> loc(#loc873)
        %1220 = "ttir.reshape"(%1218, %1219) <{shape = [1 : i32, 1000 : i32]}> : (tensor<1x1x1000xbf16>, tensor<1x1000xbf16>) -> tensor<1x1000xbf16> loc(#loc873)
        %1221 = ttir.empty() : tensor<1x1000xbf16> loc(#loc874)
        %1222 = "ttir.add"(%1216, %1220, %1221) : (tensor<1x1000xbf16>, tensor<1x1000xbf16>, tensor<1x1000xbf16>) -> tensor<1x1000xbf16> loc(#loc874)
        return %1222 : tensor<1x1000xbf16> loc(#loc)
      } loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc269 = loc("reshape.104")
#loc270 = loc("reshape.106")
#loc271 = loc("convolution.123")
#loc272 = loc("reshape.116")
#loc273 = loc("reshape.118")
#loc274 = loc("reshape.112")
#loc275 = loc("reshape.114")
#loc276 = loc("reshape.108")
#loc277 = loc("reshape.110")
#loc278 = loc("batch-norm-inference.124")
#loc279 = loc("maximum.131")
#loc280 = loc("pad.133")
#loc281 = loc("reduce-window.143")
#loc282 = loc("reshape.86")
#loc283 = loc("reshape.88")
#loc284 = loc("reshape.347")
#loc285 = loc("reshape.349")
#loc286 = loc("reshape.329")
#loc287 = loc("reshape.331")
#loc288 = loc("reshape.311")
#loc289 = loc("reshape.313")
#loc290 = loc("reshape.431")
#loc291 = loc("reshape.433")
#loc292 = loc("reshape.413")
#loc293 = loc("reshape.415")
#loc294 = loc("reshape.395")
#loc295 = loc("reshape.397")
#loc296 = loc("reshape.515")
#loc297 = loc("reshape.517")
#loc298 = loc("reshape.497")
#loc299 = loc("reshape.499")
#loc300 = loc("reshape.479")
#loc301 = loc("reshape.481")
#loc302 = loc("reshape.65")
#loc303 = loc("reshape.67")
#loc304 = loc("reshape.605")
#loc305 = loc("reshape.607")
#loc306 = loc("reshape.587")
#loc307 = loc("reshape.589")
#loc308 = loc("reshape.569")
#loc309 = loc("reshape.571")
#loc310 = loc("reshape.689")
#loc311 = loc("reshape.691")
#loc312 = loc("reshape.671")
#loc313 = loc("reshape.673")
#loc314 = loc("reshape.653")
#loc315 = loc("reshape.655")
#loc316 = loc("reshape.773")
#loc317 = loc("reshape.775")
#loc318 = loc("reshape.755")
#loc319 = loc("reshape.757")
#loc320 = loc("reshape.737")
#loc321 = loc("reshape.739")
#loc322 = loc("reshape.857")
#loc323 = loc("reshape.859")
#loc324 = loc("reshape.839")
#loc325 = loc("reshape.841")
#loc326 = loc("reshape.821")
#loc327 = loc("reshape.823")
#loc328 = loc("reshape.43")
#loc329 = loc("reshape.45")
#loc330 = loc("reshape.947")
#loc331 = loc("reshape.949")
#loc332 = loc("reshape.929")
#loc333 = loc("reshape.931")
#loc334 = loc("reshape.911")
#loc335 = loc("reshape.913")
#loc336 = loc("reshape.1031")
#loc337 = loc("reshape.1033")
#loc338 = loc("reshape.1013")
#loc339 = loc("reshape.1015")
#loc340 = loc("reshape.995")
#loc341 = loc("reshape.997")
#loc342 = loc("reshape.1115")
#loc343 = loc("reshape.1117")
#loc344 = loc("reshape.1097")
#loc345 = loc("reshape.1099")
#loc346 = loc("reshape.1079")
#loc347 = loc("reshape.1081")
#loc348 = loc("reshape.1199")
#loc349 = loc("reshape.1201")
#loc350 = loc("reshape.1181")
#loc351 = loc("reshape.1183")
#loc352 = loc("reshape.1163")
#loc353 = loc("reshape.1165")
#loc354 = loc("reshape.1283")
#loc355 = loc("reshape.1285")
#loc356 = loc("reshape.1265")
#loc357 = loc("reshape.1267")
#loc358 = loc("reshape.1247")
#loc359 = loc("reshape.1249")
#loc360 = loc("reshape.1367")
#loc361 = loc("reshape.1369")
#loc362 = loc("reshape.1349")
#loc363 = loc("reshape.1351")
#loc364 = loc("reshape.1331")
#loc365 = loc("reshape.1333")
#loc366 = loc("reshape.19")
#loc367 = loc("reshape.21")
#loc368 = loc("reshape.1457")
#loc369 = loc("reshape.1459")
#loc370 = loc("reshape.1439")
#loc371 = loc("reshape.1441")
#loc372 = loc("reshape.1421")
#loc373 = loc("reshape.1423")
#loc374 = loc("reshape.1541")
#loc375 = loc("reshape.1543")
#loc376 = loc("reshape.1523")
#loc377 = loc("reshape.1525")
#loc378 = loc("reshape.1505")
#loc379 = loc("reshape.1507")
#loc380 = loc("reshape.1625")
#loc381 = loc("reshape.1627")
#loc382 = loc("reshape.1607")
#loc383 = loc("reshape.1609")
#loc384 = loc("reshape.1589")
#loc385 = loc("reshape.1591")
#loc386 = loc("convolution.364")
#loc387 = loc("reshape.359")
#loc388 = loc("reshape.361")
#loc389 = loc("reshape.355")
#loc390 = loc("reshape.357")
#loc391 = loc("reshape.351")
#loc392 = loc("reshape.353")
#loc393 = loc("batch-norm-inference.365")
#loc394 = loc("maximum.372")
#loc395 = loc("convolution.373")
#loc396 = loc("reshape.341")
#loc397 = loc("reshape.343")
#loc398 = loc("reshape.337")
#loc399 = loc("reshape.339")
#loc400 = loc("reshape.333")
#loc401 = loc("reshape.335")
#loc402 = loc("batch-norm-inference.374")
#loc403 = loc("maximum.381")
#loc404 = loc("convolution.382")
#loc405 = loc("reshape.323")
#loc406 = loc("reshape.325")
#loc407 = loc("reshape.319")
#loc408 = loc("reshape.321")
#loc409 = loc("reshape.315")
#loc410 = loc("reshape.317")
#loc411 = loc("batch-norm-inference.383")
#loc412 = loc("convolution.304")
#loc413 = loc("reshape.98")
#loc414 = loc("reshape.100")
#loc415 = loc("reshape.94")
#loc416 = loc("reshape.96")
#loc417 = loc("reshape.90")
#loc418 = loc("reshape.92")
#loc419 = loc("batch-norm-inference.305")
#loc420 = loc("add.390")
#loc421 = loc("maximum.393")
#loc422 = loc("convolution.448")
#loc423 = loc("reshape.443")
#loc424 = loc("reshape.445")
#loc425 = loc("reshape.439")
#loc426 = loc("reshape.441")
#loc427 = loc("reshape.435")
#loc428 = loc("reshape.437")
#loc429 = loc("batch-norm-inference.449")
#loc430 = loc("maximum.456")
#loc431 = loc("convolution.457")
#loc432 = loc("reshape.425")
#loc433 = loc("reshape.427")
#loc434 = loc("reshape.421")
#loc435 = loc("reshape.423")
#loc436 = loc("reshape.417")
#loc437 = loc("reshape.419")
#loc438 = loc("batch-norm-inference.458")
#loc439 = loc("maximum.465")
#loc440 = loc("convolution.466")
#loc441 = loc("reshape.407")
#loc442 = loc("reshape.409")
#loc443 = loc("reshape.403")
#loc444 = loc("reshape.405")
#loc445 = loc("reshape.399")
#loc446 = loc("reshape.401")
#loc447 = loc("batch-norm-inference.467")
#loc448 = loc("add.474")
#loc449 = loc("maximum.477")
#loc450 = loc("convolution.532")
#loc451 = loc("reshape.527")
#loc452 = loc("reshape.529")
#loc453 = loc("reshape.523")
#loc454 = loc("reshape.525")
#loc455 = loc("reshape.519")
#loc456 = loc("reshape.521")
#loc457 = loc("batch-norm-inference.533")
#loc458 = loc("maximum.540")
#loc459 = loc("convolution.541")
#loc460 = loc("reshape.509")
#loc461 = loc("reshape.511")
#loc462 = loc("reshape.505")
#loc463 = loc("reshape.507")
#loc464 = loc("reshape.501")
#loc465 = loc("reshape.503")
#loc466 = loc("batch-norm-inference.542")
#loc467 = loc("maximum.549")
#loc468 = loc("convolution.550")
#loc469 = loc("reshape.491")
#loc470 = loc("reshape.493")
#loc471 = loc("reshape.487")
#loc472 = loc("reshape.489")
#loc473 = loc("reshape.483")
#loc474 = loc("reshape.485")
#loc475 = loc("batch-norm-inference.551")
#loc476 = loc("add.558")
#loc477 = loc("maximum.561")
#loc478 = loc("convolution.622")
#loc479 = loc("reshape.617")
#loc480 = loc("reshape.619")
#loc481 = loc("reshape.613")
#loc482 = loc("reshape.615")
#loc483 = loc("reshape.609")
#loc484 = loc("reshape.611")
#loc485 = loc("batch-norm-inference.623")
#loc486 = loc("maximum.630")
#loc487 = loc("convolution.631")
#loc488 = loc("reshape.599")
#loc489 = loc("reshape.601")
#loc490 = loc("reshape.595")
#loc491 = loc("reshape.597")
#loc492 = loc("reshape.591")
#loc493 = loc("reshape.593")
#loc494 = loc("batch-norm-inference.632")
#loc495 = loc("maximum.639")
#loc496 = loc("convolution.640")
#loc497 = loc("reshape.581")
#loc498 = loc("reshape.583")
#loc499 = loc("reshape.577")
#loc500 = loc("reshape.579")
#loc501 = loc("reshape.573")
#loc502 = loc("reshape.575")
#loc503 = loc("batch-norm-inference.641")
#loc504 = loc("convolution.562")
#loc505 = loc("reshape.77")
#loc506 = loc("reshape.79")
#loc507 = loc("reshape.73")
#loc508 = loc("reshape.75")
#loc509 = loc("reshape.69")
#loc510 = loc("reshape.71")
#loc511 = loc("batch-norm-inference.563")
#loc512 = loc("add.648")
#loc513 = loc("maximum.651")
#loc514 = loc("convolution.706")
#loc515 = loc("reshape.701")
#loc516 = loc("reshape.703")
#loc517 = loc("reshape.697")
#loc518 = loc("reshape.699")
#loc519 = loc("reshape.693")
#loc520 = loc("reshape.695")
#loc521 = loc("batch-norm-inference.707")
#loc522 = loc("maximum.714")
#loc523 = loc("convolution.715")
#loc524 = loc("reshape.683")
#loc525 = loc("reshape.685")
#loc526 = loc("reshape.679")
#loc527 = loc("reshape.681")
#loc528 = loc("reshape.675")
#loc529 = loc("reshape.677")
#loc530 = loc("batch-norm-inference.716")
#loc531 = loc("maximum.723")
#loc532 = loc("convolution.724")
#loc533 = loc("reshape.665")
#loc534 = loc("reshape.667")
#loc535 = loc("reshape.661")
#loc536 = loc("reshape.663")
#loc537 = loc("reshape.657")
#loc538 = loc("reshape.659")
#loc539 = loc("batch-norm-inference.725")
#loc540 = loc("add.732")
#loc541 = loc("maximum.735")
#loc542 = loc("convolution.790")
#loc543 = loc("reshape.785")
#loc544 = loc("reshape.787")
#loc545 = loc("reshape.781")
#loc546 = loc("reshape.783")
#loc547 = loc("reshape.777")
#loc548 = loc("reshape.779")
#loc549 = loc("batch-norm-inference.791")
#loc550 = loc("maximum.798")
#loc551 = loc("convolution.799")
#loc552 = loc("reshape.767")
#loc553 = loc("reshape.769")
#loc554 = loc("reshape.763")
#loc555 = loc("reshape.765")
#loc556 = loc("reshape.759")
#loc557 = loc("reshape.761")
#loc558 = loc("batch-norm-inference.800")
#loc559 = loc("maximum.807")
#loc560 = loc("convolution.808")
#loc561 = loc("reshape.749")
#loc562 = loc("reshape.751")
#loc563 = loc("reshape.745")
#loc564 = loc("reshape.747")
#loc565 = loc("reshape.741")
#loc566 = loc("reshape.743")
#loc567 = loc("batch-norm-inference.809")
#loc568 = loc("add.816")
#loc569 = loc("maximum.819")
#loc570 = loc("convolution.874")
#loc571 = loc("reshape.869")
#loc572 = loc("reshape.871")
#loc573 = loc("reshape.865")
#loc574 = loc("reshape.867")
#loc575 = loc("reshape.861")
#loc576 = loc("reshape.863")
#loc577 = loc("batch-norm-inference.875")
#loc578 = loc("maximum.882")
#loc579 = loc("convolution.883")
#loc580 = loc("reshape.851")
#loc581 = loc("reshape.853")
#loc582 = loc("reshape.847")
#loc583 = loc("reshape.849")
#loc584 = loc("reshape.843")
#loc585 = loc("reshape.845")
#loc586 = loc("batch-norm-inference.884")
#loc587 = loc("maximum.891")
#loc588 = loc("convolution.892")
#loc589 = loc("reshape.833")
#loc590 = loc("reshape.835")
#loc591 = loc("reshape.829")
#loc592 = loc("reshape.831")
#loc593 = loc("reshape.825")
#loc594 = loc("reshape.827")
#loc595 = loc("batch-norm-inference.893")
#loc596 = loc("add.900")
#loc597 = loc("maximum.903")
#loc598 = loc("convolution.964")
#loc599 = loc("reshape.959")
#loc600 = loc("reshape.961")
#loc601 = loc("reshape.955")
#loc602 = loc("reshape.957")
#loc603 = loc("reshape.951")
#loc604 = loc("reshape.953")
#loc605 = loc("batch-norm-inference.965")
#loc606 = loc("maximum.972")
#loc607 = loc("convolution.973")
#loc608 = loc("reshape.941")
#loc609 = loc("reshape.943")
#loc610 = loc("reshape.937")
#loc611 = loc("reshape.939")
#loc612 = loc("reshape.933")
#loc613 = loc("reshape.935")
#loc614 = loc("batch-norm-inference.974")
#loc615 = loc("maximum.981")
#loc616 = loc("convolution.982")
#loc617 = loc("reshape.923")
#loc618 = loc("reshape.925")
#loc619 = loc("reshape.919")
#loc620 = loc("reshape.921")
#loc621 = loc("reshape.915")
#loc622 = loc("reshape.917")
#loc623 = loc("batch-norm-inference.983")
#loc624 = loc("convolution.904")
#loc625 = loc("reshape.55")
#loc626 = loc("reshape.57")
#loc627 = loc("reshape.51")
#loc628 = loc("reshape.53")
#loc629 = loc("reshape.47")
#loc630 = loc("reshape.49")
#loc631 = loc("batch-norm-inference.905")
#loc632 = loc("add.990")
#loc633 = loc("maximum.993")
#loc634 = loc("convolution.1048")
#loc635 = loc("reshape.1043")
#loc636 = loc("reshape.1045")
#loc637 = loc("reshape.1039")
#loc638 = loc("reshape.1041")
#loc639 = loc("reshape.1035")
#loc640 = loc("reshape.1037")
#loc641 = loc("batch-norm-inference.1049")
#loc642 = loc("maximum.1056")
#loc643 = loc("convolution.1057")
#loc644 = loc("reshape.1025")
#loc645 = loc("reshape.1027")
#loc646 = loc("reshape.1021")
#loc647 = loc("reshape.1023")
#loc648 = loc("reshape.1017")
#loc649 = loc("reshape.1019")
#loc650 = loc("batch-norm-inference.1058")
#loc651 = loc("maximum.1065")
#loc652 = loc("convolution.1066")
#loc653 = loc("reshape.1007")
#loc654 = loc("reshape.1009")
#loc655 = loc("reshape.1003")
#loc656 = loc("reshape.1005")
#loc657 = loc("reshape.999")
#loc658 = loc("reshape.1001")
#loc659 = loc("batch-norm-inference.1067")
#loc660 = loc("add.1074")
#loc661 = loc("maximum.1077")
#loc662 = loc("convolution.1132")
#loc663 = loc("reshape.1127")
#loc664 = loc("reshape.1129")
#loc665 = loc("reshape.1123")
#loc666 = loc("reshape.1125")
#loc667 = loc("reshape.1119")
#loc668 = loc("reshape.1121")
#loc669 = loc("batch-norm-inference.1133")
#loc670 = loc("maximum.1140")
#loc671 = loc("convolution.1141")
#loc672 = loc("reshape.1109")
#loc673 = loc("reshape.1111")
#loc674 = loc("reshape.1105")
#loc675 = loc("reshape.1107")
#loc676 = loc("reshape.1101")
#loc677 = loc("reshape.1103")
#loc678 = loc("batch-norm-inference.1142")
#loc679 = loc("maximum.1149")
#loc680 = loc("convolution.1150")
#loc681 = loc("reshape.1091")
#loc682 = loc("reshape.1093")
#loc683 = loc("reshape.1087")
#loc684 = loc("reshape.1089")
#loc685 = loc("reshape.1083")
#loc686 = loc("reshape.1085")
#loc687 = loc("batch-norm-inference.1151")
#loc688 = loc("add.1158")
#loc689 = loc("maximum.1161")
#loc690 = loc("convolution.1216")
#loc691 = loc("reshape.1211")
#loc692 = loc("reshape.1213")
#loc693 = loc("reshape.1207")
#loc694 = loc("reshape.1209")
#loc695 = loc("reshape.1203")
#loc696 = loc("reshape.1205")
#loc697 = loc("batch-norm-inference.1217")
#loc698 = loc("maximum.1224")
#loc699 = loc("convolution.1225")
#loc700 = loc("reshape.1193")
#loc701 = loc("reshape.1195")
#loc702 = loc("reshape.1189")
#loc703 = loc("reshape.1191")
#loc704 = loc("reshape.1185")
#loc705 = loc("reshape.1187")
#loc706 = loc("batch-norm-inference.1226")
#loc707 = loc("maximum.1233")
#loc708 = loc("convolution.1234")
#loc709 = loc("reshape.1175")
#loc710 = loc("reshape.1177")
#loc711 = loc("reshape.1171")
#loc712 = loc("reshape.1173")
#loc713 = loc("reshape.1167")
#loc714 = loc("reshape.1169")
#loc715 = loc("batch-norm-inference.1235")
#loc716 = loc("add.1242")
#loc717 = loc("maximum.1245")
#loc718 = loc("convolution.1300")
#loc719 = loc("reshape.1295")
#loc720 = loc("reshape.1297")
#loc721 = loc("reshape.1291")
#loc722 = loc("reshape.1293")
#loc723 = loc("reshape.1287")
#loc724 = loc("reshape.1289")
#loc725 = loc("batch-norm-inference.1301")
#loc726 = loc("maximum.1308")
#loc727 = loc("convolution.1309")
#loc728 = loc("reshape.1277")
#loc729 = loc("reshape.1279")
#loc730 = loc("reshape.1273")
#loc731 = loc("reshape.1275")
#loc732 = loc("reshape.1269")
#loc733 = loc("reshape.1271")
#loc734 = loc("batch-norm-inference.1310")
#loc735 = loc("maximum.1317")
#loc736 = loc("convolution.1318")
#loc737 = loc("reshape.1259")
#loc738 = loc("reshape.1261")
#loc739 = loc("reshape.1255")
#loc740 = loc("reshape.1257")
#loc741 = loc("reshape.1251")
#loc742 = loc("reshape.1253")
#loc743 = loc("batch-norm-inference.1319")
#loc744 = loc("add.1326")
#loc745 = loc("maximum.1329")
#loc746 = loc("convolution.1384")
#loc747 = loc("reshape.1379")
#loc748 = loc("reshape.1381")
#loc749 = loc("reshape.1375")
#loc750 = loc("reshape.1377")
#loc751 = loc("reshape.1371")
#loc752 = loc("reshape.1373")
#loc753 = loc("batch-norm-inference.1385")
#loc754 = loc("maximum.1392")
#loc755 = loc("convolution.1393")
#loc756 = loc("reshape.1361")
#loc757 = loc("reshape.1363")
#loc758 = loc("reshape.1357")
#loc759 = loc("reshape.1359")
#loc760 = loc("reshape.1353")
#loc761 = loc("reshape.1355")
#loc762 = loc("batch-norm-inference.1394")
#loc763 = loc("maximum.1401")
#loc764 = loc("convolution.1402")
#loc765 = loc("reshape.1343")
#loc766 = loc("reshape.1345")
#loc767 = loc("reshape.1339")
#loc768 = loc("reshape.1341")
#loc769 = loc("reshape.1335")
#loc770 = loc("reshape.1337")
#loc771 = loc("batch-norm-inference.1403")
#loc772 = loc("add.1410")
#loc773 = loc("maximum.1413")
#loc774 = loc("convolution.1474")
#loc775 = loc("reshape.1469")
#loc776 = loc("reshape.1471")
#loc777 = loc("reshape.1465")
#loc778 = loc("reshape.1467")
#loc779 = loc("reshape.1461")
#loc780 = loc("reshape.1463")
#loc781 = loc("batch-norm-inference.1475")
#loc782 = loc("maximum.1482")
#loc783 = loc("convolution.1483")
#loc784 = loc("reshape.1451")
#loc785 = loc("reshape.1453")
#loc786 = loc("reshape.1447")
#loc787 = loc("reshape.1449")
#loc788 = loc("reshape.1443")
#loc789 = loc("reshape.1445")
#loc790 = loc("batch-norm-inference.1484")
#loc791 = loc("maximum.1491")
#loc792 = loc("convolution.1492")
#loc793 = loc("reshape.1433")
#loc794 = loc("reshape.1435")
#loc795 = loc("reshape.1429")
#loc796 = loc("reshape.1431")
#loc797 = loc("reshape.1425")
#loc798 = loc("reshape.1427")
#loc799 = loc("batch-norm-inference.1493")
#loc800 = loc("convolution.1414")
#loc801 = loc("reshape.31")
#loc802 = loc("reshape.33")
#loc803 = loc("reshape.27")
#loc804 = loc("reshape.29")
#loc805 = loc("reshape.23")
#loc806 = loc("reshape.25")
#loc807 = loc("batch-norm-inference.1415")
#loc808 = loc("add.1500")
#loc809 = loc("maximum.1503")
#loc810 = loc("convolution.1558")
#loc811 = loc("reshape.1553")
#loc812 = loc("reshape.1555")
#loc813 = loc("reshape.1549")
#loc814 = loc("reshape.1551")
#loc815 = loc("reshape.1545")
#loc816 = loc("reshape.1547")
#loc817 = loc("batch-norm-inference.1559")
#loc818 = loc("maximum.1566")
#loc819 = loc("convolution.1567")
#loc820 = loc("reshape.1535")
#loc821 = loc("reshape.1537")
#loc822 = loc("reshape.1531")
#loc823 = loc("reshape.1533")
#loc824 = loc("reshape.1527")
#loc825 = loc("reshape.1529")
#loc826 = loc("batch-norm-inference.1568")
#loc827 = loc("maximum.1575")
#loc828 = loc("convolution.1576")
#loc829 = loc("reshape.1517")
#loc830 = loc("reshape.1519")
#loc831 = loc("reshape.1513")
#loc832 = loc("reshape.1515")
#loc833 = loc("reshape.1509")
#loc834 = loc("reshape.1511")
#loc835 = loc("batch-norm-inference.1577")
#loc836 = loc("add.1584")
#loc837 = loc("maximum.1587")
#loc838 = loc("convolution.1642")
#loc839 = loc("reshape.1637")
#loc840 = loc("reshape.1639")
#loc841 = loc("reshape.1633")
#loc842 = loc("reshape.1635")
#loc843 = loc("reshape.1629")
#loc844 = loc("reshape.1631")
#loc845 = loc("batch-norm-inference.1643")
#loc846 = loc("maximum.1650")
#loc847 = loc("convolution.1651")
#loc848 = loc("reshape.1619")
#loc849 = loc("reshape.1621")
#loc850 = loc("reshape.1615")
#loc851 = loc("reshape.1617")
#loc852 = loc("reshape.1611")
#loc853 = loc("reshape.1613")
#loc854 = loc("batch-norm-inference.1652")
#loc855 = loc("maximum.1659")
#loc856 = loc("convolution.1660")
#loc857 = loc("reshape.1601")
#loc858 = loc("reshape.1603")
#loc859 = loc("reshape.1597")
#loc860 = loc("reshape.1599")
#loc861 = loc("reshape.1593")
#loc862 = loc("reshape.1595")
#loc863 = loc("batch-norm-inference.1661")
#loc864 = loc("add.1668")
#loc865 = loc("maximum.1671")
#loc866 = loc("reduce.1678")
#loc867 = loc("multiply.1687")
#loc868 = loc("reshape.11")
#loc869 = loc("reshape.13")
#loc870 = loc("transpose.14")
#loc871 = loc("dot.1690")
#loc872 = loc("reshape.4")
#loc873 = loc("broadcast.1695")
#loc874 = loc("add.1696")
