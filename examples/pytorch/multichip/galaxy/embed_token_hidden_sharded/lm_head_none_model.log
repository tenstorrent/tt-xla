2025-12-04 19:54:13.849 (   0.000s) [        CE2D1B80]   plugin_attributes.cc:58       1| PluginAttributes::PJRT_Plugin_Initialize
2025-12-04 19:54:13.849 (   0.000s) [        CE2D1B80]     client_instance.cc:648      1| ClientInstance::PJRT_Client_Create
2025-12-04 19:54:13.851 (   0.002s) [        CE2D1B80]     client_instance.cc:182      1| ClientInstance::ClientInstance
2025-12-04 19:54:13.851 (   0.002s) [        CE2D1B80]     client_instance.cc:203      1| ClientInstance::Initialize
2025-12-04 19:55:14.789 (  60.940s) [        CE2D1B80]              stubs.inc:103   WARN| STUB: PJRT_Client_TopologyDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]      error_instance.cc:52       1| ErrorInstance::PJRT_Error_Message
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]      error_instance.cc:61       1| ErrorInstance::PJRT_Error_GetCode
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]      error_instance.cc:46       1| ErrorInstance::PJRT_Error_Destroy
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     client_instance.cc:702      1| ClientInstance::PJRT_Client_PlatformVersion
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     client_instance.cc:683      1| ClientInstance::PJRT_Client_PlatformName
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     client_instance.cc:713      1| ClientInstance::PJRT_Client_Devices
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.940s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     client_instance.cc:726      1| ClientInstance::PJRT_Client_AddressableDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     client_instance.cc:776      1| ClientInstance::PJRT_Client_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]   plugin_attributes.cc:64       1| PluginAttributes::PJRT_Plugin_Attributes
2025-12-04 19:55:14.790608: W torch_xla/csrc/runtime/profiler.cpp:88] Profiler API not found for PJRT plugin
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.790 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.941s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.791 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.942s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:14.792 (  60.943s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
Using PJRT plugin directory: /home/hshah/tt-xla/python_package/pjrt_plugin_tt
Using TT-Metal from the source tree: /home/hshah/tt-xla/third_party/tt-mlir/src/tt-mlir/third_party/tt-metal/src/tt-metal
WARNING: TT plugin is setting XLA_STABLEHLO_COMPILE to 1. This is required for TT PJRT plugin to work correctly.
Setting up SPMD...
SPMD setup complete.
Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 30/30 [00:00<00:00, 2691.94it/s]
Some weights of the model checkpoint at meta-llama/Meta-Llama-3.1-70B were not used when initializing LlamaForCausalLM: ['model.layers.1.input_layernorm.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.32.input_layernorm.weight', 'model.layers.32.mlp.down_proj.weight', 'model.layers.32.mlp.gate_proj.weight', 'model.layers.32.mlp.up_proj.weight', 'model.layers.32.post_attention_layernorm.weight', 'model.layers.32.self_attn.k_proj.weight', 'model.layers.32.self_attn.o_proj.weight', 'model.layers.32.self_attn.q_proj.weight', 'model.layers.32.self_attn.v_proj.weight', 'model.layers.33.input_layernorm.weight', 'model.layers.33.mlp.down_proj.weight', 'model.layers.33.mlp.gate_proj.weight', 'model.layers.33.mlp.up_proj.weight', 'model.layers.33.post_attention_layernorm.weight', 'model.layers.33.self_attn.k_proj.weight', 'model.layers.33.self_attn.o_proj.weight', 'model.layers.33.self_attn.q_proj.weight', 'model.layers.33.self_attn.v_proj.weight', 'model.layers.34.input_layernorm.weight', 'model.layers.34.mlp.down_proj.weight', 'model.layers.34.mlp.gate_proj.weight', 'model.layers.34.mlp.up_proj.weight', 'model.layers.34.post_attention_layernorm.weight', 'model.layers.34.self_attn.k_proj.weight', 'model.layers.34.self_attn.o_proj.weight', 'model.layers.34.self_attn.q_proj.weight', 'model.layers.34.self_attn.v_proj.weight', 'model.layers.35.input_layernorm.weight', 'model.layers.35.mlp.down_proj.weight', 'model.layers.35.mlp.gate_proj.weight', 'model.layers.35.mlp.up_proj.weight', 'model.layers.35.post_attention_layernorm.weight', 'model.layers.35.self_attn.k_proj.weight', 'model.layers.35.self_attn.o_proj.weight', 'model.layers.35.self_attn.q_proj.weight', 'model.layers.35.self_attn.v_proj.weight', 'model.layers.36.input_layernorm.weight', 'model.layers.36.mlp.down_proj.weight', 'model.layers.36.mlp.gate_proj.weight', 'model.layers.36.mlp.up_proj.weight', 'model.layers.36.post_attention_layernorm.weight', 'model.layers.36.self_attn.k_proj.weight', 'model.layers.36.self_attn.o_proj.weight', 'model.layers.36.self_attn.q_proj.weight', 'model.layers.36.self_attn.v_proj.weight', 'model.layers.37.input_layernorm.weight', 'model.layers.37.mlp.down_proj.weight', 'model.layers.37.mlp.gate_proj.weight', 'model.layers.37.mlp.up_proj.weight', 'model.layers.37.post_attention_layernorm.weight', 'model.layers.37.self_attn.k_proj.weight', 'model.layers.37.self_attn.o_proj.weight', 'model.layers.37.self_attn.q_proj.weight', 'model.layers.37.self_attn.v_proj.weight', 'model.layers.38.input_layernorm.weight', 'model.layers.38.mlp.down_proj.weight', 'model.layers.38.mlp.gate_proj.weight', 'model.layers.38.mlp.up_proj.weight', 'model.layers.38.post_attention_layernorm.weight', 'model.layers.38.self_attn.k_proj.weight', 'model.layers.38.self_attn.o_proj.weight', 'model.layers.38.self_attn.q_proj.weight', 'model.layers.38.self_attn.v_proj.weight', 'model.layers.39.input_layernorm.weight', 'model.layers.39.mlp.down_proj.weight', 'model.layers.39.mlp.gate_proj.weight', 'model.layers.39.mlp.up_proj.weight', 'model.layers.39.post_attention_layernorm.weight', 'model.layers.39.self_attn.k_proj.weight', 'model.layers.39.self_attn.o_proj.weight', 'model.layers.39.self_attn.q_proj.weight', 'model.layers.39.self_attn.v_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.40.input_layernorm.weight', 'model.layers.40.mlp.down_proj.weight', 'model.layers.40.mlp.gate_proj.weight', 'model.layers.40.mlp.up_proj.weight', 'model.layers.40.post_attention_layernorm.weight', 'model.layers.40.self_attn.k_proj.weight', 'model.layers.40.self_attn.o_proj.weight', 'model.layers.40.self_attn.q_proj.weight', 'model.layers.40.self_attn.v_proj.weight', 'model.layers.41.input_layernorm.weight', 'model.layers.41.mlp.down_proj.weight', 'model.layers.41.mlp.gate_proj.weight', 'model.layers.41.mlp.up_proj.weight', 'model.layers.41.post_attention_layernorm.weight', 'model.layers.41.self_attn.k_proj.weight', 'model.layers.41.self_attn.o_proj.weight', 'model.layers.41.self_attn.q_proj.weight', 'model.layers.41.self_attn.v_proj.weight', 'model.layers.42.input_layernorm.weight', 'model.layers.42.mlp.down_proj.weight', 'model.layers.42.mlp.gate_proj.weight', 'model.layers.42.mlp.up_proj.weight', 'model.layers.42.post_attention_layernorm.weight', 'model.layers.42.self_attn.k_proj.weight', 'model.layers.42.self_attn.o_proj.weight', 'model.layers.42.self_attn.q_proj.weight', 'model.layers.42.self_attn.v_proj.weight', 'model.layers.43.input_layernorm.weight', 'model.layers.43.mlp.down_proj.weight', 'model.layers.43.mlp.gate_proj.weight', 'model.layers.43.mlp.up_proj.weight', 'model.layers.43.post_attention_layernorm.weight', 'model.layers.43.self_attn.k_proj.weight', 'model.layers.43.self_attn.o_proj.weight', 'model.layers.43.self_attn.q_proj.weight', 'model.layers.43.self_attn.v_proj.weight', 'model.layers.44.input_layernorm.weight', 'model.layers.44.mlp.down_proj.weight', 'model.layers.44.mlp.gate_proj.weight', 'model.layers.44.mlp.up_proj.weight', 'model.layers.44.post_attention_layernorm.weight', 'model.layers.44.self_attn.k_proj.weight', 'model.layers.44.self_attn.o_proj.weight', 'model.layers.44.self_attn.q_proj.weight', 'model.layers.44.self_attn.v_proj.weight', 'model.layers.45.input_layernorm.weight', 'model.layers.45.mlp.down_proj.weight', 'model.layers.45.mlp.gate_proj.weight', 'model.layers.45.mlp.up_proj.weight', 'model.layers.45.post_attention_layernorm.weight', 'model.layers.45.self_attn.k_proj.weight', 'model.layers.45.self_attn.o_proj.weight', 'model.layers.45.self_attn.q_proj.weight', 'model.layers.45.self_attn.v_proj.weight', 'model.layers.46.input_layernorm.weight', 'model.layers.46.mlp.down_proj.weight', 'model.layers.46.mlp.gate_proj.weight', 'model.layers.46.mlp.up_proj.weight', 'model.layers.46.post_attention_layernorm.weight', 'model.layers.46.self_attn.k_proj.weight', 'model.layers.46.self_attn.o_proj.weight', 'model.layers.46.self_attn.q_proj.weight', 'model.layers.46.self_attn.v_proj.weight', 'model.layers.47.input_layernorm.weight', 'model.layers.47.mlp.down_proj.weight', 'model.layers.47.mlp.gate_proj.weight', 'model.layers.47.mlp.up_proj.weight', 'model.layers.47.post_attention_layernorm.weight', 'model.layers.47.self_attn.k_proj.weight', 'model.layers.47.self_attn.o_proj.weight', 'model.layers.47.self_attn.q_proj.weight', 'model.layers.47.self_attn.v_proj.weight', 'model.layers.48.input_layernorm.weight', 'model.layers.48.mlp.down_proj.weight', 'model.layers.48.mlp.gate_proj.weight', 'model.layers.48.mlp.up_proj.weight', 'model.layers.48.post_attention_layernorm.weight', 'model.layers.48.self_attn.k_proj.weight', 'model.layers.48.self_attn.o_proj.weight', 'model.layers.48.self_attn.q_proj.weight', 'model.layers.48.self_attn.v_proj.weight', 'model.layers.49.input_layernorm.weight', 'model.layers.49.mlp.down_proj.weight', 'model.layers.49.mlp.gate_proj.weight', 'model.layers.49.mlp.up_proj.weight', 'model.layers.49.post_attention_layernorm.weight', 'model.layers.49.self_attn.k_proj.weight', 'model.layers.49.self_attn.o_proj.weight', 'model.layers.49.self_attn.q_proj.weight', 'model.layers.49.self_attn.v_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.50.input_layernorm.weight', 'model.layers.50.mlp.down_proj.weight', 'model.layers.50.mlp.gate_proj.weight', 'model.layers.50.mlp.up_proj.weight', 'model.layers.50.post_attention_layernorm.weight', 'model.layers.50.self_attn.k_proj.weight', 'model.layers.50.self_attn.o_proj.weight', 'model.layers.50.self_attn.q_proj.weight', 'model.layers.50.self_attn.v_proj.weight', 'model.layers.51.input_layernorm.weight', 'model.layers.51.mlp.down_proj.weight', 'model.layers.51.mlp.gate_proj.weight', 'model.layers.51.mlp.up_proj.weight', 'model.layers.51.post_attention_layernorm.weight', 'model.layers.51.self_attn.k_proj.weight', 'model.layers.51.self_attn.o_proj.weight', 'model.layers.51.self_attn.q_proj.weight', 'model.layers.51.self_attn.v_proj.weight', 'model.layers.52.input_layernorm.weight', 'model.layers.52.mlp.down_proj.weight', 'model.layers.52.mlp.gate_proj.weight', 'model.layers.52.mlp.up_proj.weight', 'model.layers.52.post_attention_layernorm.weight', 'model.layers.52.self_attn.k_proj.weight', 'model.layers.52.self_attn.o_proj.weight', 'model.layers.52.self_attn.q_proj.weight', 'model.layers.52.self_attn.v_proj.weight', 'model.layers.53.input_layernorm.weight', 'model.layers.53.mlp.down_proj.weight', 'model.layers.53.mlp.gate_proj.weight', 'model.layers.53.mlp.up_proj.weight', 'model.layers.53.post_attention_layernorm.weight', 'model.layers.53.self_attn.k_proj.weight', 'model.layers.53.self_attn.o_proj.weight', 'model.layers.53.self_attn.q_proj.weight', 'model.layers.53.self_attn.v_proj.weight', 'model.layers.54.input_layernorm.weight', 'model.layers.54.mlp.down_proj.weight', 'model.layers.54.mlp.gate_proj.weight', 'model.layers.54.mlp.up_proj.weight', 'model.layers.54.post_attention_layernorm.weight', 'model.layers.54.self_attn.k_proj.weight', 'model.layers.54.self_attn.o_proj.weight', 'model.layers.54.self_attn.q_proj.weight', 'model.layers.54.self_attn.v_proj.weight', 'model.layers.55.input_layernorm.weight', 'model.layers.55.mlp.down_proj.weight', 'model.layers.55.mlp.gate_proj.weight', 'model.layers.55.mlp.up_proj.weight', 'model.layers.55.post_attention_layernorm.weight', 'model.layers.55.self_attn.k_proj.weight', 'model.layers.55.self_attn.o_proj.weight', 'model.layers.55.self_attn.q_proj.weight', 'model.layers.55.self_attn.v_proj.weight', 'model.layers.56.input_layernorm.weight', 'model.layers.56.mlp.down_proj.weight', 'model.layers.56.mlp.gate_proj.weight', 'model.layers.56.mlp.up_proj.weight', 'model.layers.56.post_attention_layernorm.weight', 'model.layers.56.self_attn.k_proj.weight', 'model.layers.56.self_attn.o_proj.weight', 'model.layers.56.self_attn.q_proj.weight', 'model.layers.56.self_attn.v_proj.weight', 'model.layers.57.input_layernorm.weight', 'model.layers.57.mlp.down_proj.weight', 'model.layers.57.mlp.gate_proj.weight', 'model.layers.57.mlp.up_proj.weight', 'model.layers.57.post_attention_layernorm.weight', 'model.layers.57.self_attn.k_proj.weight', 'model.layers.57.self_attn.o_proj.weight', 'model.layers.57.self_attn.q_proj.weight', 'model.layers.57.self_attn.v_proj.weight', 'model.layers.58.input_layernorm.weight', 'model.layers.58.mlp.down_proj.weight', 'model.layers.58.mlp.gate_proj.weight', 'model.layers.58.mlp.up_proj.weight', 'model.layers.58.post_attention_layernorm.weight', 'model.layers.58.self_attn.k_proj.weight', 'model.layers.58.self_attn.o_proj.weight', 'model.layers.58.self_attn.q_proj.weight', 'model.layers.58.self_attn.v_proj.weight', 'model.layers.59.input_layernorm.weight', 'model.layers.59.mlp.down_proj.weight', 'model.layers.59.mlp.gate_proj.weight', 'model.layers.59.mlp.up_proj.weight', 'model.layers.59.post_attention_layernorm.weight', 'model.layers.59.self_attn.k_proj.weight', 'model.layers.59.self_attn.o_proj.weight', 'model.layers.59.self_attn.q_proj.weight', 'model.layers.59.self_attn.v_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.60.input_layernorm.weight', 'model.layers.60.mlp.down_proj.weight', 'model.layers.60.mlp.gate_proj.weight', 'model.layers.60.mlp.up_proj.weight', 'model.layers.60.post_attention_layernorm.weight', 'model.layers.60.self_attn.k_proj.weight', 'model.layers.60.self_attn.o_proj.weight', 'model.layers.60.self_attn.q_proj.weight', 'model.layers.60.self_attn.v_proj.weight', 'model.layers.61.input_layernorm.weight', 'model.layers.61.mlp.down_proj.weight', 'model.layers.61.mlp.gate_proj.weight', 'model.layers.61.mlp.up_proj.weight', 'model.layers.61.post_attention_layernorm.weight', 'model.layers.61.self_attn.k_proj.weight', 'model.layers.61.self_attn.o_proj.weight', 'model.layers.61.self_attn.q_proj.weight', 'model.layers.61.self_attn.v_proj.weight', 'model.layers.62.input_layernorm.weight', 'model.layers.62.mlp.down_proj.weight', 'model.layers.62.mlp.gate_proj.weight', 'model.layers.62.mlp.up_proj.weight', 'model.layers.62.post_attention_layernorm.weight', 'model.layers.62.self_attn.k_proj.weight', 'model.layers.62.self_attn.o_proj.weight', 'model.layers.62.self_attn.q_proj.weight', 'model.layers.62.self_attn.v_proj.weight', 'model.layers.63.input_layernorm.weight', 'model.layers.63.mlp.down_proj.weight', 'model.layers.63.mlp.gate_proj.weight', 'model.layers.63.mlp.up_proj.weight', 'model.layers.63.post_attention_layernorm.weight', 'model.layers.63.self_attn.k_proj.weight', 'model.layers.63.self_attn.o_proj.weight', 'model.layers.63.self_attn.q_proj.weight', 'model.layers.63.self_attn.v_proj.weight', 'model.layers.64.input_layernorm.weight', 'model.layers.64.mlp.down_proj.weight', 'model.layers.64.mlp.gate_proj.weight', 'model.layers.64.mlp.up_proj.weight', 'model.layers.64.post_attention_layernorm.weight', 'model.layers.64.self_attn.k_proj.weight', 'model.layers.64.self_attn.o_proj.weight', 'model.layers.64.self_attn.q_proj.weight', 'model.layers.64.self_attn.v_proj.weight', 'model.layers.65.input_layernorm.weight', 'model.layers.65.mlp.down_proj.weight', 'model.layers.65.mlp.gate_proj.weight', 'model.layers.65.mlp.up_proj.weight', 'model.layers.65.post_attention_layernorm.weight', 'model.layers.65.self_attn.k_proj.weight', 'model.layers.65.self_attn.o_proj.weight', 'model.layers.65.self_attn.q_proj.weight', 'model.layers.65.self_attn.v_proj.weight', 'model.layers.66.input_layernorm.weight', 'model.layers.66.mlp.down_proj.weight', 'model.layers.66.mlp.gate_proj.weight', 'model.layers.66.mlp.up_proj.weight', 'model.layers.66.post_attention_layernorm.weight', 'model.layers.66.self_attn.k_proj.weight', 'model.layers.66.self_attn.o_proj.weight', 'model.layers.66.self_attn.q_proj.weight', 'model.layers.66.self_attn.v_proj.weight', 'model.layers.67.input_layernorm.weight', 'model.layers.67.mlp.down_proj.weight', 'model.layers.67.mlp.gate_proj.weight', 'model.layers.67.mlp.up_proj.weight', 'model.layers.67.post_attention_layernorm.weight', 'model.layers.67.self_attn.k_proj.weight', 'model.layers.67.self_attn.o_proj.weight', 'model.layers.67.self_attn.q_proj.weight', 'model.layers.67.self_attn.v_proj.weight', 'model.layers.68.input_layernorm.weight', 'model.layers.68.mlp.down_proj.weight', 'model.layers.68.mlp.gate_proj.weight', 'model.layers.68.mlp.up_proj.weight', 'model.layers.68.post_attention_layernorm.weight', 'model.layers.68.self_attn.k_proj.weight', 'model.layers.68.self_attn.o_proj.weight', 'model.layers.68.self_attn.q_proj.weight', 'model.layers.68.self_attn.v_proj.weight', 'model.layers.69.input_layernorm.weight', 'model.layers.69.mlp.down_proj.weight', 'model.layers.69.mlp.gate_proj.weight', 'model.layers.69.mlp.up_proj.weight', 'model.layers.69.post_attention_layernorm.weight', 'model.layers.69.self_attn.k_proj.weight', 'model.layers.69.self_attn.o_proj.weight', 'model.layers.69.self_attn.q_proj.weight', 'model.layers.69.self_attn.v_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.70.input_layernorm.weight', 'model.layers.70.mlp.down_proj.weight', 'model.layers.70.mlp.gate_proj.weight', 'model.layers.70.mlp.up_proj.weight', 'model.layers.70.post_attention_layernorm.weight', 'model.layers.70.self_attn.k_proj.weight', 'model.layers.70.self_attn.o_proj.weight', 'model.layers.70.self_attn.q_proj.weight', 'model.layers.70.self_attn.v_proj.weight', 'model.layers.71.input_layernorm.weight', 'model.layers.71.mlp.down_proj.weight', 'model.layers.71.mlp.gate_proj.weight', 'model.layers.71.mlp.up_proj.weight', 'model.layers.71.post_attention_layernorm.weight', 'model.layers.71.self_attn.k_proj.weight', 'model.layers.71.self_attn.o_proj.weight', 'model.layers.71.self_attn.q_proj.weight', 'model.layers.71.self_attn.v_proj.weight', 'model.layers.72.input_layernorm.weight', 'model.layers.72.mlp.down_proj.weight', 'model.layers.72.mlp.gate_proj.weight', 'model.layers.72.mlp.up_proj.weight', 'model.layers.72.post_attention_layernorm.weight', 'model.layers.72.self_attn.k_proj.weight', 'model.layers.72.self_attn.o_proj.weight', 'model.layers.72.self_attn.q_proj.weight', 'model.layers.72.self_attn.v_proj.weight', 'model.layers.73.input_layernorm.weight', 'model.layers.73.mlp.down_proj.weight', 'model.layers.73.mlp.gate_proj.weight', 'model.layers.73.mlp.up_proj.weight', 'model.layers.73.post_attention_layernorm.weight', 'model.layers.73.self_attn.k_proj.weight', 'model.layers.73.self_attn.o_proj.weight', 'model.layers.73.self_attn.q_proj.weight', 'model.layers.73.self_attn.v_proj.weight', 'model.layers.74.input_layernorm.weight', 'model.layers.74.mlp.down_proj.weight', 'model.layers.74.mlp.gate_proj.weight', 'model.layers.74.mlp.up_proj.weight', 'model.layers.74.post_attention_layernorm.weight', 'model.layers.74.self_attn.k_proj.weight', 'model.layers.74.self_attn.o_proj.weight', 'model.layers.74.self_attn.q_proj.weight', 'model.layers.74.self_attn.v_proj.weight', 'model.layers.75.input_layernorm.weight', 'model.layers.75.mlp.down_proj.weight', 'model.layers.75.mlp.gate_proj.weight', 'model.layers.75.mlp.up_proj.weight', 'model.layers.75.post_attention_layernorm.weight', 'model.layers.75.self_attn.k_proj.weight', 'model.layers.75.self_attn.o_proj.weight', 'model.layers.75.self_attn.q_proj.weight', 'model.layers.75.self_attn.v_proj.weight', 'model.layers.76.input_layernorm.weight', 'model.layers.76.mlp.down_proj.weight', 'model.layers.76.mlp.gate_proj.weight', 'model.layers.76.mlp.up_proj.weight', 'model.layers.76.post_attention_layernorm.weight', 'model.layers.76.self_attn.k_proj.weight', 'model.layers.76.self_attn.o_proj.weight', 'model.layers.76.self_attn.q_proj.weight', 'model.layers.76.self_attn.v_proj.weight', 'model.layers.77.input_layernorm.weight', 'model.layers.77.mlp.down_proj.weight', 'model.layers.77.mlp.gate_proj.weight', 'model.layers.77.mlp.up_proj.weight', 'model.layers.77.post_attention_layernorm.weight', 'model.layers.77.self_attn.k_proj.weight', 'model.layers.77.self_attn.o_proj.weight', 'model.layers.77.self_attn.q_proj.weight', 'model.layers.77.self_attn.v_proj.weight', 'model.layers.78.input_layernorm.weight', 'model.layers.78.mlp.down_proj.weight', 'model.layers.78.mlp.gate_proj.weight', 'model.layers.78.mlp.up_proj.weight', 'model.layers.78.post_attention_layernorm.weight', 'model.layers.78.self_attn.k_proj.weight', 'model.layers.78.self_attn.o_proj.weight', 'model.layers.78.self_attn.q_proj.weight', 'model.layers.78.self_attn.v_proj.weight', 'model.layers.79.input_layernorm.weight', 'model.layers.79.mlp.down_proj.weight', 'model.layers.79.mlp.gate_proj.weight', 'model.layers.79.mlp.up_proj.weight', 'model.layers.79.post_attention_layernorm.weight', 'model.layers.79.self_attn.k_proj.weight', 'model.layers.79.self_attn.o_proj.weight', 'model.layers.79.self_attn.q_proj.weight', 'model.layers.79.self_attn.v_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.602 (  62.753s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.603 (  62.753s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.603 (  62.753s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.603 (  62.753s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.603 (  62.754s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.603 (  62.754s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.603 (  62.754s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.603 (  62.754s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.603 (  62.754s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.603 (  62.754s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.603 (  62.754s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.603 (  62.754s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.603 (  62.754s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.603 (  62.754s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.603 (  62.754s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.603 (  62.754s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.603 (  62.754s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.603 (  62.754s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.603 (  62.754s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.603 (  62.754s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.603 (  62.754s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.603 (  62.754s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.603 (  62.754s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.603 (  62.754s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.604 (  62.754s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.604 (  62.754s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.604 (  62.754s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.604 (  62.754s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.604 (  62.754s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.604 (  62.754s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.604 (  62.754s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.604 (  62.754s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.604 (  62.754s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.604 (  62.754s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.604 (  62.754s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.604 (  62.754s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.604 (  62.754s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.604 (  62.754s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.604 (  62.754s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.604 (  62.754s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.604 (  62.754s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.604 (  62.755s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.605 (  62.755s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.605 (  62.756s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.606 (  62.756s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.606 (  62.757s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.607 (  62.757s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.607 (  62.758s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.654 (  62.805s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.655 (  62.805s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.655 (  62.805s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.655 (  62.805s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.655 (  62.806s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.656 (  62.806s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.656 (  62.806s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.656 (  62.806s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.656 (  62.806s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.656 (  62.806s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.656 (  62.806s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.656 (  62.806s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.656 (  62.806s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.656 (  62.806s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.656 (  62.806s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.656 (  62.806s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.656 (  62.806s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.656 (  62.806s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.656 (  62.806s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.656 (  62.806s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.656 (  62.806s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.656 (  62.806s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.656 (  62.806s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.656 (  62.806s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.656 (  62.806s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.656 (  62.806s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.656 (  62.806s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.656 (  62.807s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.657 (  62.807s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.657 (  62.807s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.657 (  62.807s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.657 (  62.807s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.657 (  62.807s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.657 (  62.807s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.657 (  62.807s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.657 (  62.807s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.657 (  62.807s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.657 (  62.807s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.657 (  62.807s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.657 (  62.807s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.657 (  62.807s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.657 (  62.807s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.657 (  62.807s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.657 (  62.807s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.657 (  62.807s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.657 (  62.807s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.657 (  62.807s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.657 (  62.807s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.657 (  62.808s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.657 (  62.808s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.657 (  62.808s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.657 (  62.808s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.657 (  62.808s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.657 (  62.808s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.657 (  62.808s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.691 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.691 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.691 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.691 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.691 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.691 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.691 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.691 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.691 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.691 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.691 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.691 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.691 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.691 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.691 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.691 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.691 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.691 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.692 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.692 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.692 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.692 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.692 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.692 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.692 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.692 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.692 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.692 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.692 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.692 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.692 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.692 (  62.842s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.740 (  62.891s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.740 (  62.891s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.740 (  62.891s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.740 (  62.891s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.740 (  62.891s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.740 (  62.891s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.740 (  62.891s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.740 (  62.891s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.740 (  62.891s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.740 (  62.891s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.740 (  62.891s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.740 (  62.891s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.740 (  62.891s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.740 (  62.891s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.740 (  62.891s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.740 (  62.891s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.740 (  62.891s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.740 (  62.891s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.740 (  62.891s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.741 (  62.891s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.741 (  62.891s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.741 (  62.891s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.741 (  62.891s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.741 (  62.891s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.741 (  62.891s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.741 (  62.891s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.741 (  62.891s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.741 (  62.891s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.741 (  62.891s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.741 (  62.891s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.741 (  62.891s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.741 (  62.891s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.741 (  62.891s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.741 (  62.891s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.741 (  62.891s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.741 (  62.891s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.741 (  62.891s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.741 (  62.891s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.741 (  62.891s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.741 (  62.891s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.741 (  62.892s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.742 (  62.892s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.742 (  62.892s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.742 (  62.892s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.742 (  62.892s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.742 (  62.892s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.742 (  62.892s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.742 (  62.892s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.742 (  62.892s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.742 (  62.892s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.742 (  62.892s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.742 (  62.892s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.742 (  62.892s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.742 (  62.892s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.742 (  62.892s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.742 (  62.892s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.742 (  62.892s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.742 (  62.892s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.742 (  62.892s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.742 (  62.892s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.742 (  62.892s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.742 (  62.892s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.742 (  62.892s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.742 (  62.893s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.782 (  62.933s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.830 (  62.981s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.830 (  62.981s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.830 (  62.981s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.831 (  62.981s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.831 (  62.981s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.831 (  62.981s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.831 (  62.981s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.831 (  62.981s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.831 (  62.981s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.831 (  62.981s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.831 (  62.981s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.831 (  62.981s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.831 (  62.981s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.831 (  62.981s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.831 (  62.981s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.831 (  62.981s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.831 (  62.981s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.831 (  62.981s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.831 (  62.982s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.832 (  62.982s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.832 (  62.982s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.832 (  62.982s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.832 (  62.982s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.832 (  62.982s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.832 (  62.982s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.832 (  62.982s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.832 (  62.982s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.832 (  62.982s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.832 (  62.982s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.832 (  62.982s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.832 (  62.982s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.832 (  62.982s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.832 (  62.982s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.832 (  62.982s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.832 (  62.983s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.833 (  62.983s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.833 (  62.983s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.833 (  62.983s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.833 (  62.983s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.833 (  62.983s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.833 (  62.983s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.833 (  62.983s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.833 (  62.983s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.833 (  62.983s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.833 (  62.983s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.833 (  62.983s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.833 (  62.983s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.833 (  62.983s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.833 (  62.983s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.833 (  62.983s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.833 (  62.983s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.833 (  62.983s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.833 (  62.983s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.833 (  62.984s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.833 (  62.984s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.833 (  62.984s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.833 (  62.984s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.833 (  62.984s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.833 (  62.984s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.833 (  62.984s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.833 (  62.984s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.833 (  62.984s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.833 (  62.984s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.833 (  62.984s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.833 (  62.984s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.833 (  62.984s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.833 (  62.984s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.872 (  63.022s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.879 (  63.030s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.879 (  63.030s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.879 (  63.030s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.879 (  63.030s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.880 (  63.030s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.880 (  63.030s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.880 (  63.030s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.880 (  63.030s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.880 (  63.030s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.880 (  63.030s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.880 (  63.030s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.880 (  63.030s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.880 (  63.030s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.880 (  63.030s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.880 (  63.030s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.880 (  63.030s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.880 (  63.030s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.880 (  63.030s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.880 (  63.031s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.881 (  63.031s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.881 (  63.031s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.881 (  63.031s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.881 (  63.031s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.881 (  63.031s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.881 (  63.031s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.881 (  63.031s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.881 (  63.031s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.881 (  63.031s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.881 (  63.031s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.881 (  63.031s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.881 (  63.031s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.881 (  63.031s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.881 (  63.031s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.881 (  63.031s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.881 (  63.031s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.881 (  63.031s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.881 (  63.032s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.882 (  63.032s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.882 (  63.032s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.882 (  63.032s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.882 (  63.032s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.882 (  63.032s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.882 (  63.032s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.882 (  63.032s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.882 (  63.032s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.882 (  63.032s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.882 (  63.032s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.882 (  63.032s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.882 (  63.032s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.882 (  63.032s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.882 (  63.032s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.882 (  63.032s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.882 (  63.032s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.882 (  63.032s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.882 (  63.032s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.882 (  63.032s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.882 (  63.032s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.882 (  63.033s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.882 (  63.033s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.882 (  63.033s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.882 (  63.033s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.882 (  63.033s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.882 (  63.033s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.882 (  63.033s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.892 (  63.043s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.895 (  63.045s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.895 (  63.045s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.895 (  63.045s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.895 (  63.045s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.895 (  63.045s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.895 (  63.045s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.895 (  63.045s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.895 (  63.045s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.895 (  63.045s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.895 (  63.045s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.895 (  63.045s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.895 (  63.045s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.895 (  63.045s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.895 (  63.045s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.895 (  63.046s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.896 (  63.046s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.896 (  63.046s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.896 (  63.046s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.896 (  63.046s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.896 (  63.046s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.896 (  63.046s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.896 (  63.046s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.896 (  63.046s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.896 (  63.046s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.896 (  63.046s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.896 (  63.046s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.896 (  63.046s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.896 (  63.046s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.896 (  63.046s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.896 (  63.046s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.896 (  63.046s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.896 (  63.046s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.896 (  63.046s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.896 (  63.046s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.896 (  63.046s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.896 (  63.047s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.897 (  63.047s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.897 (  63.047s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.897 (  63.047s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.897 (  63.047s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.897 (  63.047s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.897 (  63.047s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.897 (  63.047s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.897 (  63.047s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.897 (  63.047s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.897 (  63.047s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.897 (  63.047s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.897 (  63.047s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.897 (  63.047s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.897 (  63.048s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.900 (  63.050s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.900 (  63.050s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.900 (  63.050s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.900 (  63.051s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.901 (  63.051s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.901 (  63.051s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.901 (  63.051s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.901 (  63.051s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.901 (  63.051s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.901 (  63.051s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.901 (  63.051s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.901 (  63.051s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.901 (  63.051s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.901 (  63.051s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.901 (  63.051s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.901 (  63.051s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.901 (  63.051s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.901 (  63.051s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.901 (  63.051s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.901 (  63.052s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.902 (  63.052s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.902 (  63.052s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.902 (  63.052s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.902 (  63.052s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.902 (  63.052s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.902 (  63.052s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.902 (  63.052s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.902 (  63.052s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.902 (  63.052s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.902 (  63.052s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.902 (  63.052s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.902 (  63.052s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.902 (  63.052s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.902 (  63.052s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.902 (  63.052s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.902 (  63.053s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.903 (  63.053s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.903 (  63.053s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.903 (  63.053s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.903 (  63.053s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.903 (  63.053s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.903 (  63.054s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.909 (  63.060s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.909 (  63.060s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.909 (  63.060s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.909 (  63.060s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.909 (  63.060s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.909 (  63.060s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.909 (  63.060s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.909 (  63.060s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.909 (  63.060s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.909 (  63.060s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.909 (  63.060s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.909 (  63.060s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.909 (  63.060s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.910 (  63.060s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.910 (  63.060s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.910 (  63.060s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.910 (  63.060s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.910 (  63.060s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.910 (  63.060s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.910 (  63.060s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.910 (  63.060s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.910 (  63.060s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.910 (  63.060s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.910 (  63.060s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.910 (  63.060s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.910 (  63.060s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.910 (  63.060s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.910 (  63.060s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.910 (  63.061s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.911 (  63.061s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.911 (  63.061s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.911 (  63.061s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.911 (  63.061s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.911 (  63.061s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.911 (  63.061s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.911 (  63.061s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.911 (  63.061s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.911 (  63.061s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.911 (  63.061s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.911 (  63.061s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.911 (  63.061s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.911 (  63.061s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.911 (  63.061s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.911 (  63.061s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.911 (  63.062s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.912 (  63.062s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.912 (  63.062s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.912 (  63.062s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.912 (  63.062s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.912 (  63.062s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.912 (  63.062s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.912 (  63.062s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.912 (  63.062s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.912 (  63.062s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.912 (  63.062s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.912 (  63.062s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.912 (  63.062s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:16.912 (  63.063s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:16.925 (  63.076s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:18.370 (  64.521s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.370 (  64.521s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.370 (  64.521s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.371 (  64.521s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.371 (  64.521s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.371 (  64.521s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.371 (  64.521s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.371 (  64.521s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.371 (  64.521s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.371 (  64.521s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.371 (  64.521s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.371 (  64.521s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.371 (  64.521s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.371 (  64.521s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.371 (  64.521s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.371 (  64.521s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.371 (  64.521s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.371 (  64.521s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.371 (  64.522s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.372 (  64.522s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.372 (  64.522s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.372 (  64.522s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.372 (  64.522s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.372 (  64.522s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.372 (  64.522s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.372 (  64.522s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.372 (  64.522s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.372 (  64.522s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.372 (  64.522s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.372 (  64.522s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.372 (  64.522s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.372 (  64.522s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.372 (  64.522s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.372 (  64.522s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.372 (  64.522s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.372 (  64.522s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.372 (  64.522s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.372 (  64.522s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.372 (  64.522s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.372 (  64.522s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.372 (  64.522s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.372 (  64.522s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.372 (  64.523s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.373 (  64.523s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.373 (  64.523s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.373 (  64.523s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.373 (  64.523s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.373 (  64.523s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.373 (  64.523s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.373 (  64.523s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:18.373 (  64.523s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:18.373 (  64.523s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:18.373 (  64.523s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:18.373 (  64.523s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:18.373 (  64.523s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:19.136 (  65.286s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.286s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:19.136 (  65.287s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:20.667 (  66.817s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.667 (  66.817s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.667 (  66.817s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.667 (  66.818s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.668 (  66.818s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.668 (  66.818s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.668 (  66.818s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.668 (  66.818s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.668 (  66.818s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.668 (  66.818s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.668 (  66.818s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.668 (  66.818s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.668 (  66.818s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.668 (  66.818s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.668 (  66.818s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.668 (  66.818s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.668 (  66.818s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.668 (  66.818s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.668 (  66.818s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.668 (  66.818s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.668 (  66.818s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.668 (  66.818s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.668 (  66.818s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.668 (  66.818s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.668 (  66.819s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.669 (  66.819s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.669 (  66.819s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.669 (  66.819s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.669 (  66.819s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.669 (  66.819s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.669 (  66.819s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.669 (  66.819s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.669 (  66.819s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.669 (  66.819s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.669 (  66.819s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.669 (  66.819s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.669 (  66.819s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.669 (  66.819s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.669 (  66.819s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.669 (  66.819s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.669 (  66.819s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.669 (  66.819s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.669 (  66.819s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.669 (  66.819s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.669 (  66.819s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.669 (  66.820s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.669 (  66.820s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.669 (  66.820s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.669 (  66.820s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.669 (  66.820s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.669 (  66.820s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.669 (  66.820s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.669 (  66.820s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.669 (  66.820s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.669 (  66.820s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.669 (  66.820s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.669 (  66.820s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:20.669 (  66.820s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:20.669 (  66.820s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:20.669 (  66.820s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:20.669 (  66.820s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:20.669 (  66.820s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.069s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.919 (  69.070s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.920 (  69.070s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.920 (  69.070s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.920 (  69.070s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.920 (  69.070s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.920 (  69.070s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.920 (  69.070s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.920 (  69.070s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.920 (  69.070s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.920 (  69.070s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.920 (  69.070s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.920 (  69.070s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.920 (  69.070s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.920 (  69.070s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.920 (  69.070s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.920 (  69.070s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.920 (  69.070s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.920 (  69.070s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.920 (  69.070s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.920 (  69.070s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.920 (  69.070s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.920 (  69.070s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.920 (  69.070s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.920 (  69.071s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.921 (  69.071s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.921 (  69.072s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.922 (  69.072s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.922 (  69.072s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.922 (  69.072s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.922 (  69.072s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.922 (  69.072s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.922 (  69.072s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.922 (  69.072s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.922 (  69.072s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.922 (  69.072s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.922 (  69.072s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.922 (  69.072s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.922 (  69.072s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.922 (  69.072s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.922 (  69.072s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.922 (  69.072s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.922 (  69.072s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.922 (  69.072s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.922 (  69.072s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.922 (  69.072s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.922 (  69.072s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.922 (  69.072s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.922 (  69.072s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.922 (  69.072s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.922 (  69.073s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.923 (  69.073s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.923 (  69.073s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.923 (  69.073s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.923 (  69.073s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.923 (  69.073s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.923 (  69.073s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.923 (  69.073s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.923 (  69.073s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.923 (  69.073s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.923 (  69.073s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.923 (  69.073s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.923 (  69.073s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.923 (  69.073s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.923 (  69.073s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.923 (  69.073s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.923 (  69.073s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.923 (  69.073s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.923 (  69.073s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.923 (  69.073s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.923 (  69.073s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.923 (  69.073s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.923 (  69.073s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.923 (  69.074s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.924 (  69.074s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.924 (  69.074s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.924 (  69.074s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.924 (  69.074s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.924 (  69.074s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.924 (  69.074s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.924 (  69.074s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.924 (  69.074s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.924 (  69.074s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.924 (  69.074s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.924 (  69.074s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.924 (  69.074s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.924 (  69.074s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.924 (  69.074s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.924 (  69.074s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.924 (  69.074s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.924 (  69.074s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.924 (  69.074s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.924 (  69.075s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.925 (  69.075s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.925 (  69.075s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.925 (  69.075s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.925 (  69.075s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.925 (  69.075s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.925 (  69.075s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.925 (  69.075s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.925 (  69.075s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.925 (  69.075s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.925 (  69.075s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.925 (  69.075s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.925 (  69.075s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.925 (  69.075s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.925 (  69.075s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.925 (  69.075s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.925 (  69.075s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.925 (  69.075s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.925 (  69.075s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.925 (  69.075s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.925 (  69.075s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.925 (  69.076s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.926 (  69.077s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.927 (  69.077s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.927 (  69.077s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.927 (  69.077s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.927 (  69.077s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.927 (  69.077s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.927 (  69.077s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.927 (  69.077s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.927 (  69.077s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.927 (  69.077s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.927 (  69.077s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.927 (  69.077s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.927 (  69.077s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.927 (  69.077s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.927 (  69.077s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.927 (  69.077s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.927 (  69.077s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.927 (  69.077s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.927 (  69.077s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.927 (  69.077s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.927 (  69.077s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.927 (  69.077s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.927 (  69.077s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.927 (  69.077s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.927 (  69.078s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.928 (  69.078s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]     client_instance.cc:832      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-12-04 19:55:22.928 (  69.079s) [        CE2D1B80]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.934 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.085s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.935 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.086s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.087s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.087s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.087s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.087s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.087s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.087s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.087s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.087s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.087s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.087s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.936 (  69.087s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.937 (  69.087s) [        CE2D1B80]     buffer_instance.cc:610      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-12-04 19:55:22.954 (  69.105s) [        CE2D1B80]     client_instance.cc:789      1| ClientInstance::PJRT_Client_Compile
2025-12-04 19:55:22.954 (  69.105s) [        CE2D1B80]      module_builder.cc:210      1| ModuleBuilder::buildModule
2025-12-04 19:55:22.956 (  69.106s) [        CE2D1B80]      module_builder.cc:971      1| MLIR Module vhlo:
#loc1 = loc("p0.1")
#loc2 = loc("p1.9")
#loc3 = loc("p2.14")
#loc4 = loc("p3.50")
#loc5 = loc("p4.70")
#loc6 = loc("p5.87")
#loc7 = loc("p6.116")
#loc8 = loc("p7.125")
#loc9 = loc("p8.130")
#loc10 = loc("p9.139")
#loc11 = loc("p10.208")
#loc12 = loc("p11.248")
#loc13 = loc("p12.338")
#loc14 = loc("p13.347")
#loc15 = loc("p14.393")
#loc31 = loc("reduce.30")
#loc123 = loc("reduce.286")
#loc128 = loc("reduce.295")
#loc151 = loc("reduce.318")
#loc188 = loc("reduce.373")
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8, \22_axis_1\22=4]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1> loc("p0.1"), %arg1: !vhlo.tensor_v1<1x7x!vhlo.i64_v1> loc("p1.9"), %arg2: !vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1> loc("p2.14"), %arg3: !vhlo.tensor_v1<8192x!vhlo.bf16_v1> loc("p3.50"), %arg4: !vhlo.tensor_v1<64x!vhlo.f32_v1> loc("p4.70"), %arg5: !vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1> loc("p5.87"), %arg6: !vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1> loc("p6.116"), %arg7: !vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1> loc("p7.125"), %arg8: !vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1> loc("p8.130"), %arg9: !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1> loc("p9.139"), %arg10: !vhlo.tensor_v1<1x7x!vhlo.i64_v1> loc("p10.208"), %arg11: !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1> loc("p11.248"), %arg12: !vhlo.tensor_v1<8192x!vhlo.bf16_v1> loc("p12.338"), %arg13: !vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1> loc("p13.347"), %arg14: !vhlo.tensor_v1<8192x!vhlo.bf16_v1> loc("p14.393")) -> (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>>}> : () -> !vhlo.tensor_v1<7x!vhlo.i64_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.22070313E-4> : tensor<1x7xf32>>}> : () -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1> loc(#loc)
    %5 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x7x1xf32>>}> : () -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1> loc(#loc)
    %6 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>>}> : () -> !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1> loc(#loc)
    %7 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<8.837890e-02> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc)
    %8 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1> : tensor<i64>>}> : () -> !vhlo.tensor_v1<!vhlo.i64_v1> loc(#loc)
    %9 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc)
    %10 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<-3.389530e+38> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc)
    %11 = "vhlo.broadcast_in_dim_v1"(%10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1> loc(#loc)
    %12 = "vhlo.broadcast_in_dim_v1"(%9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1> loc(#loc)
    %13 = "vhlo.broadcast_in_dim_v1"(%9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.bf16_v1> loc(#loc)
    %14 = "vhlo.broadcast_in_dim_v1"(%10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.bf16_v1> loc(#loc)
    %15 = "vhlo.broadcast_in_dim_v1"(%8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.i64_v1> loc(#loc)
    %16 = "vhlo.broadcast_in_dim_v1"(%7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1> loc(#loc)
    %17 = "vhlo.broadcast_in_dim_v1"(%3) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1> loc(#loc)
    %18 = "vhlo.reshape_v1"(%arg3) : (!vhlo.tensor_v1<8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1> loc(#loc16)
    %19 = "vhlo.custom_call_v1"(%18) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___input_layernorm_weight">}>} : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1> loc(#loc17)
    %20 = "vhlo.reshape_v1"(%19) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x!vhlo.bf16_v1> loc(#loc18)
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1> loc(#loc19)
    %22 = "vhlo.reshape_v1"(%arg2) : (!vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1> loc(#loc20)
    %23 = "vhlo.custom_call_v1"(%22) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_embed_tokens_weight">}>} : (!vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1> loc(#loc21)
    %24 = "vhlo.reshape_v1"(%23) : (!vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1> loc(#loc22)
    %25 = "vhlo.reshape_v1"(%arg1) : (!vhlo.tensor_v1<1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1> loc(#loc23)
    %26 = "vhlo.custom_call_v1"(%25) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1> loc(#loc24)
    %27 = "vhlo.reshape_v1"(%26) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1> loc(#loc25)
    %28 = "vhlo.convert_v1"(%27) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.ui32_v1> loc(#loc26)
    %29 = "vhlo.gather_v2"(%24, %28) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 8192]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1> loc(#loc27)
    %30 = "vhlo.reshape_v1"(%29) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1> loc(#loc28)
    %31 = "vhlo.convert_v1"(%30) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1> loc(#loc29)
    %32 = "vhlo.power_v1"(%31, %17) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1> loc(#loc30)
    %33 = "vhlo.reduce_v1"(%32, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg15: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.30"), %arg16: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.30")):
      %203 = "vhlo.add_v1"(%arg15, %arg16) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc32)
      "vhlo.return_v1"(%203) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1> loc(#loc31)
    %34 = "vhlo.multiply_v1"(%33, %4) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1> loc(#loc33)
    %35 = "vhlo.reshape_v1"(%34) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1> loc(#loc34)
    %36 = "vhlo.add_v1"(%35, %5) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1> loc(#loc35)
    %37 = "vhlo.rsqrt_v2"(%36) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1> loc(#loc36)
    %38 = "vhlo.reshape_v1"(%37) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1> loc(#loc37)
    %39 = "vhlo.broadcast_in_dim_v1"(%38) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1> loc(#loc38)
    %40 = "vhlo.multiply_v1"(%31, %39) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1> loc(#loc39)
    %41 = "vhlo.convert_v1"(%40) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1> loc(#loc40)
    %42 = "vhlo.multiply_v1"(%21, %41) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1> loc(#loc41)
    %43 = "vhlo.reshape_v1"(%42) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1> loc(#loc42)
    %44 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1> loc(#loc43)
    %45 = "vhlo.custom_call_v1"(%44) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1> loc(#loc44)
    %46 = "vhlo.reshape_v1"(%45) : (!vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1> loc(#loc45)
    %47 = "vhlo.transpose_v1"(%46) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,1024]{0,1}">} : (!vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x1024x!vhlo.bf16_v1> loc(#loc46)
    %48 = "vhlo.dot_general_v2"(%43, %47) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1> loc(#loc47)
    %49 = "vhlo.reshape_v1"(%48) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1> loc(#loc48)
    %50 = "vhlo.transpose_v1"(%49) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1> loc(#loc49)
    %51 = "vhlo.reshape_v1"(%arg5) : (!vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1> loc(#loc50)
    %52 = "vhlo.custom_call_v1"(%51) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1> loc(#loc51)
    %53 = "vhlo.reshape_v1"(%52) : (!vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1> loc(#loc52)
    %54 = "vhlo.transpose_v1"(%53) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,1024]{0,1}">} : (!vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x1024x!vhlo.bf16_v1> loc(#loc53)
    %55 = "vhlo.dot_general_v2"(%43, %54) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1> loc(#loc54)
    %56 = "vhlo.reshape_v1"(%55) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1> loc(#loc55)
    %57 = "vhlo.transpose_v1"(%56) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1> loc(#loc56)
    %58 = "vhlo.reshape_v1"(%arg4) : (!vhlo.tensor_v1<64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1> loc(#loc57)
    %59 = "vhlo.custom_call_v1"(%58) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"constant">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_rotary_emb_inv_freq">}>} : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1> loc(#loc58)
    %60 = "vhlo.reshape_v1"(%59) : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1> loc(#loc59)
    %61 = "vhlo.dot_general_v2"(%60, %6) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x!vhlo.f32_v1> loc(#loc60)
    %62 = "vhlo.transpose_v1"(%61) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1]> : tensor<3xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,7,64]{1,2,0}">} : (!vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1> loc(#loc61)
    %63 = "vhlo.concatenate_v1"(%62, %62) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1> loc(#loc62)
    %64 = "vhlo.cosine_v2"(%63) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1> loc(#loc63)
    %65 = "vhlo.convert_v1"(%64) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1> loc(#loc64)
    %66 = "vhlo.broadcast_in_dim_v1"(%65) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1> loc(#loc65)
    %67 = "vhlo.multiply_v1"(%57, %66) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1> loc(#loc66)
    %68 = "vhlo.slice_v1"(%57) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1> loc(#loc67)
    %69 = "vhlo.negate_v1"(%68) : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1> loc(#loc68)
    %70 = "vhlo.slice_v1"(%57) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1> loc(#loc69)
    %71 = "vhlo.concatenate_v1"(%69, %70) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1> loc(#loc70)
    %72 = "vhlo.sine_v2"(%63) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1> loc(#loc71)
    %73 = "vhlo.convert_v1"(%72) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1> loc(#loc72)
    %74 = "vhlo.broadcast_in_dim_v1"(%73) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1> loc(#loc73)
    %75 = "vhlo.multiply_v1"(%71, %74) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1> loc(#loc74)
    %76 = "vhlo.add_v1"(%67, %75) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1> loc(#loc75)
    %77 = "vhlo.reshape_v1"(%arg14) : (!vhlo.tensor_v1<8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1> loc(#loc76)
    %78 = "vhlo.custom_call_v1"(%77) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_norm_weight">}>} : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1> loc(#loc77)
    %79 = "vhlo.reshape_v1"(%78) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x!vhlo.bf16_v1> loc(#loc78)
    %80 = "vhlo.broadcast_in_dim_v1"(%79) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1> loc(#loc79)
    %81 = "vhlo.reshape_v1"(%arg11) : (!vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1> loc(#loc80)
    %82 = "vhlo.custom_call_v1"(%81) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1> loc(#loc81)
    %83 = "vhlo.reshape_v1"(%82) : (!vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1> loc(#loc82)
    %84 = "vhlo.transpose_v1"(%83) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,8192]{0,1}">} : (!vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1> loc(#loc83)
    %85 = "vhlo.dot_general_v2"(%43, %84) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1> loc(#loc84)
    %86 = "vhlo.reshape_v1"(%85) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x64x128x!vhlo.bf16_v1> loc(#loc85)
    %87 = "vhlo.transpose_v1"(%86) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,64,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x64x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1> loc(#loc86)
    %88 = "vhlo.broadcast_in_dim_v1"(%65) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1> loc(#loc87)
    %89 = "vhlo.multiply_v1"(%87, %88) : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1> loc(#loc88)
    %90 = "vhlo.slice_v1"(%87) <{limit_indices = #vhlo.tensor_v1<dense<[1, 64, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x64x!vhlo.bf16_v1> loc(#loc89)
    %91 = "vhlo.negate_v1"(%90) : (!vhlo.tensor_v1<1x64x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x64x!vhlo.bf16_v1> loc(#loc90)
    %92 = "vhlo.slice_v1"(%87) <{limit_indices = #vhlo.tensor_v1<dense<[1, 64, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x64x!vhlo.bf16_v1> loc(#loc91)
    %93 = "vhlo.concatenate_v1"(%91, %92) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x64x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1> loc(#loc92)
    %94 = "vhlo.broadcast_in_dim_v1"(%73) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1> loc(#loc93)
    %95 = "vhlo.multiply_v1"(%93, %94) : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1> loc(#loc94)
    %96 = "vhlo.add_v1"(%89, %95) : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1> loc(#loc95)
    %97 = "vhlo.broadcast_in_dim_v1"(%76) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x8x7x128x!vhlo.bf16_v1> loc(#loc96)
    %98 = "vhlo.reshape_v1"(%97) : (!vhlo.tensor_v1<1x8x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1> loc(#loc97)
    %99 = "vhlo.transpose_v1"(%98) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,64,128,7]{2,3,1,0}">} : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x128x7x!vhlo.bf16_v1> loc(#loc98)
    %100 = "vhlo.dot_general_v2"(%96, %99) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x128x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1> loc(#loc99)
    %101 = "vhlo.multiply_v1"(%100, %16) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1> loc(#loc100)
    %102 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.i64_v1> loc(#loc101)
    %103 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.i64_v1> loc(#loc102)
    %104 = "vhlo.subtract_v1"(%102, %103) : (!vhlo.tensor_v1<7x7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.i64_v1> loc(#loc103)
    %105 = "vhlo.compare_v1"(%104, %15) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GE>}> : (!vhlo.tensor_v1<7x7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.bool_v1> loc(#loc104)
    %106 = "vhlo.select_v1"(%105, %14, %13) : (!vhlo.tensor_v1<7x7x!vhlo.bool_v1>, !vhlo.tensor_v1<7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.bf16_v1> loc(#loc105)
    %107 = "vhlo.compare_v1"(%102, %103) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<7x7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.bool_v1> loc(#loc106)
    %108 = "vhlo.convert_v1"(%107) : (!vhlo.tensor_v1<7x7x!vhlo.bool_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.bf16_v1> loc(#loc107)
    %109 = "vhlo.multiply_v1"(%106, %108) : (!vhlo.tensor_v1<7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.bf16_v1> loc(#loc108)
    %110 = "vhlo.reshape_v1"(%109) : (!vhlo.tensor_v1<7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1> loc(#loc109)
    %111 = "vhlo.reshape_v1"(%arg10) : (!vhlo.tensor_v1<1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1> loc(#loc110)
    %112 = "vhlo.custom_call_v1"(%111) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_1">}>} : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1> loc(#loc111)
    %113 = "vhlo.reshape_v1"(%112) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x7x!vhlo.i64_v1> loc(#loc112)
    %114 = "vhlo.convert_v1"(%113) : (!vhlo.tensor_v1<1x1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x7x!vhlo.bf16_v1> loc(#loc113)
    %115 = "vhlo.reshape_v1"(%114) : (!vhlo.tensor_v1<1x1x1x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.bf16_v1> loc(#loc114)
    %116 = "vhlo.broadcast_in_dim_v1"(%115) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1> loc(#loc115)
    %117 = "vhlo.add_v1"(%110, %116) : (!vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1> loc(#loc116)
    %118 = "vhlo.compare_v1"(%117, %12) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x7x!vhlo.bool_v1> loc(#loc117)
    %119 = "vhlo.select_v1"(%118, %11, %110) : (!vhlo.tensor_v1<1x1x7x7x!vhlo.bool_v1>, !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1> loc(#loc118)
    %120 = "vhlo.reshape_v1"(%119) : (!vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x7x!vhlo.bf16_v1> loc(#loc119)
    %121 = "vhlo.broadcast_in_dim_v1"(%120) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1> loc(#loc120)
    %122 = "vhlo.add_v1"(%101, %121) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1> loc(#loc121)
    %123 = "vhlo.convert_v1"(%122) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1> loc(#loc122)
    %124 = "vhlo.reduce_v1"(%123, %2) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg15: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.286"), %arg16: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.286")):
      %203 = "vhlo.maximum_v1"(%arg15, %arg16) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc124)
      "vhlo.return_v1"(%203) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x!vhlo.f32_v1> loc(#loc123)
    %125 = "vhlo.broadcast_in_dim_v1"(%124) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1> loc(#loc125)
    %126 = "vhlo.subtract_v1"(%123, %125) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1> loc(#loc126)
    %127 = "vhlo.exponential_v2"(%126) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1> loc(#loc127)
    %128 = "vhlo.reduce_v1"(%127, %0) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg15: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.295"), %arg16: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.295")):
      %203 = "vhlo.add_v1"(%arg15, %arg16) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc129)
      "vhlo.return_v1"(%203) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x!vhlo.f32_v1> loc(#loc128)
    %129 = "vhlo.broadcast_in_dim_v1"(%128) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1> loc(#loc130)
    %130 = "vhlo.divide_v1"(%127, %129) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1> loc(#loc131)
    %131 = "vhlo.convert_v1"(%130) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1> loc(#loc132)
    %132 = "vhlo.broadcast_in_dim_v1"(%50) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x8x7x128x!vhlo.bf16_v1> loc(#loc133)
    %133 = "vhlo.reshape_v1"(%132) : (!vhlo.tensor_v1<1x8x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1> loc(#loc134)
    %134 = "vhlo.dot_general_v2"(%131, %133) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1> loc(#loc135)
    %135 = "vhlo.transpose_v1"(%134) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,7,64,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x64x128x!vhlo.bf16_v1> loc(#loc136)
    %136 = "vhlo.reshape_v1"(%135) : (!vhlo.tensor_v1<1x7x64x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1> loc(#loc137)
    %137 = "vhlo.reshape_v1"(%arg9) : (!vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1> loc(#loc138)
    %138 = "vhlo.custom_call_v1"(%137) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_o_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1> loc(#loc139)
    %139 = "vhlo.reshape_v1"(%138) : (!vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1> loc(#loc140)
    %140 = "vhlo.transpose_v1"(%139) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,8192]{0,1}">} : (!vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1> loc(#loc141)
    %141 = "vhlo.dot_general_v2"(%136, %140) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1> loc(#loc142)
    %142 = "vhlo.reshape_v1"(%141) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1> loc(#loc143)
    %143 = "vhlo.add_v1"(%30, %142) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1> loc(#loc144)
    %144 = "vhlo.reshape_v1"(%arg12) : (!vhlo.tensor_v1<8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1> loc(#loc145)
    %145 = "vhlo.custom_call_v1"(%144) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___post_attention_layernorm_weight">}>} : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1> loc(#loc146)
    %146 = "vhlo.reshape_v1"(%145) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x!vhlo.bf16_v1> loc(#loc147)
    %147 = "vhlo.broadcast_in_dim_v1"(%146) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1> loc(#loc148)
    %148 = "vhlo.convert_v1"(%143) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1> loc(#loc149)
    %149 = "vhlo.power_v1"(%148, %17) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1> loc(#loc150)
    %150 = "vhlo.reduce_v1"(%149, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg15: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.318"), %arg16: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.318")):
      %203 = "vhlo.add_v1"(%arg15, %arg16) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc152)
      "vhlo.return_v1"(%203) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1> loc(#loc151)
    %151 = "vhlo.multiply_v1"(%150, %4) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1> loc(#loc153)
    %152 = "vhlo.reshape_v1"(%151) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1> loc(#loc154)
    %153 = "vhlo.add_v1"(%152, %5) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1> loc(#loc155)
    %154 = "vhlo.rsqrt_v2"(%153) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1> loc(#loc156)
    %155 = "vhlo.reshape_v1"(%154) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1> loc(#loc157)
    %156 = "vhlo.broadcast_in_dim_v1"(%155) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1> loc(#loc158)
    %157 = "vhlo.multiply_v1"(%148, %156) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1> loc(#loc159)
    %158 = "vhlo.convert_v1"(%157) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1> loc(#loc160)
    %159 = "vhlo.multiply_v1"(%147, %158) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1> loc(#loc161)
    %160 = "vhlo.reshape_v1"(%159) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1> loc(#loc162)
    %161 = "vhlo.reshape_v1"(%arg13) : (!vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1> loc(#loc163)
    %162 = "vhlo.custom_call_v1"(%161) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_gate_proj_weight">}>} : (!vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1> loc(#loc164)
    %163 = "vhlo.reshape_v1"(%162) : (!vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1> loc(#loc165)
    %164 = "vhlo.transpose_v1"(%163) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,28672]{0,1}">} : (!vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1> loc(#loc166)
    %165 = "vhlo.dot_general_v2"(%160, %164) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x28672x!vhlo.bf16_v1> loc(#loc167)
    %166 = "vhlo.reshape_v1"(%165) : (!vhlo.tensor_v1<7x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1> loc(#loc168)
    %167 = "vhlo.logistic_v2"(%166) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1> loc(#loc169)
    %168 = "vhlo.multiply_v1"(%166, %167) : (!vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1> loc(#loc170)
    %169 = "vhlo.reshape_v1"(%arg8) : (!vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1> loc(#loc171)
    %170 = "vhlo.custom_call_v1"(%169) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_up_proj_weight">}>} : (!vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1> loc(#loc172)
    %171 = "vhlo.reshape_v1"(%170) : (!vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1> loc(#loc173)
    %172 = "vhlo.transpose_v1"(%171) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,28672]{0,1}">} : (!vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1> loc(#loc174)
    %173 = "vhlo.dot_general_v2"(%160, %172) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x28672x!vhlo.bf16_v1> loc(#loc175)
    %174 = "vhlo.reshape_v1"(%173) : (!vhlo.tensor_v1<7x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1> loc(#loc176)
    %175 = "vhlo.multiply_v1"(%168, %174) : (!vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1> loc(#loc177)
    %176 = "vhlo.reshape_v1"(%175) : (!vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x28672x!vhlo.bf16_v1> loc(#loc178)
    %177 = "vhlo.reshape_v1"(%arg7) : (!vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x28672x!vhlo.bf16_v1> loc(#loc179)
    %178 = "vhlo.custom_call_v1"(%177) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_down_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x28672x!vhlo.bf16_v1> loc(#loc180)
    %179 = "vhlo.reshape_v1"(%178) : (!vhlo.tensor_v1<1x8192x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1> loc(#loc181)
    %180 = "vhlo.transpose_v1"(%179) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[28672,8192]{0,1}">} : (!vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1> loc(#loc182)
    %181 = "vhlo.dot_general_v2"(%176, %180) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x28672x!vhlo.bf16_v1>, !vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1> loc(#loc183)
    %182 = "vhlo.reshape_v1"(%181) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1> loc(#loc184)
    %183 = "vhlo.add_v1"(%143, %182) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1> loc(#loc185)
    %184 = "vhlo.convert_v1"(%183) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1> loc(#loc186)
    %185 = "vhlo.power_v1"(%184, %17) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1> loc(#loc187)
    %186 = "vhlo.reduce_v1"(%185, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg15: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.373"), %arg16: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.373")):
      %203 = "vhlo.add_v1"(%arg15, %arg16) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc189)
      "vhlo.return_v1"(%203) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1> loc(#loc188)
    %187 = "vhlo.multiply_v1"(%186, %4) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1> loc(#loc190)
    %188 = "vhlo.reshape_v1"(%187) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1> loc(#loc191)
    %189 = "vhlo.add_v1"(%188, %5) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1> loc(#loc192)
    %190 = "vhlo.rsqrt_v2"(%189) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1> loc(#loc193)
    %191 = "vhlo.reshape_v1"(%190) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1> loc(#loc194)
    %192 = "vhlo.broadcast_in_dim_v1"(%191) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1> loc(#loc195)
    %193 = "vhlo.multiply_v1"(%184, %192) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1> loc(#loc196)
    %194 = "vhlo.convert_v1"(%193) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1> loc(#loc197)
    %195 = "vhlo.multiply_v1"(%80, %194) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1> loc(#loc198)
    %196 = "vhlo.reshape_v1"(%195) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1> loc(#loc199)
    %197 = "vhlo.reshape_v1"(%arg6) : (!vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1> loc(#loc200)
    %198 = "vhlo.custom_call_v1"(%197) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___lm_head_weight">}>} : (!vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1> loc(#loc201)
    %199 = "vhlo.reshape_v1"(%198) : (!vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1> loc(#loc202)
    %200 = "vhlo.transpose_v1"(%199) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,128256]{0,1}">} : (!vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x128256x!vhlo.bf16_v1> loc(#loc203)
    %201 = "vhlo.dot_general_v2"(%196, %200) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1> loc(#loc204)
    %202 = "vhlo.reshape_v1"(%201) : (!vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1> loc(#loc205)
    "vhlo.return_v1"(%50, %76, %202) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,4]<=[32]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,8,4]<=[32] last_tile_dim_replicate}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,4]<=[32]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,8,4]<=[32] last_tile_dim_replicate}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[4,8]<=[8,4]T(1,0)}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,4]<=[32]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[4,8]<=[8,4]T(1,0)}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,4]<=[32]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,4]<=[32]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">} loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc16 = loc("reshape.51")
#loc17 = loc("custom-call.52")
#loc18 = loc("reshape.53")
#loc19 = loc("broadcast.54")
#loc20 = loc("reshape.15")
#loc21 = loc("custom-call.16")
#loc22 = loc("reshape.17")
#loc23 = loc("reshape.10")
#loc24 = loc("custom-call.11")
#loc25 = loc("reshape.13")
#loc26 = loc("convert.18")
#loc27 = loc("gather.19")
#loc28 = loc("reshape.20")
#loc29 = loc("convert.21")
#loc30 = loc("power.23")
#loc32 = loc("add.29")
#loc33 = loc("multiply.39")
#loc34 = loc("reshape.40")
#loc35 = loc("add.44")
#loc36 = loc("rsqrt.45")
#loc37 = loc("reshape.46")
#loc38 = loc("broadcast.47")
#loc39 = loc("multiply.48")
#loc40 = loc("convert.49")
#loc41 = loc("multiply.55")
#loc42 = loc("reshape.56")
#loc43 = loc("reshape.2")
#loc44 = loc("custom-call.3")
#loc45 = loc("reshape.4")
#loc46 = loc("transpose.5")
#loc47 = loc("dot.57")
#loc48 = loc("reshape.59")
#loc49 = loc("transpose.60")
#loc50 = loc("reshape.88")
#loc51 = loc("custom-call.89")
#loc52 = loc("reshape.90")
#loc53 = loc("transpose.91")
#loc54 = loc("dot.93")
#loc55 = loc("reshape.95")
#loc56 = loc("transpose.96")
#loc57 = loc("reshape.71")
#loc58 = loc("custom-call.72")
#loc59 = loc("reshape.76")
#loc60 = loc("dot.79")
#loc61 = loc("transpose.80")
#loc62 = loc("concatenate.81")
#loc63 = loc("cosine.105")
#loc64 = loc("convert.108")
#loc65 = loc("broadcast.111")
#loc66 = loc("multiply.112")
#loc67 = loc("slice.98")
#loc68 = loc("negate.99")
#loc69 = loc("slice.97")
#loc70 = loc("concatenate.100")
#loc71 = loc("sine.82")
#loc72 = loc("convert.85")
#loc73 = loc("broadcast.102")
#loc74 = loc("multiply.103")
#loc75 = loc("add.115")
#loc76 = loc("reshape.394")
#loc77 = loc("custom-call.395")
#loc78 = loc("reshape.396")
#loc79 = loc("broadcast.397")
#loc80 = loc("reshape.249")
#loc81 = loc("custom-call.250")
#loc82 = loc("reshape.251")
#loc83 = loc("transpose.252")
#loc84 = loc("dot.254")
#loc85 = loc("reshape.256")
#loc86 = loc("transpose.257")
#loc87 = loc("broadcast.266")
#loc88 = loc("multiply.267")
#loc89 = loc("slice.259")
#loc90 = loc("negate.260")
#loc91 = loc("slice.258")
#loc92 = loc("concatenate.261")
#loc93 = loc("broadcast.263")
#loc94 = loc("multiply.264")
#loc95 = loc("add.270")
#loc96 = loc("broadcast.244")
#loc97 = loc("reshape.245")
#loc98 = loc("transpose.246")
#loc99 = loc("dot.271")
#loc100 = loc("multiply.274")
#loc101 = loc("broadcast.182")
#loc102 = loc("broadcast.184")
#loc103 = loc("subtract.185")
#loc104 = loc("compare.187")
#loc105 = loc("select.189")
#loc106 = loc("compare.159")
#loc107 = loc("convert.160")
#loc108 = loc("multiply.190")
#loc109 = loc("reshape.192")
#loc110 = loc("reshape.209")
#loc111 = loc("custom-call.210")
#loc112 = loc("reshape.214")
#loc113 = loc("convert.219")
#loc114 = loc("reshape.222")
#loc115 = loc("broadcast.223")
#loc116 = loc("add.224")
#loc117 = loc("compare.227")
#loc118 = loc("select.229")
#loc119 = loc("reshape.277")
#loc120 = loc("broadcast.278")
#loc121 = loc("add.279")
#loc122 = loc("convert.280")
#loc124 = loc("maximum.285")
#loc125 = loc("broadcast.287")
#loc126 = loc("subtract.288")
#loc127 = loc("exponential.289")
#loc129 = loc("add.294")
#loc130 = loc("broadcast.296")
#loc131 = loc("divide.297")
#loc132 = loc("convert.298")
#loc133 = loc("broadcast.151")
#loc134 = loc("reshape.152")
#loc135 = loc("dot.299")
#loc136 = loc("transpose.301")
#loc137 = loc("reshape.303")
#loc138 = loc("reshape.140")
#loc139 = loc("custom-call.141")
#loc140 = loc("reshape.142")
#loc141 = loc("transpose.143")
#loc142 = loc("dot.304")
#loc143 = loc("reshape.305")
#loc144 = loc("add.308")
#loc145 = loc("reshape.339")
#loc146 = loc("custom-call.340")
#loc147 = loc("reshape.341")
#loc148 = loc("broadcast.342")
#loc149 = loc("convert.309")
#loc150 = loc("power.311")
#loc152 = loc("add.317")
#loc153 = loc("multiply.327")
#loc154 = loc("reshape.328")
#loc155 = loc("add.332")
#loc156 = loc("rsqrt.333")
#loc157 = loc("reshape.334")
#loc158 = loc("broadcast.335")
#loc159 = loc("multiply.336")
#loc160 = loc("convert.337")
#loc161 = loc("multiply.343")
#loc162 = loc("reshape.352")
#loc163 = loc("reshape.348")
#loc164 = loc("custom-call.349")
#loc165 = loc("reshape.350")
#loc166 = loc("transpose.351")
#loc167 = loc("dot.353")
#loc168 = loc("reshape.354")
#loc169 = loc("logistic.355")
#loc170 = loc("multiply.356")
#loc171 = loc("reshape.131")
#loc172 = loc("custom-call.132")
#loc173 = loc("reshape.133")
#loc174 = loc("transpose.134")
#loc175 = loc("dot.345")
#loc176 = loc("reshape.346")
#loc177 = loc("multiply.357")
#loc178 = loc("reshape.358")
#loc179 = loc("reshape.126")
#loc180 = loc("custom-call.127")
#loc181 = loc("reshape.128")
#loc182 = loc("transpose.129")
#loc183 = loc("dot.359")
#loc184 = loc("reshape.360")
#loc185 = loc("add.363")
#loc186 = loc("convert.364")
#loc187 = loc("power.366")
#loc189 = loc("add.372")
#loc190 = loc("multiply.382")
#loc191 = loc("reshape.383")
#loc192 = loc("add.387")
#loc193 = loc("rsqrt.388")
#loc194 = loc("reshape.389")
#loc195 = loc("broadcast.390")
#loc196 = loc("multiply.391")
#loc197 = loc("convert.392")
#loc198 = loc("multiply.398")
#loc199 = loc("reshape.402")
#loc200 = loc("reshape.117")
#loc201 = loc("custom-call.118")
#loc202 = loc("reshape.119")
#loc203 = loc("transpose.120")
#loc204 = loc("dot.403")
#loc205 = loc("reshape.404")
------------------ END OF MLIR MODULE ------------------
// -----// IR Dump Before VhloToVersionPass (vhlo-to-version) ('builtin.module' operation: @SyncTensorsGraph.406) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8, \22_axis_1\22=4]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<1x7x!vhlo.i64_v1>, %arg2: !vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<8192x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<64x!vhlo.f32_v1>, %arg5: !vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1>, %arg7: !vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1>, %arg8: !vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<1x7x!vhlo.i64_v1>, %arg11: !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<8192x!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>, %arg14: !vhlo.tensor_v1<8192x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>>}> : () -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.22070313E-4> : tensor<1x7xf32>>}> : () -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %5 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x7x1xf32>>}> : () -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %6 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>>}> : () -> !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1>
    %7 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<8.837890e-02> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %8 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1> : tensor<i64>>}> : () -> !vhlo.tensor_v1<!vhlo.i64_v1>
    %9 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %10 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<-3.389530e+38> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %11 = "vhlo.broadcast_in_dim_v1"(%10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>
    %12 = "vhlo.broadcast_in_dim_v1"(%9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>
    %13 = "vhlo.broadcast_in_dim_v1"(%9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.bf16_v1>
    %14 = "vhlo.broadcast_in_dim_v1"(%10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.bf16_v1>
    %15 = "vhlo.broadcast_in_dim_v1"(%8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.i64_v1>
    %16 = "vhlo.broadcast_in_dim_v1"(%7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>
    %17 = "vhlo.broadcast_in_dim_v1"(%3) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %18 = "vhlo.reshape_v1"(%arg3) : (!vhlo.tensor_v1<8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %19 = "vhlo.custom_call_v1"(%18) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___input_layernorm_weight">}>} : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %20 = "vhlo.reshape_v1"(%19) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x!vhlo.bf16_v1>
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %22 = "vhlo.reshape_v1"(%arg2) : (!vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1>
    %23 = "vhlo.custom_call_v1"(%22) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_embed_tokens_weight">}>} : (!vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1>
    %24 = "vhlo.reshape_v1"(%23) : (!vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1>
    %25 = "vhlo.reshape_v1"(%arg1) : (!vhlo.tensor_v1<1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %26 = "vhlo.custom_call_v1"(%25) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %27 = "vhlo.reshape_v1"(%26) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %28 = "vhlo.convert_v1"(%27) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.ui32_v1>
    %29 = "vhlo.gather_v2"(%24, %28) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 8192]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %30 = "vhlo.reshape_v1"(%29) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %31 = "vhlo.convert_v1"(%30) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %32 = "vhlo.power_v1"(%31, %17) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %33 = "vhlo.reduce_v1"(%32, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg15: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg16: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %203 = "vhlo.add_v1"(%arg15, %arg16) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%203) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %34 = "vhlo.multiply_v1"(%33, %4) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %35 = "vhlo.reshape_v1"(%34) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %36 = "vhlo.add_v1"(%35, %5) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %37 = "vhlo.rsqrt_v2"(%36) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %38 = "vhlo.reshape_v1"(%37) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %39 = "vhlo.broadcast_in_dim_v1"(%38) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %40 = "vhlo.multiply_v1"(%31, %39) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %41 = "vhlo.convert_v1"(%40) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %42 = "vhlo.multiply_v1"(%21, %41) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %43 = "vhlo.reshape_v1"(%42) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %44 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1>
    %45 = "vhlo.custom_call_v1"(%44) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1>
    %46 = "vhlo.reshape_v1"(%45) : (!vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1>
    %47 = "vhlo.transpose_v1"(%46) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,1024]{0,1}">} : (!vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x1024x!vhlo.bf16_v1>
    %48 = "vhlo.dot_general_v2"(%43, %47) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %49 = "vhlo.reshape_v1"(%48) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>
    %50 = "vhlo.transpose_v1"(%49) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %51 = "vhlo.reshape_v1"(%arg5) : (!vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1>
    %52 = "vhlo.custom_call_v1"(%51) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1>
    %53 = "vhlo.reshape_v1"(%52) : (!vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1>
    %54 = "vhlo.transpose_v1"(%53) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,1024]{0,1}">} : (!vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x1024x!vhlo.bf16_v1>
    %55 = "vhlo.dot_general_v2"(%43, %54) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %56 = "vhlo.reshape_v1"(%55) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>
    %57 = "vhlo.transpose_v1"(%56) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %58 = "vhlo.reshape_v1"(%arg4) : (!vhlo.tensor_v1<64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %59 = "vhlo.custom_call_v1"(%58) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"constant">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_rotary_emb_inv_freq">}>} : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %60 = "vhlo.reshape_v1"(%59) : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>
    %61 = "vhlo.dot_general_v2"(%60, %6) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>
    %62 = "vhlo.transpose_v1"(%61) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1]> : tensor<3xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,7,64]{1,2,0}">} : (!vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>
    %63 = "vhlo.concatenate_v1"(%62, %62) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %64 = "vhlo.cosine_v2"(%63) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %65 = "vhlo.convert_v1"(%64) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>
    %66 = "vhlo.broadcast_in_dim_v1"(%65) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %67 = "vhlo.multiply_v1"(%57, %66) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %68 = "vhlo.slice_v1"(%57) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %69 = "vhlo.negate_v1"(%68) : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %70 = "vhlo.slice_v1"(%57) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %71 = "vhlo.concatenate_v1"(%69, %70) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %72 = "vhlo.sine_v2"(%63) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %73 = "vhlo.convert_v1"(%72) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>
    %74 = "vhlo.broadcast_in_dim_v1"(%73) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %75 = "vhlo.multiply_v1"(%71, %74) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %76 = "vhlo.add_v1"(%67, %75) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %77 = "vhlo.reshape_v1"(%arg14) : (!vhlo.tensor_v1<8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %78 = "vhlo.custom_call_v1"(%77) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_norm_weight">}>} : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %79 = "vhlo.reshape_v1"(%78) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x!vhlo.bf16_v1>
    %80 = "vhlo.broadcast_in_dim_v1"(%79) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %81 = "vhlo.reshape_v1"(%arg11) : (!vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1>
    %82 = "vhlo.custom_call_v1"(%81) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1>
    %83 = "vhlo.reshape_v1"(%82) : (!vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>
    %84 = "vhlo.transpose_v1"(%83) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,8192]{0,1}">} : (!vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>
    %85 = "vhlo.dot_general_v2"(%43, %84) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %86 = "vhlo.reshape_v1"(%85) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x64x128x!vhlo.bf16_v1>
    %87 = "vhlo.transpose_v1"(%86) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,64,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x64x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>
    %88 = "vhlo.broadcast_in_dim_v1"(%65) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>
    %89 = "vhlo.multiply_v1"(%87, %88) : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>
    %90 = "vhlo.slice_v1"(%87) <{limit_indices = #vhlo.tensor_v1<dense<[1, 64, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x64x!vhlo.bf16_v1>
    %91 = "vhlo.negate_v1"(%90) : (!vhlo.tensor_v1<1x64x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x64x!vhlo.bf16_v1>
    %92 = "vhlo.slice_v1"(%87) <{limit_indices = #vhlo.tensor_v1<dense<[1, 64, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x64x!vhlo.bf16_v1>
    %93 = "vhlo.concatenate_v1"(%91, %92) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x64x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>
    %94 = "vhlo.broadcast_in_dim_v1"(%73) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>
    %95 = "vhlo.multiply_v1"(%93, %94) : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>
    %96 = "vhlo.add_v1"(%89, %95) : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>
    %97 = "vhlo.broadcast_in_dim_v1"(%76) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x8x7x128x!vhlo.bf16_v1>
    %98 = "vhlo.reshape_v1"(%97) : (!vhlo.tensor_v1<1x8x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>
    %99 = "vhlo.transpose_v1"(%98) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,64,128,7]{2,3,1,0}">} : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x128x7x!vhlo.bf16_v1>
    %100 = "vhlo.dot_general_v2"(%96, %99) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x128x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>
    %101 = "vhlo.multiply_v1"(%100, %16) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>
    %102 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.i64_v1>
    %103 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.i64_v1>
    %104 = "vhlo.subtract_v1"(%102, %103) : (!vhlo.tensor_v1<7x7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.i64_v1>
    %105 = "vhlo.compare_v1"(%104, %15) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GE>}> : (!vhlo.tensor_v1<7x7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.bool_v1>
    %106 = "vhlo.select_v1"(%105, %14, %13) : (!vhlo.tensor_v1<7x7x!vhlo.bool_v1>, !vhlo.tensor_v1<7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.bf16_v1>
    %107 = "vhlo.compare_v1"(%102, %103) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<7x7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.bool_v1>
    %108 = "vhlo.convert_v1"(%107) : (!vhlo.tensor_v1<7x7x!vhlo.bool_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.bf16_v1>
    %109 = "vhlo.multiply_v1"(%106, %108) : (!vhlo.tensor_v1<7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.bf16_v1>
    %110 = "vhlo.reshape_v1"(%109) : (!vhlo.tensor_v1<7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>
    %111 = "vhlo.reshape_v1"(%arg10) : (!vhlo.tensor_v1<1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %112 = "vhlo.custom_call_v1"(%111) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_1">}>} : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %113 = "vhlo.reshape_v1"(%112) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x7x!vhlo.i64_v1>
    %114 = "vhlo.convert_v1"(%113) : (!vhlo.tensor_v1<1x1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x7x!vhlo.bf16_v1>
    %115 = "vhlo.reshape_v1"(%114) : (!vhlo.tensor_v1<1x1x1x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.bf16_v1>
    %116 = "vhlo.broadcast_in_dim_v1"(%115) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>
    %117 = "vhlo.add_v1"(%110, %116) : (!vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>
    %118 = "vhlo.compare_v1"(%117, %12) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x7x!vhlo.bool_v1>
    %119 = "vhlo.select_v1"(%118, %11, %110) : (!vhlo.tensor_v1<1x1x7x7x!vhlo.bool_v1>, !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>
    %120 = "vhlo.reshape_v1"(%119) : (!vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x7x!vhlo.bf16_v1>
    %121 = "vhlo.broadcast_in_dim_v1"(%120) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>
    %122 = "vhlo.add_v1"(%101, %121) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>
    %123 = "vhlo.convert_v1"(%122) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>
    %124 = "vhlo.reduce_v1"(%123, %2) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg15: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg16: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %203 = "vhlo.maximum_v1"(%arg15, %arg16) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%203) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>
    %125 = "vhlo.broadcast_in_dim_v1"(%124) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>
    %126 = "vhlo.subtract_v1"(%123, %125) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>
    %127 = "vhlo.exponential_v2"(%126) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>
    %128 = "vhlo.reduce_v1"(%127, %0) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg15: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg16: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %203 = "vhlo.add_v1"(%arg15, %arg16) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%203) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>
    %129 = "vhlo.broadcast_in_dim_v1"(%128) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>
    %130 = "vhlo.divide_v1"(%127, %129) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>
    %131 = "vhlo.convert_v1"(%130) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>
    %132 = "vhlo.broadcast_in_dim_v1"(%50) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x8x7x128x!vhlo.bf16_v1>
    %133 = "vhlo.reshape_v1"(%132) : (!vhlo.tensor_v1<1x8x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>
    %134 = "vhlo.dot_general_v2"(%131, %133) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>
    %135 = "vhlo.transpose_v1"(%134) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,7,64,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x64x128x!vhlo.bf16_v1>
    %136 = "vhlo.reshape_v1"(%135) : (!vhlo.tensor_v1<1x7x64x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %137 = "vhlo.reshape_v1"(%arg9) : (!vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1>
    %138 = "vhlo.custom_call_v1"(%137) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_o_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1>
    %139 = "vhlo.reshape_v1"(%138) : (!vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>
    %140 = "vhlo.transpose_v1"(%139) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,8192]{0,1}">} : (!vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>
    %141 = "vhlo.dot_general_v2"(%136, %140) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %142 = "vhlo.reshape_v1"(%141) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %143 = "vhlo.add_v1"(%30, %142) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %144 = "vhlo.reshape_v1"(%arg12) : (!vhlo.tensor_v1<8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %145 = "vhlo.custom_call_v1"(%144) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___post_attention_layernorm_weight">}>} : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %146 = "vhlo.reshape_v1"(%145) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x!vhlo.bf16_v1>
    %147 = "vhlo.broadcast_in_dim_v1"(%146) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %148 = "vhlo.convert_v1"(%143) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %149 = "vhlo.power_v1"(%148, %17) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %150 = "vhlo.reduce_v1"(%149, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg15: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg16: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %203 = "vhlo.add_v1"(%arg15, %arg16) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%203) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %151 = "vhlo.multiply_v1"(%150, %4) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %152 = "vhlo.reshape_v1"(%151) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %153 = "vhlo.add_v1"(%152, %5) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %154 = "vhlo.rsqrt_v2"(%153) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %155 = "vhlo.reshape_v1"(%154) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %156 = "vhlo.broadcast_in_dim_v1"(%155) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %157 = "vhlo.multiply_v1"(%148, %156) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %158 = "vhlo.convert_v1"(%157) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %159 = "vhlo.multiply_v1"(%147, %158) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %160 = "vhlo.reshape_v1"(%159) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %161 = "vhlo.reshape_v1"(%arg13) : (!vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1>
    %162 = "vhlo.custom_call_v1"(%161) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_gate_proj_weight">}>} : (!vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1>
    %163 = "vhlo.reshape_v1"(%162) : (!vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>
    %164 = "vhlo.transpose_v1"(%163) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,28672]{0,1}">} : (!vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1>
    %165 = "vhlo.dot_general_v2"(%160, %164) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x28672x!vhlo.bf16_v1>
    %166 = "vhlo.reshape_v1"(%165) : (!vhlo.tensor_v1<7x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>
    %167 = "vhlo.logistic_v2"(%166) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>
    %168 = "vhlo.multiply_v1"(%166, %167) : (!vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>
    %169 = "vhlo.reshape_v1"(%arg8) : (!vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1>
    %170 = "vhlo.custom_call_v1"(%169) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_up_proj_weight">}>} : (!vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1>
    %171 = "vhlo.reshape_v1"(%170) : (!vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>
    %172 = "vhlo.transpose_v1"(%171) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,28672]{0,1}">} : (!vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1>
    %173 = "vhlo.dot_general_v2"(%160, %172) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x28672x!vhlo.bf16_v1>
    %174 = "vhlo.reshape_v1"(%173) : (!vhlo.tensor_v1<7x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>
    %175 = "vhlo.multiply_v1"(%168, %174) : (!vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>
    %176 = "vhlo.reshape_v1"(%175) : (!vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x28672x!vhlo.bf16_v1>
    %177 = "vhlo.reshape_v1"(%arg7) : (!vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x28672x!vhlo.bf16_v1>
    %178 = "vhlo.custom_call_v1"(%177) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_down_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x28672x!vhlo.bf16_v1>
    %179 = "vhlo.reshape_v1"(%178) : (!vhlo.tensor_v1<1x8192x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1>
    %180 = "vhlo.transpose_v1"(%179) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[28672,8192]{0,1}">} : (!vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>
    %181 = "vhlo.dot_general_v2"(%176, %180) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x28672x!vhlo.bf16_v1>, !vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %182 = "vhlo.reshape_v1"(%181) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %183 = "vhlo.add_v1"(%143, %182) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %184 = "vhlo.convert_v1"(%183) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %185 = "vhlo.power_v1"(%184, %17) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %186 = "vhlo.reduce_v1"(%185, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg15: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg16: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %203 = "vhlo.add_v1"(%arg15, %arg16) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%203) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %187 = "vhlo.multiply_v1"(%186, %4) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %188 = "vhlo.reshape_v1"(%187) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %189 = "vhlo.add_v1"(%188, %5) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %190 = "vhlo.rsqrt_v2"(%189) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %191 = "vhlo.reshape_v1"(%190) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %192 = "vhlo.broadcast_in_dim_v1"(%191) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %193 = "vhlo.multiply_v1"(%184, %192) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %194 = "vhlo.convert_v1"(%193) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %195 = "vhlo.multiply_v1"(%80, %194) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %196 = "vhlo.reshape_v1"(%195) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %197 = "vhlo.reshape_v1"(%arg6) : (!vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1>
    %198 = "vhlo.custom_call_v1"(%197) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___lm_head_weight">}>} : (!vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1>
    %199 = "vhlo.reshape_v1"(%198) : (!vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1>
    %200 = "vhlo.transpose_v1"(%199) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,128256]{0,1}">} : (!vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x128256x!vhlo.bf16_v1>
    %201 = "vhlo.dot_general_v2"(%196, %200) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>
    %202 = "vhlo.reshape_v1"(%201) : (!vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>
    "vhlo.return_v1"(%50, %76, %202) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,4]<=[32]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,8,4]<=[32] last_tile_dim_replicate}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,4]<=[32]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,8,4]<=[32] last_tile_dim_replicate}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[4,8]<=[8,4]T(1,0)}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,4]<=[32]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[4,8]<=[8,4]T(1,0)}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,4]<=[32]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,4]<=[32]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump Before VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.406) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8, \22_axis_1\22=4]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1>, %arg1: !vhlo.tensor_v1<1x7x!vhlo.i64_v1>, %arg2: !vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<8192x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<64x!vhlo.f32_v1>, %arg5: !vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1>, %arg7: !vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1>, %arg8: !vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<1x7x!vhlo.i64_v1>, %arg11: !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<8192x!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>, %arg14: !vhlo.tensor_v1<8192x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>>}> : () -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.22070313E-4> : tensor<1x7xf32>>}> : () -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %5 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<9.99999974E-6> : tensor<1x7x1xf32>>}> : () -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %6 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>>}> : () -> !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1>
    %7 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<8.837890e-02> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %8 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1> : tensor<i64>>}> : () -> !vhlo.tensor_v1<!vhlo.i64_v1>
    %9 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %10 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<-3.389530e+38> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %11 = "vhlo.broadcast_in_dim_v1"(%10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>
    %12 = "vhlo.broadcast_in_dim_v1"(%9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>
    %13 = "vhlo.broadcast_in_dim_v1"(%9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.bf16_v1>
    %14 = "vhlo.broadcast_in_dim_v1"(%10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.bf16_v1>
    %15 = "vhlo.broadcast_in_dim_v1"(%8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.i64_v1>
    %16 = "vhlo.broadcast_in_dim_v1"(%7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>
    %17 = "vhlo.broadcast_in_dim_v1"(%3) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %18 = "vhlo.reshape_v1"(%arg3) : (!vhlo.tensor_v1<8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %19 = "vhlo.custom_call_v1"(%18) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___input_layernorm_weight">}>} : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %20 = "vhlo.reshape_v1"(%19) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x!vhlo.bf16_v1>
    %21 = "vhlo.broadcast_in_dim_v1"(%20) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %22 = "vhlo.reshape_v1"(%arg2) : (!vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1>
    %23 = "vhlo.custom_call_v1"(%22) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_embed_tokens_weight">}>} : (!vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1>
    %24 = "vhlo.reshape_v1"(%23) : (!vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1>
    %25 = "vhlo.reshape_v1"(%arg1) : (!vhlo.tensor_v1<1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %26 = "vhlo.custom_call_v1"(%25) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %27 = "vhlo.reshape_v1"(%26) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %28 = "vhlo.convert_v1"(%27) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.ui32_v1>
    %29 = "vhlo.gather_v2"(%24, %28) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 8192]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %30 = "vhlo.reshape_v1"(%29) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %31 = "vhlo.convert_v1"(%30) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %32 = "vhlo.power_v1"(%31, %17) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %33 = "vhlo.reduce_v1"(%32, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg15: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg16: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %203 = "vhlo.add_v1"(%arg15, %arg16) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%203) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %34 = "vhlo.multiply_v1"(%33, %4) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %35 = "vhlo.reshape_v1"(%34) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %36 = "vhlo.add_v1"(%35, %5) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %37 = "vhlo.rsqrt_v2"(%36) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %38 = "vhlo.reshape_v1"(%37) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %39 = "vhlo.broadcast_in_dim_v1"(%38) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %40 = "vhlo.multiply_v1"(%31, %39) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %41 = "vhlo.convert_v1"(%40) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %42 = "vhlo.multiply_v1"(%21, %41) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %43 = "vhlo.reshape_v1"(%42) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %44 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1>
    %45 = "vhlo.custom_call_v1"(%44) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1>
    %46 = "vhlo.reshape_v1"(%45) : (!vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1>
    %47 = "vhlo.transpose_v1"(%46) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,1024]{0,1}">} : (!vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x1024x!vhlo.bf16_v1>
    %48 = "vhlo.dot_general_v2"(%43, %47) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %49 = "vhlo.reshape_v1"(%48) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>
    %50 = "vhlo.transpose_v1"(%49) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %51 = "vhlo.reshape_v1"(%arg5) : (!vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1>
    %52 = "vhlo.custom_call_v1"(%51) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1>
    %53 = "vhlo.reshape_v1"(%52) : (!vhlo.tensor_v1<1x1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1>
    %54 = "vhlo.transpose_v1"(%53) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,1024]{0,1}">} : (!vhlo.tensor_v1<1024x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x1024x!vhlo.bf16_v1>
    %55 = "vhlo.dot_general_v2"(%43, %54) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %56 = "vhlo.reshape_v1"(%55) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>
    %57 = "vhlo.transpose_v1"(%56) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %58 = "vhlo.reshape_v1"(%arg4) : (!vhlo.tensor_v1<64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %59 = "vhlo.custom_call_v1"(%58) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"constant">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_rotary_emb_inv_freq">}>} : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %60 = "vhlo.reshape_v1"(%59) : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>
    %61 = "vhlo.dot_general_v2"(%60, %6) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>
    %62 = "vhlo.transpose_v1"(%61) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1]> : tensor<3xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,7,64]{1,2,0}">} : (!vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>
    %63 = "vhlo.concatenate_v1"(%62, %62) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %64 = "vhlo.cosine_v2"(%63) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %65 = "vhlo.convert_v1"(%64) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>
    %66 = "vhlo.broadcast_in_dim_v1"(%65) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %67 = "vhlo.multiply_v1"(%57, %66) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %68 = "vhlo.slice_v1"(%57) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %69 = "vhlo.negate_v1"(%68) : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %70 = "vhlo.slice_v1"(%57) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %71 = "vhlo.concatenate_v1"(%69, %70) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %72 = "vhlo.sine_v2"(%63) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %73 = "vhlo.convert_v1"(%72) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>
    %74 = "vhlo.broadcast_in_dim_v1"(%73) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %75 = "vhlo.multiply_v1"(%71, %74) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %76 = "vhlo.add_v1"(%67, %75) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %77 = "vhlo.reshape_v1"(%arg14) : (!vhlo.tensor_v1<8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %78 = "vhlo.custom_call_v1"(%77) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_norm_weight">}>} : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %79 = "vhlo.reshape_v1"(%78) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x!vhlo.bf16_v1>
    %80 = "vhlo.broadcast_in_dim_v1"(%79) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %81 = "vhlo.reshape_v1"(%arg11) : (!vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1>
    %82 = "vhlo.custom_call_v1"(%81) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1>
    %83 = "vhlo.reshape_v1"(%82) : (!vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>
    %84 = "vhlo.transpose_v1"(%83) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,8192]{0,1}">} : (!vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>
    %85 = "vhlo.dot_general_v2"(%43, %84) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %86 = "vhlo.reshape_v1"(%85) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x64x128x!vhlo.bf16_v1>
    %87 = "vhlo.transpose_v1"(%86) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,64,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x64x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>
    %88 = "vhlo.broadcast_in_dim_v1"(%65) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>
    %89 = "vhlo.multiply_v1"(%87, %88) : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>
    %90 = "vhlo.slice_v1"(%87) <{limit_indices = #vhlo.tensor_v1<dense<[1, 64, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x64x!vhlo.bf16_v1>
    %91 = "vhlo.negate_v1"(%90) : (!vhlo.tensor_v1<1x64x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x64x!vhlo.bf16_v1>
    %92 = "vhlo.slice_v1"(%87) <{limit_indices = #vhlo.tensor_v1<dense<[1, 64, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x64x!vhlo.bf16_v1>
    %93 = "vhlo.concatenate_v1"(%91, %92) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x64x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>
    %94 = "vhlo.broadcast_in_dim_v1"(%73) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>
    %95 = "vhlo.multiply_v1"(%93, %94) : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>
    %96 = "vhlo.add_v1"(%89, %95) : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>
    %97 = "vhlo.broadcast_in_dim_v1"(%76) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x8x7x128x!vhlo.bf16_v1>
    %98 = "vhlo.reshape_v1"(%97) : (!vhlo.tensor_v1<1x8x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>
    %99 = "vhlo.transpose_v1"(%98) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,64,128,7]{2,3,1,0}">} : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x128x7x!vhlo.bf16_v1>
    %100 = "vhlo.dot_general_v2"(%96, %99) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x128x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>
    %101 = "vhlo.multiply_v1"(%100, %16) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>
    %102 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.i64_v1>
    %103 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.i64_v1>
    %104 = "vhlo.subtract_v1"(%102, %103) : (!vhlo.tensor_v1<7x7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.i64_v1>
    %105 = "vhlo.compare_v1"(%104, %15) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GE>}> : (!vhlo.tensor_v1<7x7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.bool_v1>
    %106 = "vhlo.select_v1"(%105, %14, %13) : (!vhlo.tensor_v1<7x7x!vhlo.bool_v1>, !vhlo.tensor_v1<7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.bf16_v1>
    %107 = "vhlo.compare_v1"(%102, %103) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<7x7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.bool_v1>
    %108 = "vhlo.convert_v1"(%107) : (!vhlo.tensor_v1<7x7x!vhlo.bool_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.bf16_v1>
    %109 = "vhlo.multiply_v1"(%106, %108) : (!vhlo.tensor_v1<7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x7x!vhlo.bf16_v1>
    %110 = "vhlo.reshape_v1"(%109) : (!vhlo.tensor_v1<7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>
    %111 = "vhlo.reshape_v1"(%arg10) : (!vhlo.tensor_v1<1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %112 = "vhlo.custom_call_v1"(%111) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_1">}>} : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %113 = "vhlo.reshape_v1"(%112) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x7x!vhlo.i64_v1>
    %114 = "vhlo.convert_v1"(%113) : (!vhlo.tensor_v1<1x1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x7x!vhlo.bf16_v1>
    %115 = "vhlo.reshape_v1"(%114) : (!vhlo.tensor_v1<1x1x1x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.bf16_v1>
    %116 = "vhlo.broadcast_in_dim_v1"(%115) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>
    %117 = "vhlo.add_v1"(%110, %116) : (!vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>
    %118 = "vhlo.compare_v1"(%117, %12) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x7x!vhlo.bool_v1>
    %119 = "vhlo.select_v1"(%118, %11, %110) : (!vhlo.tensor_v1<1x1x7x7x!vhlo.bool_v1>, !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>
    %120 = "vhlo.reshape_v1"(%119) : (!vhlo.tensor_v1<1x1x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x7x!vhlo.bf16_v1>
    %121 = "vhlo.broadcast_in_dim_v1"(%120) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>
    %122 = "vhlo.add_v1"(%101, %121) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>
    %123 = "vhlo.convert_v1"(%122) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>
    %124 = "vhlo.reduce_v1"(%123, %2) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg15: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg16: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %203 = "vhlo.maximum_v1"(%arg15, %arg16) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%203) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>
    %125 = "vhlo.broadcast_in_dim_v1"(%124) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>
    %126 = "vhlo.subtract_v1"(%123, %125) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>
    %127 = "vhlo.exponential_v2"(%126) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>
    %128 = "vhlo.reduce_v1"(%127, %0) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg15: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg16: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %203 = "vhlo.add_v1"(%arg15, %arg16) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%203) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>
    %129 = "vhlo.broadcast_in_dim_v1"(%128) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>
    %130 = "vhlo.divide_v1"(%127, %129) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>
    %131 = "vhlo.convert_v1"(%130) : (!vhlo.tensor_v1<1x64x7x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>
    %132 = "vhlo.broadcast_in_dim_v1"(%50) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x8x7x128x!vhlo.bf16_v1>
    %133 = "vhlo.reshape_v1"(%132) : (!vhlo.tensor_v1<1x8x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>
    %134 = "vhlo.dot_general_v2"(%131, %133) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"grad_x"> = #vhlo.string_v1<"false">, #vhlo.string_v1<"grad_y"> = #vhlo.string_v1<"false">}>} : (!vhlo.tensor_v1<1x64x7x7x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>
    %135 = "vhlo.transpose_v1"(%134) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,7,64,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x64x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x64x128x!vhlo.bf16_v1>
    %136 = "vhlo.reshape_v1"(%135) : (!vhlo.tensor_v1<1x7x64x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %137 = "vhlo.reshape_v1"(%arg9) : (!vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1>
    %138 = "vhlo.custom_call_v1"(%137) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___self_attn_o_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1>
    %139 = "vhlo.reshape_v1"(%138) : (!vhlo.tensor_v1<1x8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>
    %140 = "vhlo.transpose_v1"(%139) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,8192]{0,1}">} : (!vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>
    %141 = "vhlo.dot_general_v2"(%136, %140) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %142 = "vhlo.reshape_v1"(%141) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %143 = "vhlo.add_v1"(%30, %142) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %144 = "vhlo.reshape_v1"(%arg12) : (!vhlo.tensor_v1<8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %145 = "vhlo.custom_call_v1"(%144) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___post_attention_layernorm_weight">}>} : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %146 = "vhlo.reshape_v1"(%145) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x!vhlo.bf16_v1>
    %147 = "vhlo.broadcast_in_dim_v1"(%146) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %148 = "vhlo.convert_v1"(%143) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %149 = "vhlo.power_v1"(%148, %17) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %150 = "vhlo.reduce_v1"(%149, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg15: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg16: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %203 = "vhlo.add_v1"(%arg15, %arg16) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%203) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %151 = "vhlo.multiply_v1"(%150, %4) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %152 = "vhlo.reshape_v1"(%151) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %153 = "vhlo.add_v1"(%152, %5) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %154 = "vhlo.rsqrt_v2"(%153) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %155 = "vhlo.reshape_v1"(%154) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %156 = "vhlo.broadcast_in_dim_v1"(%155) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %157 = "vhlo.multiply_v1"(%148, %156) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %158 = "vhlo.convert_v1"(%157) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %159 = "vhlo.multiply_v1"(%147, %158) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %160 = "vhlo.reshape_v1"(%159) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %161 = "vhlo.reshape_v1"(%arg13) : (!vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1>
    %162 = "vhlo.custom_call_v1"(%161) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_gate_proj_weight">}>} : (!vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1>
    %163 = "vhlo.reshape_v1"(%162) : (!vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>
    %164 = "vhlo.transpose_v1"(%163) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,28672]{0,1}">} : (!vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1>
    %165 = "vhlo.dot_general_v2"(%160, %164) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x28672x!vhlo.bf16_v1>
    %166 = "vhlo.reshape_v1"(%165) : (!vhlo.tensor_v1<7x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>
    %167 = "vhlo.logistic_v2"(%166) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>
    %168 = "vhlo.multiply_v1"(%166, %167) : (!vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>
    %169 = "vhlo.reshape_v1"(%arg8) : (!vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1>
    %170 = "vhlo.custom_call_v1"(%169) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_up_proj_weight">}>} : (!vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1>
    %171 = "vhlo.reshape_v1"(%170) : (!vhlo.tensor_v1<1x28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>
    %172 = "vhlo.transpose_v1"(%171) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,28672]{0,1}">} : (!vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1>
    %173 = "vhlo.dot_general_v2"(%160, %172) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x28672x!vhlo.bf16_v1>
    %174 = "vhlo.reshape_v1"(%173) : (!vhlo.tensor_v1<7x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>
    %175 = "vhlo.multiply_v1"(%168, %174) : (!vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>
    %176 = "vhlo.reshape_v1"(%175) : (!vhlo.tensor_v1<1x7x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x28672x!vhlo.bf16_v1>
    %177 = "vhlo.reshape_v1"(%arg7) : (!vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x28672x!vhlo.bf16_v1>
    %178 = "vhlo.custom_call_v1"(%177) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_layers__modules__0___mlp_down_proj_weight">}>} : (!vhlo.tensor_v1<1x8192x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x28672x!vhlo.bf16_v1>
    %179 = "vhlo.reshape_v1"(%178) : (!vhlo.tensor_v1<1x8192x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1>
    %180 = "vhlo.transpose_v1"(%179) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[28672,8192]{0,1}">} : (!vhlo.tensor_v1<8192x28672x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>
    %181 = "vhlo.dot_general_v2"(%176, %180) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x28672x!vhlo.bf16_v1>, !vhlo.tensor_v1<28672x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %182 = "vhlo.reshape_v1"(%181) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %183 = "vhlo.add_v1"(%143, %182) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %184 = "vhlo.convert_v1"(%183) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %185 = "vhlo.power_v1"(%184, %17) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %186 = "vhlo.reduce_v1"(%185, %0) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg15: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg16: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %203 = "vhlo.add_v1"(%arg15, %arg16) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%203) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %187 = "vhlo.multiply_v1"(%186, %4) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %188 = "vhlo.reshape_v1"(%187) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %189 = "vhlo.add_v1"(%188, %5) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %190 = "vhlo.rsqrt_v2"(%189) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %191 = "vhlo.reshape_v1"(%190) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %192 = "vhlo.broadcast_in_dim_v1"(%191) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %193 = "vhlo.multiply_v1"(%184, %192) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %194 = "vhlo.convert_v1"(%193) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %195 = "vhlo.multiply_v1"(%80, %194) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %196 = "vhlo.reshape_v1"(%195) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %197 = "vhlo.reshape_v1"(%arg6) : (!vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1>
    %198 = "vhlo.custom_call_v1"(%197) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___lm_head_weight">}>} : (!vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1>
    %199 = "vhlo.reshape_v1"(%198) : (!vhlo.tensor_v1<1x128256x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1>
    %200 = "vhlo.transpose_v1"(%199) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[8192,128256]{0,1}">} : (!vhlo.tensor_v1<128256x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8192x128256x!vhlo.bf16_v1>
    %201 = "vhlo.dot_general_v2"(%196, %200) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>
    %202 = "vhlo.reshape_v1"(%201) : (!vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>
    "vhlo.return_v1"(%50, %76, %202) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,4]<=[32]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,8,4]<=[32] last_tile_dim_replicate}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,4]<=[32]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[1,8,4]<=[32] last_tile_dim_replicate}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[4,8]<=[8,4]T(1,0)}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,4]<=[32]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[4,8]<=[8,4]T(1,0)}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,4]<=[32]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,4]<=[32]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump After VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.406) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8, \22_axis_1\22=4]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1024x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}"}, %arg1: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<128256x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8,4]<=[32] last_tile_dim_replicate}"}, %arg3: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg4: tensor<64xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg5: tensor<1024x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}"}, %arg6: tensor<128256x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8,4]<=[32] last_tile_dim_replicate}"}, %arg7: tensor<8192x28672xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[4,8]<=[8,4]T(1,0)}"}, %arg8: tensor<28672x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}"}, %arg9: tensor<8192x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[4,8]<=[8,4]T(1,0)}"}, %arg10: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg11: tensor<8192x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}"}, %arg12: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg13: tensor<28672x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}"}, %arg14: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}) -> (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
    %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
    %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
    %c_6 = stablehlo.constant dense<1> : tensor<i64>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %4 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
    %5 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %7 = stablehlo.reshape %arg3 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %8 = stablehlo.custom_call @tt.mark_argument(%7) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}} : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %9 = stablehlo.reshape %8 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %10 = stablehlo.broadcast_in_dim %9, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %11 = stablehlo.reshape %arg2 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %12 = stablehlo.custom_call @tt.mark_argument(%11) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_embed_tokens_weight"}} : (tensor<1x128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %13 = stablehlo.reshape %12 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %14 = stablehlo.reshape %arg1 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %15 = stablehlo.custom_call @tt.mark_argument(%14) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<1x1x7xi64>) -> tensor<1x1x7xi64>
    %16 = stablehlo.reshape %15 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %17 = stablehlo.convert %16 : (tensor<7xi64>) -> tensor<7xui32>
    %18 = "stablehlo.gather"(%13, %17) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
    %19 = stablehlo.reshape %18 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %20 = stablehlo.convert %19 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %21 = stablehlo.power %20, %6 : tensor<1x7x8192xf32>
    %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %23 = stablehlo.multiply %22, %cst_2 : tensor<1x7xf32>
    %24 = stablehlo.reshape %23 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %25 = stablehlo.add %24, %cst_3 : tensor<1x7x1xf32>
    %26 = stablehlo.rsqrt %25 : tensor<1x7x1xf32>
    %27 = stablehlo.reshape %26 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %29 = stablehlo.multiply %20, %28 : tensor<1x7x8192xf32>
    %30 = stablehlo.convert %29 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %31 = stablehlo.multiply %10, %30 : tensor<1x7x8192xbf16>
    %32 = stablehlo.reshape %31 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %33 = stablehlo.reshape %arg0 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %34 = stablehlo.custom_call @tt.mark_argument(%33) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}} : (tensor<1x1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %35 = stablehlo.reshape %34 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %36 = stablehlo.transpose %35, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %37 = stablehlo.dot_general %32, %36, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %38 = stablehlo.reshape %37 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %39 = stablehlo.transpose %38, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %40 = stablehlo.reshape %arg5 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %41 = stablehlo.custom_call @tt.mark_argument(%40) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}} : (tensor<1x1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %42 = stablehlo.reshape %41 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %43 = stablehlo.transpose %42, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %44 = stablehlo.dot_general %32, %43, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %45 = stablehlo.reshape %44 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %46 = stablehlo.transpose %45, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %47 = stablehlo.reshape %arg4 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %48 = stablehlo.custom_call @tt.mark_argument(%47) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___model_rotary_emb_inv_freq"}} : (tensor<1x1x64xf32>) -> tensor<1x1x64xf32>
    %49 = stablehlo.reshape %48 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %50 = stablehlo.dot_general %49, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %51 = stablehlo.transpose %50, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %52 = stablehlo.concatenate %51, %51, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %53 = stablehlo.cosine %52 : tensor<1x7x128xf32>
    %54 = stablehlo.convert %53 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %56 = stablehlo.multiply %46, %55 : tensor<1x8x7x128xbf16>
    %57 = stablehlo.slice %46 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %58 = stablehlo.negate %57 : tensor<1x8x7x64xbf16>
    %59 = stablehlo.slice %46 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %60 = stablehlo.concatenate %58, %59, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %61 = stablehlo.sine %52 : tensor<1x7x128xf32>
    %62 = stablehlo.convert %61 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %63 = stablehlo.broadcast_in_dim %62, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %64 = stablehlo.multiply %60, %63 : tensor<1x8x7x128xbf16>
    %65 = stablehlo.add %56, %64 : tensor<1x8x7x128xbf16>
    %66 = stablehlo.reshape %arg14 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %67 = stablehlo.custom_call @tt.mark_argument(%66) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_norm_weight"}} : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %68 = stablehlo.reshape %67 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %69 = stablehlo.broadcast_in_dim %68, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %70 = stablehlo.reshape %arg11 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %71 = stablehlo.custom_call @tt.mark_argument(%70) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}} : (tensor<1x8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %72 = stablehlo.reshape %71 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %73 = stablehlo.transpose %72, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %74 = stablehlo.dot_general %32, %73, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %75 = stablehlo.reshape %74 : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
    %76 = stablehlo.transpose %75, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
    %77 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %78 = stablehlo.multiply %76, %77 : tensor<1x64x7x128xbf16>
    %79 = stablehlo.slice %76 [0:1, 0:64, 0:7, 64:128] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %80 = stablehlo.negate %79 : tensor<1x64x7x64xbf16>
    %81 = stablehlo.slice %76 [0:1, 0:64, 0:7, 0:64] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %82 = stablehlo.concatenate %80, %81, dim = 3 : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
    %83 = stablehlo.broadcast_in_dim %62, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %84 = stablehlo.multiply %82, %83 : tensor<1x64x7x128xbf16>
    %85 = stablehlo.add %78, %84 : tensor<1x64x7x128xbf16>
    %86 = stablehlo.broadcast_in_dim %65, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %87 = stablehlo.reshape %86 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %88 = stablehlo.transpose %87, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
    %89 = stablehlo.dot_general %85, %88, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
    %90 = stablehlo.multiply %89, %5 : tensor<1x64x7x7xbf16>
    %91 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %92 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %93 = stablehlo.subtract %91, %92 : tensor<7x7xi64>
    %94 = stablehlo.compare  GE, %93, %4 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %95 = stablehlo.select %94, %3, %2 : tensor<7x7xi1>, tensor<7x7xbf16>
    %96 = stablehlo.compare  GT, %91, %92 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %97 = stablehlo.convert %96 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
    %98 = stablehlo.multiply %95, %97 : tensor<7x7xbf16>
    %99 = stablehlo.reshape %98 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
    %100 = stablehlo.reshape %arg10 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %101 = stablehlo.custom_call @tt.mark_argument(%100) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_1"}} : (tensor<1x1x7xi64>) -> tensor<1x1x7xi64>
    %102 = stablehlo.reshape %101 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
    %103 = stablehlo.convert %102 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
    %104 = stablehlo.reshape %103 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
    %106 = stablehlo.add %99, %105 : tensor<1x1x7x7xbf16>
    %107 = stablehlo.compare  EQ, %106, %1 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
    %108 = stablehlo.select %107, %0, %99 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
    %109 = stablehlo.reshape %108 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
    %110 = stablehlo.broadcast_in_dim %109, dims = [0, 2, 3] : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
    %111 = stablehlo.add %90, %110 : tensor<1x64x7x7xbf16>
    %112 = stablehlo.convert %111 : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
    %113 = stablehlo.reduce(%112 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %114 = stablehlo.broadcast_in_dim %113, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %115 = stablehlo.subtract %112, %114 : tensor<1x64x7x7xf32>
    %116 = stablehlo.exponential %115 : tensor<1x64x7x7xf32>
    %117 = stablehlo.reduce(%116 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %118 = stablehlo.broadcast_in_dim %117, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %119 = stablehlo.divide %116, %118 : tensor<1x64x7x7xf32>
    %120 = stablehlo.convert %119 : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
    %121 = stablehlo.broadcast_in_dim %39, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %122 = stablehlo.reshape %121 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %123 = stablehlo.dot_general %120, %122, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %124 = stablehlo.transpose %123, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
    %125 = stablehlo.reshape %124 : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
    %126 = stablehlo.reshape %arg9 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %127 = stablehlo.custom_call @tt.mark_argument(%126) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}} : (tensor<1x8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %128 = stablehlo.reshape %127 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %129 = stablehlo.transpose %128, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %130 = stablehlo.dot_general %125, %129, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %131 = stablehlo.reshape %130 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %132 = stablehlo.add %19, %131 : tensor<1x7x8192xbf16>
    %133 = stablehlo.reshape %arg12 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %134 = stablehlo.custom_call @tt.mark_argument(%133) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}} : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %135 = stablehlo.reshape %134 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %136 = stablehlo.broadcast_in_dim %135, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %137 = stablehlo.convert %132 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %138 = stablehlo.power %137, %6 : tensor<1x7x8192xf32>
    %139 = stablehlo.reduce(%138 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %140 = stablehlo.multiply %139, %cst_2 : tensor<1x7xf32>
    %141 = stablehlo.reshape %140 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %142 = stablehlo.add %141, %cst_3 : tensor<1x7x1xf32>
    %143 = stablehlo.rsqrt %142 : tensor<1x7x1xf32>
    %144 = stablehlo.reshape %143 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %145 = stablehlo.broadcast_in_dim %144, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %146 = stablehlo.multiply %137, %145 : tensor<1x7x8192xf32>
    %147 = stablehlo.convert %146 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %148 = stablehlo.multiply %136, %147 : tensor<1x7x8192xbf16>
    %149 = stablehlo.reshape %148 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %150 = stablehlo.reshape %arg13 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %151 = stablehlo.custom_call @tt.mark_argument(%150) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}} : (tensor<1x28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %152 = stablehlo.reshape %151 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %153 = stablehlo.transpose %152, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %154 = stablehlo.dot_general %149, %153, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %155 = stablehlo.reshape %154 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %156 = stablehlo.logistic %155 : tensor<1x7x28672xbf16>
    %157 = stablehlo.multiply %155, %156 : tensor<1x7x28672xbf16>
    %158 = stablehlo.reshape %arg8 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %159 = stablehlo.custom_call @tt.mark_argument(%158) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}} : (tensor<1x28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %160 = stablehlo.reshape %159 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %161 = stablehlo.transpose %160, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %162 = stablehlo.dot_general %149, %161, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %163 = stablehlo.reshape %162 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %164 = stablehlo.multiply %157, %163 : tensor<1x7x28672xbf16>
    %165 = stablehlo.reshape %164 : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
    %166 = stablehlo.reshape %arg7 : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
    %167 = stablehlo.custom_call @tt.mark_argument(%166) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}} : (tensor<1x8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
    %168 = stablehlo.reshape %167 : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
    %169 = stablehlo.transpose %168, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
    %170 = stablehlo.dot_general %165, %169, contracting_dims = [1] x [0] : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
    %171 = stablehlo.reshape %170 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %172 = stablehlo.add %132, %171 : tensor<1x7x8192xbf16>
    %173 = stablehlo.convert %172 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %174 = stablehlo.power %173, %6 : tensor<1x7x8192xf32>
    %175 = stablehlo.reduce(%174 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %176 = stablehlo.multiply %175, %cst_2 : tensor<1x7xf32>
    %177 = stablehlo.reshape %176 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %178 = stablehlo.add %177, %cst_3 : tensor<1x7x1xf32>
    %179 = stablehlo.rsqrt %178 : tensor<1x7x1xf32>
    %180 = stablehlo.reshape %179 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %181 = stablehlo.broadcast_in_dim %180, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %182 = stablehlo.multiply %173, %181 : tensor<1x7x8192xf32>
    %183 = stablehlo.convert %182 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %184 = stablehlo.multiply %69, %183 : tensor<1x7x8192xbf16>
    %185 = stablehlo.reshape %184 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %186 = stablehlo.reshape %arg6 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %187 = stablehlo.custom_call @tt.mark_argument(%186) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___lm_head_weight"}} : (tensor<1x128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %188 = stablehlo.reshape %187 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %189 = stablehlo.transpose %188, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
    %190 = stablehlo.dot_general %185, %189, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
    %191 = stablehlo.reshape %190 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %39, %65, %191 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


2025-12-04 19:55:23.101 (  69.251s) [        CE2D1B80]      module_builder.cc:971      1| MLIR Module shlo:
#loc1 = loc("p0.1")
#loc2 = loc("p1.9")
#loc3 = loc("p2.14")
#loc4 = loc("p3.50")
#loc5 = loc("p4.70")
#loc6 = loc("p5.87")
#loc7 = loc("p6.116")
#loc8 = loc("p7.125")
#loc9 = loc("p8.130")
#loc10 = loc("p9.139")
#loc11 = loc("p10.208")
#loc12 = loc("p11.248")
#loc13 = loc("p12.338")
#loc14 = loc("p13.347")
#loc15 = loc("p14.393")
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8, \22_axis_1\22=4]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1024x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}"} loc("p0.1"), %arg1: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"} loc("p1.9"), %arg2: tensor<128256x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8,4]<=[32] last_tile_dim_replicate}"} loc("p2.14"), %arg3: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("p3.50"), %arg4: tensor<64xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("p4.70"), %arg5: tensor<1024x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}"} loc("p5.87"), %arg6: tensor<128256x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8,4]<=[32] last_tile_dim_replicate}"} loc("p6.116"), %arg7: tensor<8192x28672xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[4,8]<=[8,4]T(1,0)}"} loc("p7.125"), %arg8: tensor<28672x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}"} loc("p8.130"), %arg9: tensor<8192x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[4,8]<=[8,4]T(1,0)}"} loc("p9.139"), %arg10: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"} loc("p10.208"), %arg11: tensor<8192x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}"} loc("p11.248"), %arg12: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("p12.338"), %arg13: tensor<28672x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}"} loc("p13.347"), %arg14: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("p14.393")) -> (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64> loc(#loc)
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)
    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32> loc(#loc)
    %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32> loc(#loc)
    %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32> loc(#loc)
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32> loc(#loc)
    %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16> loc(#loc)
    %c_6 = stablehlo.constant dense<1> : tensor<i64> loc(#loc)
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16> loc(#loc)
    %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16> loc(#loc)
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16> loc(#loc)
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16> loc(#loc)
    %4 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64> loc(#loc)
    %5 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<bf16>) -> tensor<1x64x7x7xbf16> loc(#loc)
    %6 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32> loc(#loc)
    %7 = stablehlo.reshape %arg3 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16> loc(#loc16)
    %8 = stablehlo.custom_call @tt.mark_argument(%7) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}} : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xbf16> loc(#loc17)
    %9 = stablehlo.reshape %8 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16> loc(#loc18)
    %10 = stablehlo.broadcast_in_dim %9, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16> loc(#loc19)
    %11 = stablehlo.reshape %arg2 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16> loc(#loc20)
    %12 = stablehlo.custom_call @tt.mark_argument(%11) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_embed_tokens_weight"}} : (tensor<1x128256x8192xbf16>) -> tensor<1x128256x8192xbf16> loc(#loc21)
    %13 = stablehlo.reshape %12 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16> loc(#loc22)
    %14 = stablehlo.reshape %arg1 : (tensor<1x7xi64>) -> tensor<1x1x7xi64> loc(#loc23)
    %15 = stablehlo.custom_call @tt.mark_argument(%14) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<1x1x7xi64>) -> tensor<1x1x7xi64> loc(#loc24)
    %16 = stablehlo.reshape %15 : (tensor<1x1x7xi64>) -> tensor<7xi64> loc(#loc25)
    %17 = stablehlo.convert %16 : (tensor<7xi64>) -> tensor<7xui32> loc(#loc26)
    %18 = "stablehlo.gather"(%13, %17) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16> loc(#loc27)
    %19 = stablehlo.reshape %18 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16> loc(#loc28)
    %20 = stablehlo.convert %19 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32> loc(#loc29)
    %21 = stablehlo.power %20, %6 : tensor<1x7x8192xf32> loc(#loc30)
    %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32> loc(#loc31)
    %23 = stablehlo.multiply %22, %cst_2 : tensor<1x7xf32> loc(#loc32)
    %24 = stablehlo.reshape %23 : (tensor<1x7xf32>) -> tensor<1x7x1xf32> loc(#loc33)
    %25 = stablehlo.add %24, %cst_3 : tensor<1x7x1xf32> loc(#loc34)
    %26 = stablehlo.rsqrt %25 : tensor<1x7x1xf32> loc(#loc35)
    %27 = stablehlo.reshape %26 : (tensor<1x7x1xf32>) -> tensor<1x7xf32> loc(#loc36)
    %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32> loc(#loc37)
    %29 = stablehlo.multiply %20, %28 : tensor<1x7x8192xf32> loc(#loc38)
    %30 = stablehlo.convert %29 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16> loc(#loc39)
    %31 = stablehlo.multiply %10, %30 : tensor<1x7x8192xbf16> loc(#loc40)
    %32 = stablehlo.reshape %31 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16> loc(#loc41)
    %33 = stablehlo.reshape %arg0 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16> loc(#loc42)
    %34 = stablehlo.custom_call @tt.mark_argument(%33) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}} : (tensor<1x1024x8192xbf16>) -> tensor<1x1024x8192xbf16> loc(#loc43)
    %35 = stablehlo.reshape %34 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16> loc(#loc44)
    %36 = stablehlo.transpose %35, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16> loc(#loc45)
    %37 = stablehlo.dot_general %32, %36, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16> loc(#loc46)
    %38 = stablehlo.reshape %37 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16> loc(#loc47)
    %39 = stablehlo.transpose %38, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16> loc(#loc48)
    %40 = stablehlo.reshape %arg5 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16> loc(#loc49)
    %41 = stablehlo.custom_call @tt.mark_argument(%40) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}} : (tensor<1x1024x8192xbf16>) -> tensor<1x1024x8192xbf16> loc(#loc50)
    %42 = stablehlo.reshape %41 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16> loc(#loc51)
    %43 = stablehlo.transpose %42, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16> loc(#loc52)
    %44 = stablehlo.dot_general %32, %43, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16> loc(#loc53)
    %45 = stablehlo.reshape %44 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16> loc(#loc54)
    %46 = stablehlo.transpose %45, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16> loc(#loc55)
    %47 = stablehlo.reshape %arg4 : (tensor<64xf32>) -> tensor<1x1x64xf32> loc(#loc56)
    %48 = stablehlo.custom_call @tt.mark_argument(%47) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "constant", ttir.name = "l__self___model_rotary_emb_inv_freq"}} : (tensor<1x1x64xf32>) -> tensor<1x1x64xf32> loc(#loc57)
    %49 = stablehlo.reshape %48 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32> loc(#loc58)
    %50 = stablehlo.dot_general %49, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32> loc(#loc59)
    %51 = stablehlo.transpose %50, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32> loc(#loc60)
    %52 = stablehlo.concatenate %51, %51, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32> loc(#loc61)
    %53 = stablehlo.cosine %52 : tensor<1x7x128xf32> loc(#loc62)
    %54 = stablehlo.convert %53 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16> loc(#loc63)
    %55 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16> loc(#loc64)
    %56 = stablehlo.multiply %46, %55 : tensor<1x8x7x128xbf16> loc(#loc65)
    %57 = stablehlo.slice %46 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16> loc(#loc66)
    %58 = stablehlo.negate %57 : tensor<1x8x7x64xbf16> loc(#loc67)
    %59 = stablehlo.slice %46 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16> loc(#loc68)
    %60 = stablehlo.concatenate %58, %59, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16> loc(#loc69)
    %61 = stablehlo.sine %52 : tensor<1x7x128xf32> loc(#loc70)
    %62 = stablehlo.convert %61 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16> loc(#loc71)
    %63 = stablehlo.broadcast_in_dim %62, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16> loc(#loc72)
    %64 = stablehlo.multiply %60, %63 : tensor<1x8x7x128xbf16> loc(#loc73)
    %65 = stablehlo.add %56, %64 : tensor<1x8x7x128xbf16> loc(#loc74)
    %66 = stablehlo.reshape %arg14 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16> loc(#loc75)
    %67 = stablehlo.custom_call @tt.mark_argument(%66) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_norm_weight"}} : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xbf16> loc(#loc76)
    %68 = stablehlo.reshape %67 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16> loc(#loc77)
    %69 = stablehlo.broadcast_in_dim %68, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16> loc(#loc78)
    %70 = stablehlo.reshape %arg11 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16> loc(#loc79)
    %71 = stablehlo.custom_call @tt.mark_argument(%70) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}} : (tensor<1x8192x8192xbf16>) -> tensor<1x8192x8192xbf16> loc(#loc80)
    %72 = stablehlo.reshape %71 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16> loc(#loc81)
    %73 = stablehlo.transpose %72, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16> loc(#loc82)
    %74 = stablehlo.dot_general %32, %73, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16> loc(#loc83)
    %75 = stablehlo.reshape %74 : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16> loc(#loc84)
    %76 = stablehlo.transpose %75, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16> loc(#loc85)
    %77 = stablehlo.broadcast_in_dim %54, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16> loc(#loc86)
    %78 = stablehlo.multiply %76, %77 : tensor<1x64x7x128xbf16> loc(#loc87)
    %79 = stablehlo.slice %76 [0:1, 0:64, 0:7, 64:128] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16> loc(#loc88)
    %80 = stablehlo.negate %79 : tensor<1x64x7x64xbf16> loc(#loc89)
    %81 = stablehlo.slice %76 [0:1, 0:64, 0:7, 0:64] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16> loc(#loc90)
    %82 = stablehlo.concatenate %80, %81, dim = 3 : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16> loc(#loc91)
    %83 = stablehlo.broadcast_in_dim %62, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16> loc(#loc92)
    %84 = stablehlo.multiply %82, %83 : tensor<1x64x7x128xbf16> loc(#loc93)
    %85 = stablehlo.add %78, %84 : tensor<1x64x7x128xbf16> loc(#loc94)
    %86 = stablehlo.broadcast_in_dim %65, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16> loc(#loc95)
    %87 = stablehlo.reshape %86 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16> loc(#loc96)
    %88 = stablehlo.transpose %87, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16> loc(#loc97)
    %89 = stablehlo.dot_general %85, %88, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16> loc(#loc98)
    %90 = stablehlo.multiply %89, %5 : tensor<1x64x7x7xbf16> loc(#loc99)
    %91 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64> loc(#loc100)
    %92 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64> loc(#loc101)
    %93 = stablehlo.subtract %91, %92 : tensor<7x7xi64> loc(#loc102)
    %94 = stablehlo.compare  GE, %93, %4 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1> loc(#loc103)
    %95 = stablehlo.select %94, %3, %2 : tensor<7x7xi1>, tensor<7x7xbf16> loc(#loc104)
    %96 = stablehlo.compare  GT, %91, %92 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1> loc(#loc105)
    %97 = stablehlo.convert %96 : (tensor<7x7xi1>) -> tensor<7x7xbf16> loc(#loc106)
    %98 = stablehlo.multiply %95, %97 : tensor<7x7xbf16> loc(#loc107)
    %99 = stablehlo.reshape %98 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16> loc(#loc108)
    %100 = stablehlo.reshape %arg10 : (tensor<1x7xi64>) -> tensor<1x1x7xi64> loc(#loc109)
    %101 = stablehlo.custom_call @tt.mark_argument(%100) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_1"}} : (tensor<1x1x7xi64>) -> tensor<1x1x7xi64> loc(#loc110)
    %102 = stablehlo.reshape %101 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64> loc(#loc111)
    %103 = stablehlo.convert %102 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16> loc(#loc112)
    %104 = stablehlo.reshape %103 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16> loc(#loc113)
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16> loc(#loc114)
    %106 = stablehlo.add %99, %105 : tensor<1x1x7x7xbf16> loc(#loc115)
    %107 = stablehlo.compare  EQ, %106, %1 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1> loc(#loc116)
    %108 = stablehlo.select %107, %0, %99 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16> loc(#loc117)
    %109 = stablehlo.reshape %108 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16> loc(#loc118)
    %110 = stablehlo.broadcast_in_dim %109, dims = [0, 2, 3] : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16> loc(#loc119)
    %111 = stablehlo.add %90, %110 : tensor<1x64x7x7xbf16> loc(#loc120)
    %112 = stablehlo.convert %111 : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32> loc(#loc121)
    %113 = stablehlo.reduce(%112 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32> loc(#loc122)
    %114 = stablehlo.broadcast_in_dim %113, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32> loc(#loc123)
    %115 = stablehlo.subtract %112, %114 : tensor<1x64x7x7xf32> loc(#loc124)
    %116 = stablehlo.exponential %115 : tensor<1x64x7x7xf32> loc(#loc125)
    %117 = stablehlo.reduce(%116 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32> loc(#loc126)
    %118 = stablehlo.broadcast_in_dim %117, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32> loc(#loc127)
    %119 = stablehlo.divide %116, %118 : tensor<1x64x7x7xf32> loc(#loc128)
    %120 = stablehlo.convert %119 : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16> loc(#loc129)
    %121 = stablehlo.broadcast_in_dim %39, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16> loc(#loc130)
    %122 = stablehlo.reshape %121 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16> loc(#loc131)
    %123 = stablehlo.dot_general %120, %122, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16> loc(#loc132)
    %124 = stablehlo.transpose %123, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16> loc(#loc133)
    %125 = stablehlo.reshape %124 : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16> loc(#loc134)
    %126 = stablehlo.reshape %arg9 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16> loc(#loc135)
    %127 = stablehlo.custom_call @tt.mark_argument(%126) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}} : (tensor<1x8192x8192xbf16>) -> tensor<1x8192x8192xbf16> loc(#loc136)
    %128 = stablehlo.reshape %127 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16> loc(#loc137)
    %129 = stablehlo.transpose %128, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16> loc(#loc138)
    %130 = stablehlo.dot_general %125, %129, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16> loc(#loc139)
    %131 = stablehlo.reshape %130 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16> loc(#loc140)
    %132 = stablehlo.add %19, %131 : tensor<1x7x8192xbf16> loc(#loc141)
    %133 = stablehlo.reshape %arg12 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16> loc(#loc142)
    %134 = stablehlo.custom_call @tt.mark_argument(%133) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}} : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xbf16> loc(#loc143)
    %135 = stablehlo.reshape %134 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16> loc(#loc144)
    %136 = stablehlo.broadcast_in_dim %135, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16> loc(#loc145)
    %137 = stablehlo.convert %132 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32> loc(#loc146)
    %138 = stablehlo.power %137, %6 : tensor<1x7x8192xf32> loc(#loc147)
    %139 = stablehlo.reduce(%138 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32> loc(#loc148)
    %140 = stablehlo.multiply %139, %cst_2 : tensor<1x7xf32> loc(#loc149)
    %141 = stablehlo.reshape %140 : (tensor<1x7xf32>) -> tensor<1x7x1xf32> loc(#loc150)
    %142 = stablehlo.add %141, %cst_3 : tensor<1x7x1xf32> loc(#loc151)
    %143 = stablehlo.rsqrt %142 : tensor<1x7x1xf32> loc(#loc152)
    %144 = stablehlo.reshape %143 : (tensor<1x7x1xf32>) -> tensor<1x7xf32> loc(#loc153)
    %145 = stablehlo.broadcast_in_dim %144, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32> loc(#loc154)
    %146 = stablehlo.multiply %137, %145 : tensor<1x7x8192xf32> loc(#loc155)
    %147 = stablehlo.convert %146 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16> loc(#loc156)
    %148 = stablehlo.multiply %136, %147 : tensor<1x7x8192xbf16> loc(#loc157)
    %149 = stablehlo.reshape %148 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16> loc(#loc158)
    %150 = stablehlo.reshape %arg13 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16> loc(#loc159)
    %151 = stablehlo.custom_call @tt.mark_argument(%150) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}} : (tensor<1x28672x8192xbf16>) -> tensor<1x28672x8192xbf16> loc(#loc160)
    %152 = stablehlo.reshape %151 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16> loc(#loc161)
    %153 = stablehlo.transpose %152, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16> loc(#loc162)
    %154 = stablehlo.dot_general %149, %153, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16> loc(#loc163)
    %155 = stablehlo.reshape %154 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16> loc(#loc164)
    %156 = stablehlo.logistic %155 : tensor<1x7x28672xbf16> loc(#loc165)
    %157 = stablehlo.multiply %155, %156 : tensor<1x7x28672xbf16> loc(#loc166)
    %158 = stablehlo.reshape %arg8 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16> loc(#loc167)
    %159 = stablehlo.custom_call @tt.mark_argument(%158) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}} : (tensor<1x28672x8192xbf16>) -> tensor<1x28672x8192xbf16> loc(#loc168)
    %160 = stablehlo.reshape %159 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16> loc(#loc169)
    %161 = stablehlo.transpose %160, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16> loc(#loc170)
    %162 = stablehlo.dot_general %149, %161, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16> loc(#loc171)
    %163 = stablehlo.reshape %162 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16> loc(#loc172)
    %164 = stablehlo.multiply %157, %163 : tensor<1x7x28672xbf16> loc(#loc173)
    %165 = stablehlo.reshape %164 : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16> loc(#loc174)
    %166 = stablehlo.reshape %arg7 : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16> loc(#loc175)
    %167 = stablehlo.custom_call @tt.mark_argument(%166) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}} : (tensor<1x8192x28672xbf16>) -> tensor<1x8192x28672xbf16> loc(#loc176)
    %168 = stablehlo.reshape %167 : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16> loc(#loc177)
    %169 = stablehlo.transpose %168, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16> loc(#loc178)
    %170 = stablehlo.dot_general %165, %169, contracting_dims = [1] x [0] : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16> loc(#loc179)
    %171 = stablehlo.reshape %170 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16> loc(#loc180)
    %172 = stablehlo.add %132, %171 : tensor<1x7x8192xbf16> loc(#loc181)
    %173 = stablehlo.convert %172 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32> loc(#loc182)
    %174 = stablehlo.power %173, %6 : tensor<1x7x8192xf32> loc(#loc183)
    %175 = stablehlo.reduce(%174 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32> loc(#loc184)
    %176 = stablehlo.multiply %175, %cst_2 : tensor<1x7xf32> loc(#loc185)
    %177 = stablehlo.reshape %176 : (tensor<1x7xf32>) -> tensor<1x7x1xf32> loc(#loc186)
    %178 = stablehlo.add %177, %cst_3 : tensor<1x7x1xf32> loc(#loc187)
    %179 = stablehlo.rsqrt %178 : tensor<1x7x1xf32> loc(#loc188)
    %180 = stablehlo.reshape %179 : (tensor<1x7x1xf32>) -> tensor<1x7xf32> loc(#loc189)
    %181 = stablehlo.broadcast_in_dim %180, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32> loc(#loc190)
    %182 = stablehlo.multiply %173, %181 : tensor<1x7x8192xf32> loc(#loc191)
    %183 = stablehlo.convert %182 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16> loc(#loc192)
    %184 = stablehlo.multiply %69, %183 : tensor<1x7x8192xbf16> loc(#loc193)
    %185 = stablehlo.reshape %184 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16> loc(#loc194)
    %186 = stablehlo.reshape %arg6 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16> loc(#loc195)
    %187 = stablehlo.custom_call @tt.mark_argument(%186) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___lm_head_weight"}} : (tensor<1x128256x8192xbf16>) -> tensor<1x128256x8192xbf16> loc(#loc196)
    %188 = stablehlo.reshape %187 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16> loc(#loc197)
    %189 = stablehlo.transpose %188, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16> loc(#loc198)
    %190 = stablehlo.dot_general %185, %189, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16> loc(#loc199)
    %191 = stablehlo.reshape %190 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16> loc(#loc200)
    return %39, %65, %191 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc16 = loc("reshape.51")
#loc17 = loc("custom-call.52")
#loc18 = loc("reshape.53")
#loc19 = loc("broadcast.54")
#loc20 = loc("reshape.15")
#loc21 = loc("custom-call.16")
#loc22 = loc("reshape.17")
#loc23 = loc("reshape.10")
#loc24 = loc("custom-call.11")
#loc25 = loc("reshape.13")
#loc26 = loc("convert.18")
#loc27 = loc("gather.19")
#loc28 = loc("reshape.20")
#loc29 = loc("convert.21")
#loc30 = loc("power.23")
#loc31 = loc("reduce.30")
#loc32 = loc("multiply.39")
#loc33 = loc("reshape.40")
#loc34 = loc("add.44")
#loc35 = loc("rsqrt.45")
#loc36 = loc("reshape.46")
#loc37 = loc("broadcast.47")
#loc38 = loc("multiply.48")
#loc39 = loc("convert.49")
#loc40 = loc("multiply.55")
#loc41 = loc("reshape.56")
#loc42 = loc("reshape.2")
#loc43 = loc("custom-call.3")
#loc44 = loc("reshape.4")
#loc45 = loc("transpose.5")
#loc46 = loc("dot.57")
#loc47 = loc("reshape.59")
#loc48 = loc("transpose.60")
#loc49 = loc("reshape.88")
#loc50 = loc("custom-call.89")
#loc51 = loc("reshape.90")
#loc52 = loc("transpose.91")
#loc53 = loc("dot.93")
#loc54 = loc("reshape.95")
#loc55 = loc("transpose.96")
#loc56 = loc("reshape.71")
#loc57 = loc("custom-call.72")
#loc58 = loc("reshape.76")
#loc59 = loc("dot.79")
#loc60 = loc("transpose.80")
#loc61 = loc("concatenate.81")
#loc62 = loc("cosine.105")
#loc63 = loc("convert.108")
#loc64 = loc("broadcast.111")
#loc65 = loc("multiply.112")
#loc66 = loc("slice.98")
#loc67 = loc("negate.99")
#loc68 = loc("slice.97")
#loc69 = loc("concatenate.100")
#loc70 = loc("sine.82")
#loc71 = loc("convert.85")
#loc72 = loc("broadcast.102")
#loc73 = loc("multiply.103")
#loc74 = loc("add.115")
#loc75 = loc("reshape.394")
#loc76 = loc("custom-call.395")
#loc77 = loc("reshape.396")
#loc78 = loc("broadcast.397")
#loc79 = loc("reshape.249")
#loc80 = loc("custom-call.250")
#loc81 = loc("reshape.251")
#loc82 = loc("transpose.252")
#loc83 = loc("dot.254")
#loc84 = loc("reshape.256")
#loc85 = loc("transpose.257")
#loc86 = loc("broadcast.266")
#loc87 = loc("multiply.267")
#loc88 = loc("slice.259")
#loc89 = loc("negate.260")
#loc90 = loc("slice.258")
#loc91 = loc("concatenate.261")
#loc92 = loc("broadcast.263")
#loc93 = loc("multiply.264")
#loc94 = loc("add.270")
#loc95 = loc("broadcast.244")
#loc96 = loc("reshape.245")
#loc97 = loc("transpose.246")
#loc98 = loc("dot.271")
#loc99 = loc("multiply.274")
#loc100 = loc("broadcast.182")
#loc101 = loc("broadcast.184")
#loc102 = loc("subtract.185")
#loc103 = loc("compare.187")
#loc104 = loc("select.189")
#loc105 = loc("compare.159")
#loc106 = loc("convert.160")
#loc107 = loc("multiply.190")
#loc108 = loc("reshape.192")
#loc109 = loc("reshape.209")
#loc110 = loc("custom-call.210")
#loc111 = loc("reshape.214")
#loc112 = loc("convert.219")
#loc113 = loc("reshape.222")
#loc114 = loc("broadcast.223")
#loc115 = loc("add.224")
#loc116 = loc("compare.227")
#loc117 = loc("select.229")
#loc118 = loc("reshape.277")
#loc119 = loc("broadcast.278")
#loc120 = loc("add.279")
#loc121 = loc("convert.280")
#loc122 = loc("reduce.286")
#loc123 = loc("broadcast.287")
#loc124 = loc("subtract.288")
#loc125 = loc("exponential.289")
#loc126 = loc("reduce.295")
#loc127 = loc("broadcast.296")
#loc128 = loc("divide.297")
#loc129 = loc("convert.298")
#loc130 = loc("broadcast.151")
#loc131 = loc("reshape.152")
#loc132 = loc("dot.299")
#loc133 = loc("transpose.301")
#loc134 = loc("reshape.303")
#loc135 = loc("reshape.140")
#loc136 = loc("custom-call.141")
#loc137 = loc("reshape.142")
#loc138 = loc("transpose.143")
#loc139 = loc("dot.304")
#loc140 = loc("reshape.305")
#loc141 = loc("add.308")
#loc142 = loc("reshape.339")
#loc143 = loc("custom-call.340")
#loc144 = loc("reshape.341")
#loc145 = loc("broadcast.342")
#loc146 = loc("convert.309")
#loc147 = loc("power.311")
#loc148 = loc("reduce.318")
#loc149 = loc("multiply.327")
#loc150 = loc("reshape.328")
#loc151 = loc("add.332")
#loc152 = loc("rsqrt.333")
#loc153 = loc("reshape.334")
#loc154 = loc("broadcast.335")
#loc155 = loc("multiply.336")
#loc156 = loc("convert.337")
#loc157 = loc("multiply.343")
#loc158 = loc("reshape.352")
#loc159 = loc("reshape.348")
#loc160 = loc("custom-call.349")
#loc161 = loc("reshape.350")
#loc162 = loc("transpose.351")
#loc163 = loc("dot.353")
#loc164 = loc("reshape.354")
#loc165 = loc("logistic.355")
#loc166 = loc("multiply.356")
#loc167 = loc("reshape.131")
#loc168 = loc("custom-call.132")
#loc169 = loc("reshape.133")
#loc170 = loc("transpose.134")
#loc171 = loc("dot.345")
#loc172 = loc("reshape.346")
#loc173 = loc("multiply.357")
#loc174 = loc("reshape.358")
#loc175 = loc("reshape.126")
#loc176 = loc("custom-call.127")
#loc177 = loc("reshape.128")
#loc178 = loc("transpose.129")
#loc179 = loc("dot.359")
#loc180 = loc("reshape.360")
#loc181 = loc("add.363")
#loc182 = loc("convert.364")
#loc183 = loc("power.366")
#loc184 = loc("reduce.373")
#loc185 = loc("multiply.382")
#loc186 = loc("reshape.383")
#loc187 = loc("add.387")
#loc188 = loc("rsqrt.388")
#loc189 = loc("reshape.389")
#loc190 = loc("broadcast.390")
#loc191 = loc("multiply.391")
#loc192 = loc("convert.392")
#loc193 = loc("multiply.398")
#loc194 = loc("reshape.402")
#loc195 = loc("reshape.117")
#loc196 = loc("custom-call.118")
#loc197 = loc("reshape.119")
#loc198 = loc("transpose.120")
#loc199 = loc("dot.403")
#loc200 = loc("reshape.404")
------------------ END OF MLIR MODULE ------------------
2025-12-04 19:55:23.199 (  69.349s) [        CE2D1B80]      module_builder.cc:971      1| MLIR Module shlo_frontend:
#loc1 = loc("p0.1")
#loc2 = loc("p1.9")
#loc3 = loc("p2.14")
#loc4 = loc("p3.50")
#loc5 = loc("p4.70")
#loc6 = loc("p5.87")
#loc7 = loc("p6.116")
#loc8 = loc("p7.125")
#loc9 = loc("p8.130")
#loc10 = loc("p9.139")
#loc11 = loc("p10.208")
#loc12 = loc("p11.248")
#loc13 = loc("p12.338")
#loc14 = loc("p13.347")
#loc15 = loc("p14.393")
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8, \22_axis_1\22=4]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1024x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"} loc("p0.1"), %arg1: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"} loc("p1.9"), %arg2: tensor<128256x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8,4]<=[32] last_tile_dim_replicate}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_embed_tokens_weight"} loc("p2.14"), %arg3: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"} loc("p3.50"), %arg4: tensor<64xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "l__self___model_rotary_emb_inv_freq"} loc("p4.70"), %arg5: tensor<1024x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"} loc("p5.87"), %arg6: tensor<128256x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8,4]<=[32] last_tile_dim_replicate}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___lm_head_weight"} loc("p6.116"), %arg7: tensor<8192x28672xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[4,8]<=[8,4]T(1,0)}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"} loc("p7.125"), %arg8: tensor<28672x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"} loc("p8.130"), %arg9: tensor<8192x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[4,8]<=[8,4]T(1,0)}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"} loc("p9.139"), %arg10: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_1"} loc("p10.208"), %arg11: tensor<8192x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"} loc("p11.248"), %arg12: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"} loc("p12.338"), %arg13: tensor<28672x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"} loc("p13.347"), %arg14: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_norm_weight"} loc("p14.393")) -> (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64> loc(#loc)
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)
    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32> loc(#loc)
    %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32> loc(#loc)
    %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32> loc(#loc)
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32> loc(#loc)
    %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16> loc(#loc)
    %c_6 = stablehlo.constant dense<1> : tensor<i64> loc(#loc)
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16> loc(#loc)
    %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16> loc(#loc)
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16> loc(#loc)
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16> loc(#loc)
    %4 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64> loc(#loc)
    %5 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<bf16>) -> tensor<1x64x7x7xbf16> loc(#loc)
    %6 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32> loc(#loc)
    %7 = stablehlo.reshape %arg3 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16> loc(#loc16)
    %8 = stablehlo.reshape %7 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16> loc(#loc17)
    %9 = stablehlo.broadcast_in_dim %8, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16> loc(#loc18)
    %10 = stablehlo.reshape %arg2 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16> loc(#loc19)
    %11 = stablehlo.reshape %10 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16> loc(#loc20)
    %12 = stablehlo.reshape %arg1 : (tensor<1x7xi64>) -> tensor<1x1x7xi64> loc(#loc21)
    %13 = stablehlo.reshape %12 : (tensor<1x1x7xi64>) -> tensor<7xi64> loc(#loc22)
    %14 = stablehlo.convert %13 : (tensor<7xi64>) -> tensor<7xui32> loc(#loc23)
    %15 = "stablehlo.gather"(%11, %14) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16> loc(#loc24)
    %16 = stablehlo.reshape %15 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16> loc(#loc25)
    %17 = stablehlo.convert %16 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32> loc(#loc26)
    %18 = stablehlo.power %17, %6 : tensor<1x7x8192xf32> loc(#loc27)
    %19 = stablehlo.reduce(%18 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32> loc(#loc28)
    %20 = stablehlo.multiply %19, %cst_2 : tensor<1x7xf32> loc(#loc29)
    %21 = stablehlo.reshape %20 : (tensor<1x7xf32>) -> tensor<1x7x1xf32> loc(#loc30)
    %22 = stablehlo.add %21, %cst_3 : tensor<1x7x1xf32> loc(#loc31)
    %23 = stablehlo.rsqrt %22 : tensor<1x7x1xf32> loc(#loc32)
    %24 = stablehlo.reshape %23 : (tensor<1x7x1xf32>) -> tensor<1x7xf32> loc(#loc33)
    %25 = stablehlo.broadcast_in_dim %24, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32> loc(#loc34)
    %26 = stablehlo.multiply %17, %25 : tensor<1x7x8192xf32> loc(#loc35)
    %27 = stablehlo.convert %26 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16> loc(#loc36)
    %28 = stablehlo.multiply %9, %27 : tensor<1x7x8192xbf16> loc(#loc37)
    %29 = stablehlo.reshape %28 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16> loc(#loc38)
    %30 = stablehlo.reshape %arg0 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16> loc(#loc39)
    %31 = stablehlo.reshape %30 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16> loc(#loc40)
    %32 = stablehlo.transpose %31, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16> loc(#loc41)
    %33 = stablehlo.dot_general %29, %32, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16> loc(#loc42)
    %34 = stablehlo.reshape %33 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16> loc(#loc43)
    %35 = stablehlo.transpose %34, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16> loc(#loc44)
    %36 = stablehlo.reshape %arg5 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16> loc(#loc45)
    %37 = stablehlo.reshape %36 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16> loc(#loc46)
    %38 = stablehlo.transpose %37, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16> loc(#loc47)
    %39 = stablehlo.dot_general %29, %38, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16> loc(#loc48)
    %40 = stablehlo.reshape %39 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16> loc(#loc49)
    %41 = stablehlo.transpose %40, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16> loc(#loc50)
    %42 = stablehlo.reshape %arg4 : (tensor<64xf32>) -> tensor<1x1x64xf32> loc(#loc51)
    %43 = stablehlo.reshape %42 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32> loc(#loc52)
    %44 = stablehlo.dot_general %43, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32> loc(#loc53)
    %45 = stablehlo.transpose %44, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32> loc(#loc54)
    %46 = stablehlo.concatenate %45, %45, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32> loc(#loc55)
    %47 = stablehlo.cosine %46 : tensor<1x7x128xf32> loc(#loc56)
    %48 = stablehlo.convert %47 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16> loc(#loc57)
    %49 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16> loc(#loc58)
    %50 = stablehlo.multiply %41, %49 : tensor<1x8x7x128xbf16> loc(#loc59)
    %51 = stablehlo.slice %41 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16> loc(#loc60)
    %52 = stablehlo.negate %51 : tensor<1x8x7x64xbf16> loc(#loc61)
    %53 = stablehlo.slice %41 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16> loc(#loc62)
    %54 = stablehlo.concatenate %52, %53, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16> loc(#loc63)
    %55 = stablehlo.sine %46 : tensor<1x7x128xf32> loc(#loc64)
    %56 = stablehlo.convert %55 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16> loc(#loc65)
    %57 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16> loc(#loc66)
    %58 = stablehlo.multiply %54, %57 : tensor<1x8x7x128xbf16> loc(#loc67)
    %59 = stablehlo.add %50, %58 : tensor<1x8x7x128xbf16> loc(#loc68)
    %60 = stablehlo.reshape %arg14 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16> loc(#loc69)
    %61 = stablehlo.reshape %60 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16> loc(#loc70)
    %62 = stablehlo.broadcast_in_dim %61, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16> loc(#loc71)
    %63 = stablehlo.reshape %arg11 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16> loc(#loc72)
    %64 = stablehlo.reshape %63 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16> loc(#loc73)
    %65 = stablehlo.transpose %64, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16> loc(#loc74)
    %66 = stablehlo.dot_general %29, %65, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16> loc(#loc75)
    %67 = stablehlo.reshape %66 : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16> loc(#loc76)
    %68 = stablehlo.transpose %67, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16> loc(#loc77)
    %69 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16> loc(#loc78)
    %70 = stablehlo.multiply %68, %69 : tensor<1x64x7x128xbf16> loc(#loc79)
    %71 = stablehlo.slice %68 [0:1, 0:64, 0:7, 64:128] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16> loc(#loc80)
    %72 = stablehlo.negate %71 : tensor<1x64x7x64xbf16> loc(#loc81)
    %73 = stablehlo.slice %68 [0:1, 0:64, 0:7, 0:64] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16> loc(#loc82)
    %74 = stablehlo.concatenate %72, %73, dim = 3 : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16> loc(#loc83)
    %75 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16> loc(#loc84)
    %76 = stablehlo.multiply %74, %75 : tensor<1x64x7x128xbf16> loc(#loc85)
    %77 = stablehlo.add %70, %76 : tensor<1x64x7x128xbf16> loc(#loc86)
    %78 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16> loc(#loc87)
    %79 = stablehlo.reshape %78 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16> loc(#loc88)
    %80 = stablehlo.transpose %79, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16> loc(#loc89)
    %81 = stablehlo.dot_general %77, %80, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16> loc(#loc90)
    %82 = stablehlo.multiply %81, %5 : tensor<1x64x7x7xbf16> loc(#loc91)
    %83 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64> loc(#loc92)
    %84 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64> loc(#loc93)
    %85 = stablehlo.subtract %83, %84 : tensor<7x7xi64> loc(#loc94)
    %86 = stablehlo.compare  GE, %85, %4 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1> loc(#loc95)
    %87 = stablehlo.select %86, %3, %2 : tensor<7x7xi1>, tensor<7x7xbf16> loc(#loc96)
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1> loc(#loc97)
    %89 = stablehlo.convert %88 : (tensor<7x7xi1>) -> tensor<7x7xbf16> loc(#loc98)
    %90 = stablehlo.multiply %87, %89 : tensor<7x7xbf16> loc(#loc99)
    %91 = stablehlo.reshape %90 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16> loc(#loc100)
    %92 = stablehlo.reshape %arg10 : (tensor<1x7xi64>) -> tensor<1x1x7xi64> loc(#loc101)
    %93 = stablehlo.reshape %92 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64> loc(#loc102)
    %94 = stablehlo.convert %93 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16> loc(#loc103)
    %95 = stablehlo.reshape %94 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16> loc(#loc104)
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16> loc(#loc105)
    %97 = stablehlo.add %91, %96 : tensor<1x1x7x7xbf16> loc(#loc106)
    %98 = stablehlo.compare  EQ, %97, %1 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1> loc(#loc107)
    %99 = stablehlo.select %98, %0, %91 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16> loc(#loc108)
    %100 = stablehlo.reshape %99 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16> loc(#loc109)
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16> loc(#loc110)
    %102 = stablehlo.add %82, %101 : tensor<1x64x7x7xbf16> loc(#loc111)
    %103 = stablehlo.convert %102 : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32> loc(#loc112)
    %104 = stablehlo.reduce(%103 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32> loc(#loc113)
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32> loc(#loc114)
    %106 = stablehlo.subtract %103, %105 : tensor<1x64x7x7xf32> loc(#loc115)
    %107 = stablehlo.exponential %106 : tensor<1x64x7x7xf32> loc(#loc116)
    %108 = stablehlo.reduce(%107 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32> loc(#loc117)
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32> loc(#loc118)
    %110 = stablehlo.divide %107, %109 : tensor<1x64x7x7xf32> loc(#loc119)
    %111 = stablehlo.convert %110 : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16> loc(#loc120)
    %112 = stablehlo.broadcast_in_dim %35, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16> loc(#loc121)
    %113 = stablehlo.reshape %112 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16> loc(#loc122)
    %114 = stablehlo.dot_general %111, %113, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16> loc(#loc123)
    %115 = stablehlo.transpose %114, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16> loc(#loc124)
    %116 = stablehlo.reshape %115 : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16> loc(#loc125)
    %117 = stablehlo.reshape %arg9 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16> loc(#loc126)
    %118 = stablehlo.reshape %117 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16> loc(#loc127)
    %119 = stablehlo.transpose %118, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16> loc(#loc128)
    %120 = stablehlo.dot_general %116, %119, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16> loc(#loc129)
    %121 = stablehlo.reshape %120 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16> loc(#loc130)
    %122 = stablehlo.add %16, %121 : tensor<1x7x8192xbf16> loc(#loc131)
    %123 = stablehlo.reshape %arg12 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16> loc(#loc132)
    %124 = stablehlo.reshape %123 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16> loc(#loc133)
    %125 = stablehlo.broadcast_in_dim %124, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16> loc(#loc134)
    %126 = stablehlo.convert %122 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32> loc(#loc135)
    %127 = stablehlo.power %126, %6 : tensor<1x7x8192xf32> loc(#loc136)
    %128 = stablehlo.reduce(%127 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32> loc(#loc137)
    %129 = stablehlo.multiply %128, %cst_2 : tensor<1x7xf32> loc(#loc138)
    %130 = stablehlo.reshape %129 : (tensor<1x7xf32>) -> tensor<1x7x1xf32> loc(#loc139)
    %131 = stablehlo.add %130, %cst_3 : tensor<1x7x1xf32> loc(#loc140)
    %132 = stablehlo.rsqrt %131 : tensor<1x7x1xf32> loc(#loc141)
    %133 = stablehlo.reshape %132 : (tensor<1x7x1xf32>) -> tensor<1x7xf32> loc(#loc142)
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32> loc(#loc143)
    %135 = stablehlo.multiply %126, %134 : tensor<1x7x8192xf32> loc(#loc144)
    %136 = stablehlo.convert %135 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16> loc(#loc145)
    %137 = stablehlo.multiply %125, %136 : tensor<1x7x8192xbf16> loc(#loc146)
    %138 = stablehlo.reshape %137 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16> loc(#loc147)
    %139 = stablehlo.reshape %arg13 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16> loc(#loc148)
    %140 = stablehlo.reshape %139 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16> loc(#loc149)
    %141 = stablehlo.transpose %140, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16> loc(#loc150)
    %142 = stablehlo.dot_general %138, %141, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16> loc(#loc151)
    %143 = stablehlo.reshape %142 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16> loc(#loc152)
    %144 = stablehlo.logistic %143 : tensor<1x7x28672xbf16> loc(#loc153)
    %145 = stablehlo.multiply %143, %144 : tensor<1x7x28672xbf16> loc(#loc154)
    %146 = stablehlo.reshape %arg8 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16> loc(#loc155)
    %147 = stablehlo.reshape %146 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16> loc(#loc156)
    %148 = stablehlo.transpose %147, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16> loc(#loc157)
    %149 = stablehlo.dot_general %138, %148, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16> loc(#loc158)
    %150 = stablehlo.reshape %149 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16> loc(#loc159)
    %151 = stablehlo.multiply %145, %150 : tensor<1x7x28672xbf16> loc(#loc160)
    %152 = stablehlo.reshape %151 : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16> loc(#loc161)
    %153 = stablehlo.reshape %arg7 : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16> loc(#loc162)
    %154 = stablehlo.reshape %153 : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16> loc(#loc163)
    %155 = stablehlo.transpose %154, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16> loc(#loc164)
    %156 = stablehlo.dot_general %152, %155, contracting_dims = [1] x [0] : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16> loc(#loc165)
    %157 = stablehlo.reshape %156 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16> loc(#loc166)
    %158 = stablehlo.add %122, %157 : tensor<1x7x8192xbf16> loc(#loc167)
    %159 = stablehlo.convert %158 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32> loc(#loc168)
    %160 = stablehlo.power %159, %6 : tensor<1x7x8192xf32> loc(#loc169)
    %161 = stablehlo.reduce(%160 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32> loc(#loc170)
    %162 = stablehlo.multiply %161, %cst_2 : tensor<1x7xf32> loc(#loc171)
    %163 = stablehlo.reshape %162 : (tensor<1x7xf32>) -> tensor<1x7x1xf32> loc(#loc172)
    %164 = stablehlo.add %163, %cst_3 : tensor<1x7x1xf32> loc(#loc173)
    %165 = stablehlo.rsqrt %164 : tensor<1x7x1xf32> loc(#loc174)
    %166 = stablehlo.reshape %165 : (tensor<1x7x1xf32>) -> tensor<1x7xf32> loc(#loc175)
    %167 = stablehlo.broadcast_in_dim %166, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32> loc(#loc176)
    %168 = stablehlo.multiply %159, %167 : tensor<1x7x8192xf32> loc(#loc177)
    %169 = stablehlo.convert %168 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16> loc(#loc178)
    %170 = stablehlo.multiply %62, %169 : tensor<1x7x8192xbf16> loc(#loc179)
    %171 = stablehlo.reshape %170 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16> loc(#loc180)
    %172 = stablehlo.reshape %arg6 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16> loc(#loc181)
    %173 = stablehlo.reshape %172 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16> loc(#loc182)
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16> loc(#loc183)
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16> loc(#loc184)
    %176 = stablehlo.reshape %175 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16> loc(#loc185)
    return %35, %59, %176 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc16 = loc("reshape.51")
#loc17 = loc("reshape.53")
#loc18 = loc("broadcast.54")
#loc19 = loc("reshape.15")
#loc20 = loc("reshape.17")
#loc21 = loc("reshape.10")
#loc22 = loc("reshape.13")
#loc23 = loc("convert.18")
#loc24 = loc("gather.19")
#loc25 = loc("reshape.20")
#loc26 = loc("convert.21")
#loc27 = loc("power.23")
#loc28 = loc("reduce.30")
#loc29 = loc("multiply.39")
#loc30 = loc("reshape.40")
#loc31 = loc("add.44")
#loc32 = loc("rsqrt.45")
#loc33 = loc("reshape.46")
#loc34 = loc("broadcast.47")
#loc35 = loc("multiply.48")
#loc36 = loc("convert.49")
#loc37 = loc("multiply.55")
#loc38 = loc("reshape.56")
#loc39 = loc("reshape.2")
#loc40 = loc("reshape.4")
#loc41 = loc("transpose.5")
#loc42 = loc("dot.57")
#loc43 = loc("reshape.59")
#loc44 = loc("transpose.60")
#loc45 = loc("reshape.88")
#loc46 = loc("reshape.90")
#loc47 = loc("transpose.91")
#loc48 = loc("dot.93")
#loc49 = loc("reshape.95")
#loc50 = loc("transpose.96")
#loc51 = loc("reshape.71")
#loc52 = loc("reshape.76")
#loc53 = loc("dot.79")
#loc54 = loc("transpose.80")
#loc55 = loc("concatenate.81")
#loc56 = loc("cosine.105")
#loc57 = loc("convert.108")
#loc58 = loc("broadcast.111")
#loc59 = loc("multiply.112")
#loc60 = loc("slice.98")
#loc61 = loc("negate.99")
#loc62 = loc("slice.97")
#loc63 = loc("concatenate.100")
#loc64 = loc("sine.82")
#loc65 = loc("convert.85")
#loc66 = loc("broadcast.102")
#loc67 = loc("multiply.103")
#loc68 = loc("add.115")
#loc69 = loc("reshape.394")
#loc70 = loc("reshape.396")
#loc71 = loc("broadcast.397")
#loc72 = loc("reshape.249")
#loc73 = loc("reshape.251")
#loc74 = loc("transpose.252")
#loc75 = loc("dot.254")
#loc76 = loc("reshape.256")
#loc77 = loc("transpose.257")
#loc78 = loc("broadcast.266")
#loc79 = loc("multiply.267")
#loc80 = loc("slice.259")
#loc81 = loc("negate.260")
#loc82 = loc("slice.258")
#loc83 = loc("concatenate.261")
#loc84 = loc("broadcast.263")
#loc85 = loc("multiply.264")
#loc86 = loc("add.270")
#loc87 = loc("broadcast.244")
#loc88 = loc("reshape.245")
#loc89 = loc("transpose.246")
#loc90 = loc("dot.271")
#loc91 = loc("multiply.274")
#loc92 = loc("broadcast.182")
#loc93 = loc("broadcast.184")
#loc94 = loc("subtract.185")
#loc95 = loc("compare.187")
#loc96 = loc("select.189")
#loc97 = loc("compare.159")
#loc98 = loc("convert.160")
#loc99 = loc("multiply.190")
#loc100 = loc("reshape.192")
#loc101 = loc("reshape.209")
#loc102 = loc("reshape.214")
#loc103 = loc("convert.219")
#loc104 = loc("reshape.222")
#loc105 = loc("broadcast.223")
#loc106 = loc("add.224")
#loc107 = loc("compare.227")
#loc108 = loc("select.229")
#loc109 = loc("reshape.277")
#loc110 = loc("broadcast.278")
#loc111 = loc("add.279")
#loc112 = loc("convert.280")
#loc113 = loc("reduce.286")
#loc114 = loc("broadcast.287")
#loc115 = loc("subtract.288")
#loc116 = loc("exponential.289")
#loc117 = loc("reduce.295")
#loc118 = loc("broadcast.296")
#loc119 = loc("divide.297")
#loc120 = loc("convert.298")
#loc121 = loc("broadcast.151")
#loc122 = loc("reshape.152")
#loc123 = loc("dot.299")
#loc124 = loc("transpose.301")
#loc125 = loc("reshape.303")
#loc126 = loc("reshape.140")
#loc127 = loc("reshape.142")
#loc128 = loc("transpose.143")
#loc129 = loc("dot.304")
#loc130 = loc("reshape.305")
#loc131 = loc("add.308")
#loc132 = loc("reshape.339")
#loc133 = loc("reshape.341")
#loc134 = loc("broadcast.342")
#loc135 = loc("convert.309")
#loc136 = loc("power.311")
#loc137 = loc("reduce.318")
#loc138 = loc("multiply.327")
#loc139 = loc("reshape.328")
#loc140 = loc("add.332")
#loc141 = loc("rsqrt.333")
#loc142 = loc("reshape.334")
#loc143 = loc("broadcast.335")
#loc144 = loc("multiply.336")
#loc145 = loc("convert.337")
#loc146 = loc("multiply.343")
#loc147 = loc("reshape.352")
#loc148 = loc("reshape.348")
#loc149 = loc("reshape.350")
#loc150 = loc("transpose.351")
#loc151 = loc("dot.353")
#loc152 = loc("reshape.354")
#loc153 = loc("logistic.355")
#loc154 = loc("multiply.356")
#loc155 = loc("reshape.131")
#loc156 = loc("reshape.133")
#loc157 = loc("transpose.134")
#loc158 = loc("dot.345")
#loc159 = loc("reshape.346")
#loc160 = loc("multiply.357")
#loc161 = loc("reshape.358")
#loc162 = loc("reshape.126")
#loc163 = loc("reshape.128")
#loc164 = loc("transpose.129")
#loc165 = loc("dot.359")
#loc166 = loc("reshape.360")
#loc167 = loc("add.363")
#loc168 = loc("convert.364")
#loc169 = loc("power.366")
#loc170 = loc("reduce.373")
#loc171 = loc("multiply.382")
#loc172 = loc("reshape.383")
#loc173 = loc("add.387")
#loc174 = loc("rsqrt.388")
#loc175 = loc("reshape.389")
#loc176 = loc("broadcast.390")
#loc177 = loc("multiply.391")
#loc178 = loc("convert.392")
#loc179 = loc("multiply.398")
#loc180 = loc("reshape.402")
#loc181 = loc("reshape.117")
#loc182 = loc("reshape.119")
#loc183 = loc("transpose.120")
#loc184 = loc("dot.403")
#loc185 = loc("reshape.404")
------------------ END OF MLIR MODULE ------------------
// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.406) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8, \22_axis_1\22=4]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1024x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg1: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"}, %arg2: tensor<128256x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8,4]<=[32] last_tile_dim_replicate}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg3: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg4: tensor<64xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg5: tensor<1024x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg6: tensor<128256x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8,4]<=[32] last_tile_dim_replicate}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___lm_head_weight"}, %arg7: tensor<8192x28672xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[4,8]<=[8,4]T(1,0)}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg8: tensor<28672x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg9: tensor<8192x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[4,8]<=[8,4]T(1,0)}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg10: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_1"}, %arg11: tensor<8192x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg12: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg13: tensor<28672x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg14: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
    %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
    %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
    %c_6 = stablehlo.constant dense<1> : tensor<i64>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %4 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
    %5 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %7 = stablehlo.reshape %arg3 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %8 = stablehlo.reshape %7 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %9 = stablehlo.broadcast_in_dim %8, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %10 = stablehlo.reshape %arg2 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %12 = stablehlo.reshape %arg1 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %13 = stablehlo.reshape %12 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %14 = stablehlo.convert %13 : (tensor<7xi64>) -> tensor<7xui32>
    %15 = "stablehlo.gather"(%11, %14) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
    %16 = stablehlo.reshape %15 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %17 = stablehlo.convert %16 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %18 = stablehlo.power %17, %6 : tensor<1x7x8192xf32>
    %19 = stablehlo.reduce(%18 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %20 = stablehlo.multiply %19, %cst_2 : tensor<1x7xf32>
    %21 = stablehlo.reshape %20 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %22 = stablehlo.add %21, %cst_3 : tensor<1x7x1xf32>
    %23 = stablehlo.rsqrt %22 : tensor<1x7x1xf32>
    %24 = stablehlo.reshape %23 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %25 = stablehlo.broadcast_in_dim %24, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %26 = stablehlo.multiply %17, %25 : tensor<1x7x8192xf32>
    %27 = stablehlo.convert %26 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %28 = stablehlo.multiply %9, %27 : tensor<1x7x8192xbf16>
    %29 = stablehlo.reshape %28 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %30 = stablehlo.reshape %arg0 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %32 = stablehlo.transpose %31, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %33 = stablehlo.dot_general %29, %32, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %34 = stablehlo.reshape %33 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %35 = stablehlo.transpose %34, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %36 = stablehlo.reshape %arg5 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %37 = stablehlo.reshape %36 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %38 = stablehlo.transpose %37, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %39 = stablehlo.dot_general %29, %38, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %40 = stablehlo.reshape %39 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %41 = stablehlo.transpose %40, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %42 = stablehlo.reshape %arg4 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %43 = stablehlo.reshape %42 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %44 = stablehlo.dot_general %43, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %45 = stablehlo.transpose %44, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %46 = stablehlo.concatenate %45, %45, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %47 = stablehlo.cosine %46 : tensor<1x7x128xf32>
    %48 = stablehlo.convert %47 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %49 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %50 = stablehlo.multiply %41, %49 : tensor<1x8x7x128xbf16>
    %51 = stablehlo.slice %41 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %52 = stablehlo.negate %51 : tensor<1x8x7x64xbf16>
    %53 = stablehlo.slice %41 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %54 = stablehlo.concatenate %52, %53, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %55 = stablehlo.sine %46 : tensor<1x7x128xf32>
    %56 = stablehlo.convert %55 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %57 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.multiply %54, %57 : tensor<1x8x7x128xbf16>
    %59 = stablehlo.add %50, %58 : tensor<1x8x7x128xbf16>
    %60 = stablehlo.reshape %arg14 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %61 = stablehlo.reshape %60 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %62 = stablehlo.broadcast_in_dim %61, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %63 = stablehlo.reshape %arg11 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %64 = stablehlo.reshape %63 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %65 = stablehlo.transpose %64, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %66 = stablehlo.dot_general %29, %65, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %67 = stablehlo.reshape %66 : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
    %68 = stablehlo.transpose %67, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
    %69 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %70 = stablehlo.multiply %68, %69 : tensor<1x64x7x128xbf16>
    %71 = stablehlo.slice %68 [0:1, 0:64, 0:7, 64:128] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %72 = stablehlo.negate %71 : tensor<1x64x7x64xbf16>
    %73 = stablehlo.slice %68 [0:1, 0:64, 0:7, 0:64] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %74 = stablehlo.concatenate %72, %73, dim = 3 : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
    %75 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %76 = stablehlo.multiply %74, %75 : tensor<1x64x7x128xbf16>
    %77 = stablehlo.add %70, %76 : tensor<1x64x7x128xbf16>
    %78 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %79 = stablehlo.reshape %78 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %80 = stablehlo.transpose %79, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
    %81 = stablehlo.dot_general %77, %80, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
    %82 = stablehlo.multiply %81, %5 : tensor<1x64x7x7xbf16>
    %83 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %84 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %85 = stablehlo.subtract %83, %84 : tensor<7x7xi64>
    %86 = stablehlo.compare  GE, %85, %4 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %87 = stablehlo.select %86, %3, %2 : tensor<7x7xi1>, tensor<7x7xbf16>
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %89 = stablehlo.convert %88 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
    %90 = stablehlo.multiply %87, %89 : tensor<7x7xbf16>
    %91 = stablehlo.reshape %90 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
    %92 = stablehlo.reshape %arg10 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %93 = stablehlo.reshape %92 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
    %94 = stablehlo.convert %93 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
    %95 = stablehlo.reshape %94 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
    %97 = stablehlo.add %91, %96 : tensor<1x1x7x7xbf16>
    %98 = stablehlo.compare  EQ, %97, %1 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
    %99 = stablehlo.select %98, %0, %91 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
    %100 = stablehlo.reshape %99 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
    %102 = stablehlo.add %82, %101 : tensor<1x64x7x7xbf16>
    %103 = stablehlo.convert %102 : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
    %104 = stablehlo.reduce(%103 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %106 = stablehlo.subtract %103, %105 : tensor<1x64x7x7xf32>
    %107 = stablehlo.exponential %106 : tensor<1x64x7x7xf32>
    %108 = stablehlo.reduce(%107 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %110 = stablehlo.divide %107, %109 : tensor<1x64x7x7xf32>
    %111 = stablehlo.convert %110 : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
    %112 = stablehlo.broadcast_in_dim %35, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %113 = stablehlo.reshape %112 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %114 = stablehlo.dot_general %111, %113, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %115 = stablehlo.transpose %114, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
    %116 = stablehlo.reshape %115 : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
    %117 = stablehlo.reshape %arg9 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %118 = stablehlo.reshape %117 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %119 = stablehlo.transpose %118, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %120 = stablehlo.dot_general %116, %119, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %121 = stablehlo.reshape %120 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %122 = stablehlo.add %16, %121 : tensor<1x7x8192xbf16>
    %123 = stablehlo.reshape %arg12 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %124 = stablehlo.reshape %123 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %125 = stablehlo.broadcast_in_dim %124, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %126 = stablehlo.convert %122 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %127 = stablehlo.power %126, %6 : tensor<1x7x8192xf32>
    %128 = stablehlo.reduce(%127 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %129 = stablehlo.multiply %128, %cst_2 : tensor<1x7xf32>
    %130 = stablehlo.reshape %129 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %131 = stablehlo.add %130, %cst_3 : tensor<1x7x1xf32>
    %132 = stablehlo.rsqrt %131 : tensor<1x7x1xf32>
    %133 = stablehlo.reshape %132 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %135 = stablehlo.multiply %126, %134 : tensor<1x7x8192xf32>
    %136 = stablehlo.convert %135 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %137 = stablehlo.multiply %125, %136 : tensor<1x7x8192xbf16>
    %138 = stablehlo.reshape %137 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %139 = stablehlo.reshape %arg13 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %141 = stablehlo.transpose %140, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %142 = stablehlo.dot_general %138, %141, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %143 = stablehlo.reshape %142 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %144 = stablehlo.logistic %143 : tensor<1x7x28672xbf16>
    %145 = stablehlo.multiply %143, %144 : tensor<1x7x28672xbf16>
    %146 = stablehlo.reshape %arg8 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %148 = stablehlo.transpose %147, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %149 = stablehlo.dot_general %138, %148, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %150 = stablehlo.reshape %149 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %151 = stablehlo.multiply %145, %150 : tensor<1x7x28672xbf16>
    %152 = stablehlo.reshape %151 : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
    %153 = stablehlo.reshape %arg7 : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
    %155 = stablehlo.transpose %154, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
    %156 = stablehlo.dot_general %152, %155, contracting_dims = [1] x [0] : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
    %157 = stablehlo.reshape %156 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %158 = stablehlo.add %122, %157 : tensor<1x7x8192xbf16>
    %159 = stablehlo.convert %158 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %160 = stablehlo.power %159, %6 : tensor<1x7x8192xf32>
    %161 = stablehlo.reduce(%160 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %162 = stablehlo.multiply %161, %cst_2 : tensor<1x7xf32>
    %163 = stablehlo.reshape %162 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %164 = stablehlo.add %163, %cst_3 : tensor<1x7x1xf32>
    %165 = stablehlo.rsqrt %164 : tensor<1x7x1xf32>
    %166 = stablehlo.reshape %165 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %167 = stablehlo.broadcast_in_dim %166, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %168 = stablehlo.multiply %159, %167 : tensor<1x7x8192xf32>
    %169 = stablehlo.convert %168 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %170 = stablehlo.multiply %62, %169 : tensor<1x7x8192xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %172 = stablehlo.reshape %arg6 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
    %176 = stablehlo.reshape %175 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %35, %59, %176 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8, \22_axis_1\22=4]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1024x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg1: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"}, %arg2: tensor<128256x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8,4]<=[32] last_tile_dim_replicate}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg3: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg4: tensor<64xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg5: tensor<1024x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg6: tensor<128256x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8,4]<=[32] last_tile_dim_replicate}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___lm_head_weight"}, %arg7: tensor<8192x28672xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[4,8]<=[8,4]T(1,0)}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg8: tensor<28672x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg9: tensor<8192x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[4,8]<=[8,4]T(1,0)}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg10: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_1"}, %arg11: tensor<8192x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg12: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg13: tensor<28672x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg14: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
    %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
    %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
    %c_6 = stablehlo.constant dense<1> : tensor<i64>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %4 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
    %5 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %7 = stablehlo.reshape %arg3 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %8 = stablehlo.reshape %7 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %9 = stablehlo.broadcast_in_dim %8, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %10 = stablehlo.reshape %arg2 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %12 = stablehlo.reshape %arg1 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %13 = stablehlo.reshape %12 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %14 = stablehlo.convert %13 : (tensor<7xi64>) -> tensor<7xui32>
    %15 = "stablehlo.gather"(%11, %14) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
    %16 = stablehlo.reshape %15 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %17 = stablehlo.convert %16 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %18 = stablehlo.power %17, %6 : tensor<1x7x8192xf32>
    %19 = stablehlo.reduce(%18 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %20 = stablehlo.multiply %19, %cst_2 : tensor<1x7xf32>
    %21 = stablehlo.reshape %20 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %22 = stablehlo.add %21, %cst_3 : tensor<1x7x1xf32>
    %23 = stablehlo.rsqrt %22 : tensor<1x7x1xf32>
    %24 = stablehlo.reshape %23 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %25 = stablehlo.broadcast_in_dim %24, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %26 = stablehlo.multiply %17, %25 : tensor<1x7x8192xf32>
    %27 = stablehlo.convert %26 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %28 = stablehlo.multiply %9, %27 : tensor<1x7x8192xbf16>
    %29 = stablehlo.reshape %28 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %30 = stablehlo.reshape %arg0 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %32 = stablehlo.transpose %31, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %33 = stablehlo.dot_general %29, %32, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %34 = stablehlo.reshape %33 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %35 = stablehlo.transpose %34, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %36 = stablehlo.reshape %arg5 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %37 = stablehlo.reshape %36 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %38 = stablehlo.transpose %37, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %39 = stablehlo.dot_general %29, %38, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %40 = stablehlo.reshape %39 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %41 = stablehlo.transpose %40, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %42 = stablehlo.reshape %arg4 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %43 = stablehlo.reshape %42 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %44 = stablehlo.dot_general %43, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %45 = stablehlo.transpose %44, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %46 = stablehlo.concatenate %45, %45, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %47 = stablehlo.cosine %46 : tensor<1x7x128xf32>
    %48 = stablehlo.convert %47 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %49 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %50 = stablehlo.multiply %41, %49 : tensor<1x8x7x128xbf16>
    %51 = stablehlo.slice %41 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %52 = stablehlo.negate %51 : tensor<1x8x7x64xbf16>
    %53 = stablehlo.slice %41 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %54 = stablehlo.concatenate %52, %53, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %55 = stablehlo.sine %46 : tensor<1x7x128xf32>
    %56 = stablehlo.convert %55 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %57 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.multiply %54, %57 : tensor<1x8x7x128xbf16>
    %59 = stablehlo.add %50, %58 : tensor<1x8x7x128xbf16>
    %60 = stablehlo.reshape %arg14 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %61 = stablehlo.reshape %60 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %62 = stablehlo.broadcast_in_dim %61, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %63 = stablehlo.reshape %arg11 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %64 = stablehlo.reshape %63 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %65 = stablehlo.transpose %64, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %66 = stablehlo.dot_general %29, %65, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %67 = stablehlo.reshape %66 : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
    %68 = stablehlo.transpose %67, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
    %69 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %70 = stablehlo.multiply %68, %69 : tensor<1x64x7x128xbf16>
    %71 = stablehlo.slice %68 [0:1, 0:64, 0:7, 64:128] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %72 = stablehlo.negate %71 : tensor<1x64x7x64xbf16>
    %73 = stablehlo.slice %68 [0:1, 0:64, 0:7, 0:64] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %74 = stablehlo.concatenate %72, %73, dim = 3 : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
    %75 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %76 = stablehlo.multiply %74, %75 : tensor<1x64x7x128xbf16>
    %77 = stablehlo.add %70, %76 : tensor<1x64x7x128xbf16>
    %78 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %79 = stablehlo.reshape %78 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %80 = stablehlo.transpose %79, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
    %81 = stablehlo.dot_general %77, %80, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
    %82 = stablehlo.multiply %81, %5 : tensor<1x64x7x7xbf16>
    %83 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %84 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %85 = stablehlo.subtract %83, %84 : tensor<7x7xi64>
    %86 = stablehlo.compare  GE, %85, %4 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %87 = stablehlo.select %86, %3, %2 : tensor<7x7xi1>, tensor<7x7xbf16>
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %89 = stablehlo.convert %88 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
    %90 = stablehlo.multiply %87, %89 : tensor<7x7xbf16>
    %91 = stablehlo.reshape %90 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
    %92 = stablehlo.reshape %arg10 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %93 = stablehlo.reshape %92 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
    %94 = stablehlo.convert %93 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
    %95 = stablehlo.reshape %94 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
    %97 = stablehlo.add %91, %96 : tensor<1x1x7x7xbf16>
    %98 = stablehlo.compare  EQ, %97, %1 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
    %99 = stablehlo.select %98, %0, %91 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
    %100 = stablehlo.reshape %99 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
    %102 = stablehlo.add %82, %101 : tensor<1x64x7x7xbf16>
    %103 = stablehlo.convert %102 : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
    %104 = stablehlo.reduce(%103 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %106 = stablehlo.subtract %103, %105 : tensor<1x64x7x7xf32>
    %107 = stablehlo.exponential %106 : tensor<1x64x7x7xf32>
    %108 = stablehlo.reduce(%107 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %110 = stablehlo.divide %107, %109 : tensor<1x64x7x7xf32>
    %111 = stablehlo.convert %110 : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
    %112 = stablehlo.broadcast_in_dim %35, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %113 = stablehlo.reshape %112 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %114 = stablehlo.dot_general %111, %113, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %115 = stablehlo.transpose %114, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
    %116 = stablehlo.reshape %115 : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
    %117 = stablehlo.reshape %arg9 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %118 = stablehlo.reshape %117 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %119 = stablehlo.transpose %118, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %120 = stablehlo.dot_general %116, %119, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %121 = stablehlo.reshape %120 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %122 = stablehlo.add %16, %121 : tensor<1x7x8192xbf16>
    %123 = stablehlo.reshape %arg12 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %124 = stablehlo.reshape %123 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %125 = stablehlo.broadcast_in_dim %124, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %126 = stablehlo.convert %122 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %127 = stablehlo.power %126, %6 : tensor<1x7x8192xf32>
    %128 = stablehlo.reduce(%127 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %129 = stablehlo.multiply %128, %cst_2 : tensor<1x7xf32>
    %130 = stablehlo.reshape %129 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %131 = stablehlo.add %130, %cst_3 : tensor<1x7x1xf32>
    %132 = stablehlo.rsqrt %131 : tensor<1x7x1xf32>
    %133 = stablehlo.reshape %132 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %135 = stablehlo.multiply %126, %134 : tensor<1x7x8192xf32>
    %136 = stablehlo.convert %135 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %137 = stablehlo.multiply %125, %136 : tensor<1x7x8192xbf16>
    %138 = stablehlo.reshape %137 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %139 = stablehlo.reshape %arg13 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %141 = stablehlo.transpose %140, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %142 = stablehlo.dot_general %138, %141, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %143 = stablehlo.reshape %142 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %144 = stablehlo.logistic %143 : tensor<1x7x28672xbf16>
    %145 = stablehlo.multiply %143, %144 : tensor<1x7x28672xbf16>
    %146 = stablehlo.reshape %arg8 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %148 = stablehlo.transpose %147, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %149 = stablehlo.dot_general %138, %148, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %150 = stablehlo.reshape %149 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %151 = stablehlo.multiply %145, %150 : tensor<1x7x28672xbf16>
    %152 = stablehlo.reshape %151 : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
    %153 = stablehlo.reshape %arg7 : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
    %155 = stablehlo.transpose %154, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
    %156 = stablehlo.dot_general %152, %155, contracting_dims = [1] x [0] : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
    %157 = stablehlo.reshape %156 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %158 = stablehlo.add %122, %157 : tensor<1x7x8192xbf16>
    %159 = stablehlo.convert %158 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %160 = stablehlo.power %159, %6 : tensor<1x7x8192xf32>
    %161 = stablehlo.reduce(%160 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %162 = stablehlo.multiply %161, %cst_2 : tensor<1x7xf32>
    %163 = stablehlo.reshape %162 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %164 = stablehlo.add %163, %cst_3 : tensor<1x7x1xf32>
    %165 = stablehlo.rsqrt %164 : tensor<1x7x1xf32>
    %166 = stablehlo.reshape %165 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %167 = stablehlo.broadcast_in_dim %166, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %168 = stablehlo.multiply %159, %167 : tensor<1x7x8192xf32>
    %169 = stablehlo.convert %168 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %170 = stablehlo.multiply %62, %169 : tensor<1x7x8192xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %172 = stablehlo.reshape %arg6 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
    %176 = stablehlo.reshape %175 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %35, %59, %176 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before TTPopulateArgumentTypes (tt-populate-argument-types) ('builtin.module' operation: @SyncTensorsGraph.406) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8, \22_axis_1\22=4]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1024x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg1: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"}, %arg2: tensor<128256x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8,4]<=[32] last_tile_dim_replicate}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg3: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg4: tensor<64xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg5: tensor<1024x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg6: tensor<128256x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8,4]<=[32] last_tile_dim_replicate}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___lm_head_weight"}, %arg7: tensor<8192x28672xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[4,8]<=[8,4]T(1,0)}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg8: tensor<28672x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg9: tensor<8192x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[4,8]<=[8,4]T(1,0)}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg10: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_1"}, %arg11: tensor<8192x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg12: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg13: tensor<28672x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg14: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
    %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
    %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
    %c_6 = stablehlo.constant dense<1> : tensor<i64>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %4 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
    %5 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %7 = stablehlo.reshape %arg3 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %8 = stablehlo.reshape %7 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %9 = stablehlo.broadcast_in_dim %8, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %10 = stablehlo.reshape %arg2 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %12 = stablehlo.reshape %arg1 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %13 = stablehlo.reshape %12 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %14 = stablehlo.convert %13 : (tensor<7xi64>) -> tensor<7xui32>
    %15 = "stablehlo.gather"(%11, %14) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
    %16 = stablehlo.reshape %15 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %17 = stablehlo.convert %16 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %18 = stablehlo.power %17, %6 : tensor<1x7x8192xf32>
    %19 = stablehlo.reduce(%18 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %20 = stablehlo.multiply %19, %cst_2 : tensor<1x7xf32>
    %21 = stablehlo.reshape %20 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %22 = stablehlo.add %21, %cst_3 : tensor<1x7x1xf32>
    %23 = stablehlo.rsqrt %22 : tensor<1x7x1xf32>
    %24 = stablehlo.reshape %23 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %25 = stablehlo.broadcast_in_dim %24, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %26 = stablehlo.multiply %17, %25 : tensor<1x7x8192xf32>
    %27 = stablehlo.convert %26 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %28 = stablehlo.multiply %9, %27 : tensor<1x7x8192xbf16>
    %29 = stablehlo.reshape %28 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %30 = stablehlo.reshape %arg0 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %32 = stablehlo.transpose %31, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %33 = stablehlo.dot_general %29, %32, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %34 = stablehlo.reshape %33 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %35 = stablehlo.transpose %34, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %36 = stablehlo.reshape %arg5 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %37 = stablehlo.reshape %36 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %38 = stablehlo.transpose %37, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %39 = stablehlo.dot_general %29, %38, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %40 = stablehlo.reshape %39 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %41 = stablehlo.transpose %40, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %42 = stablehlo.reshape %arg4 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %43 = stablehlo.reshape %42 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %44 = stablehlo.dot_general %43, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %45 = stablehlo.transpose %44, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %46 = stablehlo.concatenate %45, %45, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %47 = stablehlo.cosine %46 : tensor<1x7x128xf32>
    %48 = stablehlo.convert %47 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %49 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %50 = stablehlo.multiply %41, %49 : tensor<1x8x7x128xbf16>
    %51 = stablehlo.slice %41 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %52 = stablehlo.negate %51 : tensor<1x8x7x64xbf16>
    %53 = stablehlo.slice %41 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %54 = stablehlo.concatenate %52, %53, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %55 = stablehlo.sine %46 : tensor<1x7x128xf32>
    %56 = stablehlo.convert %55 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %57 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.multiply %54, %57 : tensor<1x8x7x128xbf16>
    %59 = stablehlo.add %50, %58 : tensor<1x8x7x128xbf16>
    %60 = stablehlo.reshape %arg14 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %61 = stablehlo.reshape %60 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %62 = stablehlo.broadcast_in_dim %61, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %63 = stablehlo.reshape %arg11 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %64 = stablehlo.reshape %63 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %65 = stablehlo.transpose %64, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %66 = stablehlo.dot_general %29, %65, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %67 = stablehlo.reshape %66 : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
    %68 = stablehlo.transpose %67, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
    %69 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %70 = stablehlo.multiply %68, %69 : tensor<1x64x7x128xbf16>
    %71 = stablehlo.slice %68 [0:1, 0:64, 0:7, 64:128] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %72 = stablehlo.negate %71 : tensor<1x64x7x64xbf16>
    %73 = stablehlo.slice %68 [0:1, 0:64, 0:7, 0:64] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %74 = stablehlo.concatenate %72, %73, dim = 3 : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
    %75 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %76 = stablehlo.multiply %74, %75 : tensor<1x64x7x128xbf16>
    %77 = stablehlo.add %70, %76 : tensor<1x64x7x128xbf16>
    %78 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %79 = stablehlo.reshape %78 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %80 = stablehlo.transpose %79, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
    %81 = stablehlo.dot_general %77, %80, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
    %82 = stablehlo.multiply %81, %5 : tensor<1x64x7x7xbf16>
    %83 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %84 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %85 = stablehlo.subtract %83, %84 : tensor<7x7xi64>
    %86 = stablehlo.compare  GE, %85, %4 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %87 = stablehlo.select %86, %3, %2 : tensor<7x7xi1>, tensor<7x7xbf16>
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %89 = stablehlo.convert %88 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
    %90 = stablehlo.multiply %87, %89 : tensor<7x7xbf16>
    %91 = stablehlo.reshape %90 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
    %92 = stablehlo.reshape %arg10 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %93 = stablehlo.reshape %92 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
    %94 = stablehlo.convert %93 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
    %95 = stablehlo.reshape %94 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
    %97 = stablehlo.add %91, %96 : tensor<1x1x7x7xbf16>
    %98 = stablehlo.compare  EQ, %97, %1 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
    %99 = stablehlo.select %98, %0, %91 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
    %100 = stablehlo.reshape %99 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
    %102 = stablehlo.add %82, %101 : tensor<1x64x7x7xbf16>
    %103 = stablehlo.convert %102 : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
    %104 = stablehlo.reduce(%103 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %106 = stablehlo.subtract %103, %105 : tensor<1x64x7x7xf32>
    %107 = stablehlo.exponential %106 : tensor<1x64x7x7xf32>
    %108 = stablehlo.reduce(%107 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %110 = stablehlo.divide %107, %109 : tensor<1x64x7x7xf32>
    %111 = stablehlo.convert %110 : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
    %112 = stablehlo.broadcast_in_dim %35, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %113 = stablehlo.reshape %112 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %114 = stablehlo.dot_general %111, %113, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %115 = stablehlo.transpose %114, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
    %116 = stablehlo.reshape %115 : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
    %117 = stablehlo.reshape %arg9 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %118 = stablehlo.reshape %117 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %119 = stablehlo.transpose %118, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %120 = stablehlo.dot_general %116, %119, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %121 = stablehlo.reshape %120 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %122 = stablehlo.add %16, %121 : tensor<1x7x8192xbf16>
    %123 = stablehlo.reshape %arg12 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %124 = stablehlo.reshape %123 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %125 = stablehlo.broadcast_in_dim %124, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %126 = stablehlo.convert %122 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %127 = stablehlo.power %126, %6 : tensor<1x7x8192xf32>
    %128 = stablehlo.reduce(%127 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %129 = stablehlo.multiply %128, %cst_2 : tensor<1x7xf32>
    %130 = stablehlo.reshape %129 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %131 = stablehlo.add %130, %cst_3 : tensor<1x7x1xf32>
    %132 = stablehlo.rsqrt %131 : tensor<1x7x1xf32>
    %133 = stablehlo.reshape %132 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %135 = stablehlo.multiply %126, %134 : tensor<1x7x8192xf32>
    %136 = stablehlo.convert %135 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %137 = stablehlo.multiply %125, %136 : tensor<1x7x8192xbf16>
    %138 = stablehlo.reshape %137 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %139 = stablehlo.reshape %arg13 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %141 = stablehlo.transpose %140, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %142 = stablehlo.dot_general %138, %141, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %143 = stablehlo.reshape %142 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %144 = stablehlo.logistic %143 : tensor<1x7x28672xbf16>
    %145 = stablehlo.multiply %143, %144 : tensor<1x7x28672xbf16>
    %146 = stablehlo.reshape %arg8 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %148 = stablehlo.transpose %147, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %149 = stablehlo.dot_general %138, %148, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %150 = stablehlo.reshape %149 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %151 = stablehlo.multiply %145, %150 : tensor<1x7x28672xbf16>
    %152 = stablehlo.reshape %151 : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
    %153 = stablehlo.reshape %arg7 : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
    %155 = stablehlo.transpose %154, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
    %156 = stablehlo.dot_general %152, %155, contracting_dims = [1] x [0] : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
    %157 = stablehlo.reshape %156 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %158 = stablehlo.add %122, %157 : tensor<1x7x8192xbf16>
    %159 = stablehlo.convert %158 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %160 = stablehlo.power %159, %6 : tensor<1x7x8192xf32>
    %161 = stablehlo.reduce(%160 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %162 = stablehlo.multiply %161, %cst_2 : tensor<1x7xf32>
    %163 = stablehlo.reshape %162 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %164 = stablehlo.add %163, %cst_3 : tensor<1x7x1xf32>
    %165 = stablehlo.rsqrt %164 : tensor<1x7x1xf32>
    %166 = stablehlo.reshape %165 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %167 = stablehlo.broadcast_in_dim %166, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %168 = stablehlo.multiply %159, %167 : tensor<1x7x8192xf32>
    %169 = stablehlo.convert %168 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %170 = stablehlo.multiply %62, %169 : tensor<1x7x8192xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %172 = stablehlo.reshape %arg6 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
    %176 = stablehlo.reshape %175 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %35, %59, %176 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.406) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8, \22_axis_1\22=4]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1024x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg1: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"}, %arg2: tensor<128256x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8,4]<=[32] last_tile_dim_replicate}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg3: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg4: tensor<64xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg5: tensor<1024x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg6: tensor<128256x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8,4]<=[32] last_tile_dim_replicate}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___lm_head_weight"}, %arg7: tensor<8192x28672xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[4,8]<=[8,4]T(1,0)}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg8: tensor<28672x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg9: tensor<8192x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[4,8]<=[8,4]T(1,0)}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg10: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_1"}, %arg11: tensor<8192x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg12: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg13: tensor<28672x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg14: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
    %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
    %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
    %c_6 = stablehlo.constant dense<1> : tensor<i64>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %4 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
    %5 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %7 = stablehlo.reshape %arg3 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %8 = stablehlo.reshape %7 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %9 = stablehlo.broadcast_in_dim %8, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %10 = stablehlo.reshape %arg2 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %12 = stablehlo.reshape %arg1 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %13 = stablehlo.reshape %12 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %14 = stablehlo.convert %13 : (tensor<7xi64>) -> tensor<7xui32>
    %15 = "stablehlo.gather"(%11, %14) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
    %16 = stablehlo.reshape %15 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %17 = stablehlo.convert %16 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %18 = stablehlo.power %17, %6 : tensor<1x7x8192xf32>
    %19 = stablehlo.reduce(%18 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %20 = stablehlo.multiply %19, %cst_2 : tensor<1x7xf32>
    %21 = stablehlo.reshape %20 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %22 = stablehlo.add %21, %cst_3 : tensor<1x7x1xf32>
    %23 = stablehlo.rsqrt %22 : tensor<1x7x1xf32>
    %24 = stablehlo.reshape %23 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %25 = stablehlo.broadcast_in_dim %24, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %26 = stablehlo.multiply %17, %25 : tensor<1x7x8192xf32>
    %27 = stablehlo.convert %26 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %28 = stablehlo.multiply %9, %27 : tensor<1x7x8192xbf16>
    %29 = stablehlo.reshape %28 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %30 = stablehlo.reshape %arg0 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %32 = stablehlo.transpose %31, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %33 = stablehlo.dot_general %29, %32, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %34 = stablehlo.reshape %33 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %35 = stablehlo.transpose %34, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %36 = stablehlo.reshape %arg5 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %37 = stablehlo.reshape %36 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %38 = stablehlo.transpose %37, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %39 = stablehlo.dot_general %29, %38, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %40 = stablehlo.reshape %39 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %41 = stablehlo.transpose %40, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %42 = stablehlo.reshape %arg4 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %43 = stablehlo.reshape %42 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %44 = stablehlo.dot_general %43, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %45 = stablehlo.transpose %44, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %46 = stablehlo.concatenate %45, %45, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %47 = stablehlo.cosine %46 : tensor<1x7x128xf32>
    %48 = stablehlo.convert %47 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %49 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %50 = stablehlo.multiply %41, %49 : tensor<1x8x7x128xbf16>
    %51 = stablehlo.slice %41 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %52 = stablehlo.negate %51 : tensor<1x8x7x64xbf16>
    %53 = stablehlo.slice %41 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %54 = stablehlo.concatenate %52, %53, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %55 = stablehlo.sine %46 : tensor<1x7x128xf32>
    %56 = stablehlo.convert %55 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %57 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.multiply %54, %57 : tensor<1x8x7x128xbf16>
    %59 = stablehlo.add %50, %58 : tensor<1x8x7x128xbf16>
    %60 = stablehlo.reshape %arg14 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %61 = stablehlo.reshape %60 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %62 = stablehlo.broadcast_in_dim %61, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %63 = stablehlo.reshape %arg11 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %64 = stablehlo.reshape %63 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %65 = stablehlo.transpose %64, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %66 = stablehlo.dot_general %29, %65, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %67 = stablehlo.reshape %66 : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
    %68 = stablehlo.transpose %67, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
    %69 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %70 = stablehlo.multiply %68, %69 : tensor<1x64x7x128xbf16>
    %71 = stablehlo.slice %68 [0:1, 0:64, 0:7, 64:128] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %72 = stablehlo.negate %71 : tensor<1x64x7x64xbf16>
    %73 = stablehlo.slice %68 [0:1, 0:64, 0:7, 0:64] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %74 = stablehlo.concatenate %72, %73, dim = 3 : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
    %75 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %76 = stablehlo.multiply %74, %75 : tensor<1x64x7x128xbf16>
    %77 = stablehlo.add %70, %76 : tensor<1x64x7x128xbf16>
    %78 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %79 = stablehlo.reshape %78 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %80 = stablehlo.transpose %79, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
    %81 = stablehlo.dot_general %77, %80, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
    %82 = stablehlo.multiply %81, %5 : tensor<1x64x7x7xbf16>
    %83 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %84 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %85 = stablehlo.subtract %83, %84 : tensor<7x7xi64>
    %86 = stablehlo.compare  GE, %85, %4 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %87 = stablehlo.select %86, %3, %2 : tensor<7x7xi1>, tensor<7x7xbf16>
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %89 = stablehlo.convert %88 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
    %90 = stablehlo.multiply %87, %89 : tensor<7x7xbf16>
    %91 = stablehlo.reshape %90 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
    %92 = stablehlo.reshape %arg10 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %93 = stablehlo.reshape %92 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
    %94 = stablehlo.convert %93 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
    %95 = stablehlo.reshape %94 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
    %97 = stablehlo.add %91, %96 : tensor<1x1x7x7xbf16>
    %98 = stablehlo.compare  EQ, %97, %1 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
    %99 = stablehlo.select %98, %0, %91 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
    %100 = stablehlo.reshape %99 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
    %102 = stablehlo.add %82, %101 : tensor<1x64x7x7xbf16>
    %103 = stablehlo.convert %102 : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
    %104 = stablehlo.reduce(%103 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %106 = stablehlo.subtract %103, %105 : tensor<1x64x7x7xf32>
    %107 = stablehlo.exponential %106 : tensor<1x64x7x7xf32>
    %108 = stablehlo.reduce(%107 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %110 = stablehlo.divide %107, %109 : tensor<1x64x7x7xf32>
    %111 = stablehlo.convert %110 : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
    %112 = stablehlo.broadcast_in_dim %35, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %113 = stablehlo.reshape %112 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %114 = stablehlo.dot_general %111, %113, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %115 = stablehlo.transpose %114, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
    %116 = stablehlo.reshape %115 : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
    %117 = stablehlo.reshape %arg9 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %118 = stablehlo.reshape %117 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %119 = stablehlo.transpose %118, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %120 = stablehlo.dot_general %116, %119, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %121 = stablehlo.reshape %120 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %122 = stablehlo.add %16, %121 : tensor<1x7x8192xbf16>
    %123 = stablehlo.reshape %arg12 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %124 = stablehlo.reshape %123 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %125 = stablehlo.broadcast_in_dim %124, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %126 = stablehlo.convert %122 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %127 = stablehlo.power %126, %6 : tensor<1x7x8192xf32>
    %128 = stablehlo.reduce(%127 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %129 = stablehlo.multiply %128, %cst_2 : tensor<1x7xf32>
    %130 = stablehlo.reshape %129 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %131 = stablehlo.add %130, %cst_3 : tensor<1x7x1xf32>
    %132 = stablehlo.rsqrt %131 : tensor<1x7x1xf32>
    %133 = stablehlo.reshape %132 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %135 = stablehlo.multiply %126, %134 : tensor<1x7x8192xf32>
    %136 = stablehlo.convert %135 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %137 = stablehlo.multiply %125, %136 : tensor<1x7x8192xbf16>
    %138 = stablehlo.reshape %137 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %139 = stablehlo.reshape %arg13 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %141 = stablehlo.transpose %140, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %142 = stablehlo.dot_general %138, %141, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %143 = stablehlo.reshape %142 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %144 = stablehlo.logistic %143 : tensor<1x7x28672xbf16>
    %145 = stablehlo.multiply %143, %144 : tensor<1x7x28672xbf16>
    %146 = stablehlo.reshape %arg8 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %148 = stablehlo.transpose %147, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %149 = stablehlo.dot_general %138, %148, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %150 = stablehlo.reshape %149 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %151 = stablehlo.multiply %145, %150 : tensor<1x7x28672xbf16>
    %152 = stablehlo.reshape %151 : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
    %153 = stablehlo.reshape %arg7 : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
    %155 = stablehlo.transpose %154, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
    %156 = stablehlo.dot_general %152, %155, contracting_dims = [1] x [0] : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
    %157 = stablehlo.reshape %156 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %158 = stablehlo.add %122, %157 : tensor<1x7x8192xbf16>
    %159 = stablehlo.convert %158 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %160 = stablehlo.power %159, %6 : tensor<1x7x8192xf32>
    %161 = stablehlo.reduce(%160 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %162 = stablehlo.multiply %161, %cst_2 : tensor<1x7xf32>
    %163 = stablehlo.reshape %162 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %164 = stablehlo.add %163, %cst_3 : tensor<1x7x1xf32>
    %165 = stablehlo.rsqrt %164 : tensor<1x7x1xf32>
    %166 = stablehlo.reshape %165 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %167 = stablehlo.broadcast_in_dim %166, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %168 = stablehlo.multiply %159, %167 : tensor<1x7x8192xf32>
    %169 = stablehlo.convert %168 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %170 = stablehlo.multiply %62, %169 : tensor<1x7x8192xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %172 = stablehlo.reshape %arg6 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
    %176 = stablehlo.reshape %175 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %35, %59, %176 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump After ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.406) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8, \22_axis_1\22=4]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1024x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg1: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg2: tensor<128256x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8,4]<=[32] last_tile_dim_replicate}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg3: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg4: tensor<64xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg5: tensor<1024x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg6: tensor<128256x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8,4]<=[32] last_tile_dim_replicate}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg7: tensor<8192x28672xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[4,8]<=[8,4]T(1,0)}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg8: tensor<28672x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg9: tensor<8192x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[4,8]<=[8,4]T(1,0)}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg10: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg11: tensor<8192x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg12: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg13: tensor<28672x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg14: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
    %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
    %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
    %c_6 = stablehlo.constant dense<1> : tensor<i64>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %4 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
    %5 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %7 = stablehlo.reshape %arg3 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %8 = stablehlo.reshape %7 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %9 = stablehlo.broadcast_in_dim %8, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %10 = stablehlo.reshape %arg2 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %12 = stablehlo.reshape %arg1 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %13 = stablehlo.reshape %12 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %14 = stablehlo.convert %13 : (tensor<7xi64>) -> tensor<7xui32>
    %15 = "stablehlo.gather"(%11, %14) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
    %16 = stablehlo.reshape %15 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %17 = stablehlo.convert %16 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %18 = stablehlo.power %17, %6 : tensor<1x7x8192xf32>
    %19 = stablehlo.reduce(%18 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %20 = stablehlo.multiply %19, %cst_2 : tensor<1x7xf32>
    %21 = stablehlo.reshape %20 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %22 = stablehlo.add %21, %cst_3 : tensor<1x7x1xf32>
    %23 = stablehlo.rsqrt %22 : tensor<1x7x1xf32>
    %24 = stablehlo.reshape %23 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %25 = stablehlo.broadcast_in_dim %24, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %26 = stablehlo.multiply %17, %25 : tensor<1x7x8192xf32>
    %27 = stablehlo.convert %26 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %28 = stablehlo.multiply %9, %27 : tensor<1x7x8192xbf16>
    %29 = stablehlo.reshape %28 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %30 = stablehlo.reshape %arg0 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %32 = stablehlo.transpose %31, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %33 = stablehlo.dot_general %29, %32, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %34 = stablehlo.reshape %33 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %35 = stablehlo.transpose %34, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %36 = stablehlo.reshape %arg5 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %37 = stablehlo.reshape %36 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %38 = stablehlo.transpose %37, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %39 = stablehlo.dot_general %29, %38, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %40 = stablehlo.reshape %39 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %41 = stablehlo.transpose %40, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %42 = stablehlo.reshape %arg4 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %43 = stablehlo.reshape %42 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %44 = stablehlo.dot_general %43, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %45 = stablehlo.transpose %44, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %46 = stablehlo.concatenate %45, %45, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %47 = stablehlo.cosine %46 : tensor<1x7x128xf32>
    %48 = stablehlo.convert %47 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %49 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %50 = stablehlo.multiply %41, %49 : tensor<1x8x7x128xbf16>
    %51 = stablehlo.slice %41 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %52 = stablehlo.negate %51 : tensor<1x8x7x64xbf16>
    %53 = stablehlo.slice %41 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %54 = stablehlo.concatenate %52, %53, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %55 = stablehlo.sine %46 : tensor<1x7x128xf32>
    %56 = stablehlo.convert %55 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %57 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.multiply %54, %57 : tensor<1x8x7x128xbf16>
    %59 = stablehlo.add %50, %58 : tensor<1x8x7x128xbf16>
    %60 = stablehlo.reshape %arg14 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %61 = stablehlo.reshape %60 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %62 = stablehlo.broadcast_in_dim %61, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %63 = stablehlo.reshape %arg11 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %64 = stablehlo.reshape %63 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %65 = stablehlo.transpose %64, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %66 = stablehlo.dot_general %29, %65, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %67 = stablehlo.reshape %66 : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
    %68 = stablehlo.transpose %67, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
    %69 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %70 = stablehlo.multiply %68, %69 : tensor<1x64x7x128xbf16>
    %71 = stablehlo.slice %68 [0:1, 0:64, 0:7, 64:128] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %72 = stablehlo.negate %71 : tensor<1x64x7x64xbf16>
    %73 = stablehlo.slice %68 [0:1, 0:64, 0:7, 0:64] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %74 = stablehlo.concatenate %72, %73, dim = 3 : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
    %75 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %76 = stablehlo.multiply %74, %75 : tensor<1x64x7x128xbf16>
    %77 = stablehlo.add %70, %76 : tensor<1x64x7x128xbf16>
    %78 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %79 = stablehlo.reshape %78 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %80 = stablehlo.transpose %79, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
    %81 = stablehlo.dot_general %77, %80, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
    %82 = stablehlo.multiply %81, %5 : tensor<1x64x7x7xbf16>
    %83 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %84 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %85 = stablehlo.subtract %83, %84 : tensor<7x7xi64>
    %86 = stablehlo.compare  GE, %85, %4 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %87 = stablehlo.select %86, %3, %2 : tensor<7x7xi1>, tensor<7x7xbf16>
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %89 = stablehlo.convert %88 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
    %90 = stablehlo.multiply %87, %89 : tensor<7x7xbf16>
    %91 = stablehlo.reshape %90 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
    %92 = stablehlo.reshape %arg10 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %93 = stablehlo.reshape %92 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
    %94 = stablehlo.convert %93 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
    %95 = stablehlo.reshape %94 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
    %97 = stablehlo.add %91, %96 : tensor<1x1x7x7xbf16>
    %98 = stablehlo.compare  EQ, %97, %1 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
    %99 = stablehlo.select %98, %0, %91 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
    %100 = stablehlo.reshape %99 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
    %102 = stablehlo.add %82, %101 : tensor<1x64x7x7xbf16>
    %103 = stablehlo.convert %102 : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
    %104 = stablehlo.reduce(%103 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %106 = stablehlo.subtract %103, %105 : tensor<1x64x7x7xf32>
    %107 = stablehlo.exponential %106 : tensor<1x64x7x7xf32>
    %108 = stablehlo.reduce(%107 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %110 = stablehlo.divide %107, %109 : tensor<1x64x7x7xf32>
    %111 = stablehlo.convert %110 : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
    %112 = stablehlo.broadcast_in_dim %35, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %113 = stablehlo.reshape %112 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %114 = stablehlo.dot_general %111, %113, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %115 = stablehlo.transpose %114, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
    %116 = stablehlo.reshape %115 : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
    %117 = stablehlo.reshape %arg9 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %118 = stablehlo.reshape %117 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %119 = stablehlo.transpose %118, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %120 = stablehlo.dot_general %116, %119, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %121 = stablehlo.reshape %120 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %122 = stablehlo.add %16, %121 : tensor<1x7x8192xbf16>
    %123 = stablehlo.reshape %arg12 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %124 = stablehlo.reshape %123 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %125 = stablehlo.broadcast_in_dim %124, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %126 = stablehlo.convert %122 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %127 = stablehlo.power %126, %6 : tensor<1x7x8192xf32>
    %128 = stablehlo.reduce(%127 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %129 = stablehlo.multiply %128, %cst_2 : tensor<1x7xf32>
    %130 = stablehlo.reshape %129 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %131 = stablehlo.add %130, %cst_3 : tensor<1x7x1xf32>
    %132 = stablehlo.rsqrt %131 : tensor<1x7x1xf32>
    %133 = stablehlo.reshape %132 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %135 = stablehlo.multiply %126, %134 : tensor<1x7x8192xf32>
    %136 = stablehlo.convert %135 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %137 = stablehlo.multiply %125, %136 : tensor<1x7x8192xbf16>
    %138 = stablehlo.reshape %137 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %139 = stablehlo.reshape %arg13 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %141 = stablehlo.transpose %140, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %142 = stablehlo.dot_general %138, %141, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %143 = stablehlo.reshape %142 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %144 = stablehlo.logistic %143 : tensor<1x7x28672xbf16>
    %145 = stablehlo.multiply %143, %144 : tensor<1x7x28672xbf16>
    %146 = stablehlo.reshape %arg8 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %148 = stablehlo.transpose %147, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %149 = stablehlo.dot_general %138, %148, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %150 = stablehlo.reshape %149 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %151 = stablehlo.multiply %145, %150 : tensor<1x7x28672xbf16>
    %152 = stablehlo.reshape %151 : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
    %153 = stablehlo.reshape %arg7 : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
    %155 = stablehlo.transpose %154, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
    %156 = stablehlo.dot_general %152, %155, contracting_dims = [1] x [0] : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
    %157 = stablehlo.reshape %156 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %158 = stablehlo.add %122, %157 : tensor<1x7x8192xbf16>
    %159 = stablehlo.convert %158 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %160 = stablehlo.power %159, %6 : tensor<1x7x8192xf32>
    %161 = stablehlo.reduce(%160 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %162 = stablehlo.multiply %161, %cst_2 : tensor<1x7xf32>
    %163 = stablehlo.reshape %162 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %164 = stablehlo.add %163, %cst_3 : tensor<1x7x1xf32>
    %165 = stablehlo.rsqrt %164 : tensor<1x7x1xf32>
    %166 = stablehlo.reshape %165 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %167 = stablehlo.broadcast_in_dim %166, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %168 = stablehlo.multiply %159, %167 : tensor<1x7x8192xf32>
    %169 = stablehlo.convert %168 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %170 = stablehlo.multiply %62, %169 : tensor<1x7x8192xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %172 = stablehlo.reshape %arg6 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
    %176 = stablehlo.reshape %175 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %35, %59, %176 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before ConvertXlaSdyToSdyPass (convert-xla-sdy-to-sdy) ('builtin.module' operation: @SyncTensorsGraph.406) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8, \22_axis_1\22=4]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1024x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg1: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg2: tensor<128256x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8,4]<=[32] last_tile_dim_replicate}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg3: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg4: tensor<64xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg5: tensor<1024x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg6: tensor<128256x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[1,8,4]<=[32] last_tile_dim_replicate}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg7: tensor<8192x28672xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[4,8]<=[8,4]T(1,0)}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg8: tensor<28672x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg9: tensor<8192x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_1\22}, {\22_axis_0\22}]>"}, mhlo.sharding = "{devices=[4,8]<=[8,4]T(1,0)}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg10: tensor<1x7xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg11: tensor<8192x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg12: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg13: tensor<28672x8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {\22_axis_1\22}]>"}, mhlo.sharding = "{devices=[8,4]<=[32]}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg14: tensor<8192xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
    %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
    %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
    %c_6 = stablehlo.constant dense<1> : tensor<i64>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %4 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
    %5 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %7 = stablehlo.reshape %arg3 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %8 = stablehlo.reshape %7 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %9 = stablehlo.broadcast_in_dim %8, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %10 = stablehlo.reshape %arg2 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %12 = stablehlo.reshape %arg1 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %13 = stablehlo.reshape %12 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %14 = stablehlo.convert %13 : (tensor<7xi64>) -> tensor<7xui32>
    %15 = "stablehlo.gather"(%11, %14) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
    %16 = stablehlo.reshape %15 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %17 = stablehlo.convert %16 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %18 = stablehlo.power %17, %6 : tensor<1x7x8192xf32>
    %19 = stablehlo.reduce(%18 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %20 = stablehlo.multiply %19, %cst_2 : tensor<1x7xf32>
    %21 = stablehlo.reshape %20 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %22 = stablehlo.add %21, %cst_3 : tensor<1x7x1xf32>
    %23 = stablehlo.rsqrt %22 : tensor<1x7x1xf32>
    %24 = stablehlo.reshape %23 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %25 = stablehlo.broadcast_in_dim %24, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %26 = stablehlo.multiply %17, %25 : tensor<1x7x8192xf32>
    %27 = stablehlo.convert %26 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %28 = stablehlo.multiply %9, %27 : tensor<1x7x8192xbf16>
    %29 = stablehlo.reshape %28 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %30 = stablehlo.reshape %arg0 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %32 = stablehlo.transpose %31, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %33 = stablehlo.dot_general %29, %32, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %34 = stablehlo.reshape %33 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %35 = stablehlo.transpose %34, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %36 = stablehlo.reshape %arg5 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %37 = stablehlo.reshape %36 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %38 = stablehlo.transpose %37, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %39 = stablehlo.dot_general %29, %38, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %40 = stablehlo.reshape %39 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %41 = stablehlo.transpose %40, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %42 = stablehlo.reshape %arg4 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %43 = stablehlo.reshape %42 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %44 = stablehlo.dot_general %43, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %45 = stablehlo.transpose %44, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %46 = stablehlo.concatenate %45, %45, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %47 = stablehlo.cosine %46 : tensor<1x7x128xf32>
    %48 = stablehlo.convert %47 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %49 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %50 = stablehlo.multiply %41, %49 : tensor<1x8x7x128xbf16>
    %51 = stablehlo.slice %41 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %52 = stablehlo.negate %51 : tensor<1x8x7x64xbf16>
    %53 = stablehlo.slice %41 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %54 = stablehlo.concatenate %52, %53, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %55 = stablehlo.sine %46 : tensor<1x7x128xf32>
    %56 = stablehlo.convert %55 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %57 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.multiply %54, %57 : tensor<1x8x7x128xbf16>
    %59 = stablehlo.add %50, %58 : tensor<1x8x7x128xbf16>
    %60 = stablehlo.reshape %arg14 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %61 = stablehlo.reshape %60 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %62 = stablehlo.broadcast_in_dim %61, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %63 = stablehlo.reshape %arg11 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %64 = stablehlo.reshape %63 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %65 = stablehlo.transpose %64, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %66 = stablehlo.dot_general %29, %65, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %67 = stablehlo.reshape %66 : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
    %68 = stablehlo.transpose %67, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
    %69 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %70 = stablehlo.multiply %68, %69 : tensor<1x64x7x128xbf16>
    %71 = stablehlo.slice %68 [0:1, 0:64, 0:7, 64:128] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %72 = stablehlo.negate %71 : tensor<1x64x7x64xbf16>
    %73 = stablehlo.slice %68 [0:1, 0:64, 0:7, 0:64] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %74 = stablehlo.concatenate %72, %73, dim = 3 : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
    %75 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %76 = stablehlo.multiply %74, %75 : tensor<1x64x7x128xbf16>
    %77 = stablehlo.add %70, %76 : tensor<1x64x7x128xbf16>
    %78 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %79 = stablehlo.reshape %78 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %80 = stablehlo.transpose %79, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
    %81 = stablehlo.dot_general %77, %80, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
    %82 = stablehlo.multiply %81, %5 : tensor<1x64x7x7xbf16>
    %83 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %84 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %85 = stablehlo.subtract %83, %84 : tensor<7x7xi64>
    %86 = stablehlo.compare  GE, %85, %4 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %87 = stablehlo.select %86, %3, %2 : tensor<7x7xi1>, tensor<7x7xbf16>
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %89 = stablehlo.convert %88 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
    %90 = stablehlo.multiply %87, %89 : tensor<7x7xbf16>
    %91 = stablehlo.reshape %90 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
    %92 = stablehlo.reshape %arg10 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %93 = stablehlo.reshape %92 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
    %94 = stablehlo.convert %93 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
    %95 = stablehlo.reshape %94 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
    %97 = stablehlo.add %91, %96 : tensor<1x1x7x7xbf16>
    %98 = stablehlo.compare  EQ, %97, %1 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
    %99 = stablehlo.select %98, %0, %91 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
    %100 = stablehlo.reshape %99 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
    %102 = stablehlo.add %82, %101 : tensor<1x64x7x7xbf16>
    %103 = stablehlo.convert %102 : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
    %104 = stablehlo.reduce(%103 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %106 = stablehlo.subtract %103, %105 : tensor<1x64x7x7xf32>
    %107 = stablehlo.exponential %106 : tensor<1x64x7x7xf32>
    %108 = stablehlo.reduce(%107 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %110 = stablehlo.divide %107, %109 : tensor<1x64x7x7xf32>
    %111 = stablehlo.convert %110 : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
    %112 = stablehlo.broadcast_in_dim %35, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %113 = stablehlo.reshape %112 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %114 = stablehlo.dot_general %111, %113, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %115 = stablehlo.transpose %114, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
    %116 = stablehlo.reshape %115 : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
    %117 = stablehlo.reshape %arg9 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %118 = stablehlo.reshape %117 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %119 = stablehlo.transpose %118, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %120 = stablehlo.dot_general %116, %119, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %121 = stablehlo.reshape %120 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %122 = stablehlo.add %16, %121 : tensor<1x7x8192xbf16>
    %123 = stablehlo.reshape %arg12 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %124 = stablehlo.reshape %123 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %125 = stablehlo.broadcast_in_dim %124, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %126 = stablehlo.convert %122 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %127 = stablehlo.power %126, %6 : tensor<1x7x8192xf32>
    %128 = stablehlo.reduce(%127 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %129 = stablehlo.multiply %128, %cst_2 : tensor<1x7xf32>
    %130 = stablehlo.reshape %129 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %131 = stablehlo.add %130, %cst_3 : tensor<1x7x1xf32>
    %132 = stablehlo.rsqrt %131 : tensor<1x7x1xf32>
    %133 = stablehlo.reshape %132 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %135 = stablehlo.multiply %126, %134 : tensor<1x7x8192xf32>
    %136 = stablehlo.convert %135 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %137 = stablehlo.multiply %125, %136 : tensor<1x7x8192xbf16>
    %138 = stablehlo.reshape %137 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %139 = stablehlo.reshape %arg13 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %141 = stablehlo.transpose %140, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %142 = stablehlo.dot_general %138, %141, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %143 = stablehlo.reshape %142 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %144 = stablehlo.logistic %143 : tensor<1x7x28672xbf16>
    %145 = stablehlo.multiply %143, %144 : tensor<1x7x28672xbf16>
    %146 = stablehlo.reshape %arg8 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %148 = stablehlo.transpose %147, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %149 = stablehlo.dot_general %138, %148, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %150 = stablehlo.reshape %149 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %151 = stablehlo.multiply %145, %150 : tensor<1x7x28672xbf16>
    %152 = stablehlo.reshape %151 : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
    %153 = stablehlo.reshape %arg7 : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
    %155 = stablehlo.transpose %154, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
    %156 = stablehlo.dot_general %152, %155, contracting_dims = [1] x [0] : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
    %157 = stablehlo.reshape %156 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %158 = stablehlo.add %122, %157 : tensor<1x7x8192xbf16>
    %159 = stablehlo.convert %158 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %160 = stablehlo.power %159, %6 : tensor<1x7x8192xf32>
    %161 = stablehlo.reduce(%160 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %162 = stablehlo.multiply %161, %cst_2 : tensor<1x7xf32>
    %163 = stablehlo.reshape %162 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %164 = stablehlo.add %163, %cst_3 : tensor<1x7x1xf32>
    %165 = stablehlo.rsqrt %164 : tensor<1x7x1xf32>
    %166 = stablehlo.reshape %165 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %167 = stablehlo.broadcast_in_dim %166, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %168 = stablehlo.multiply %159, %167 : tensor<1x7x8192xf32>
    %169 = stablehlo.convert %168 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %170 = stablehlo.multiply %62, %169 : tensor<1x7x8192xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %172 = stablehlo.reshape %arg6 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
    %176 = stablehlo.reshape %175 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %35, %59, %176 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump After ConvertXlaSdyToSdyPass (convert-xla-sdy-to-sdy) ('builtin.module' operation: @SyncTensorsGraph.406) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8, "_axis_1"=4]>
  func.func @main(%arg0: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg1: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg2: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg3: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg4: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg5: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg6: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg7: tensor<8192x28672xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg8: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg9: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg10: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg11: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg12: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg13: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg14: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
    %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
    %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
    %c_6 = stablehlo.constant dense<1> : tensor<i64>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %4 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
    %5 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %7 = stablehlo.reshape %arg3 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %8 = stablehlo.reshape %7 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %9 = stablehlo.broadcast_in_dim %8, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %10 = stablehlo.reshape %arg2 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %12 = stablehlo.reshape %arg1 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %13 = stablehlo.reshape %12 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %14 = stablehlo.convert %13 : (tensor<7xi64>) -> tensor<7xui32>
    %15 = "stablehlo.gather"(%11, %14) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
    %16 = stablehlo.reshape %15 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %17 = stablehlo.convert %16 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %18 = stablehlo.power %17, %6 : tensor<1x7x8192xf32>
    %19 = stablehlo.reduce(%18 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %20 = stablehlo.multiply %19, %cst_2 : tensor<1x7xf32>
    %21 = stablehlo.reshape %20 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %22 = stablehlo.add %21, %cst_3 : tensor<1x7x1xf32>
    %23 = stablehlo.rsqrt %22 : tensor<1x7x1xf32>
    %24 = stablehlo.reshape %23 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %25 = stablehlo.broadcast_in_dim %24, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %26 = stablehlo.multiply %17, %25 : tensor<1x7x8192xf32>
    %27 = stablehlo.convert %26 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %28 = stablehlo.multiply %9, %27 : tensor<1x7x8192xbf16>
    %29 = stablehlo.reshape %28 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %30 = stablehlo.reshape %arg0 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %32 = stablehlo.transpose %31, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %33 = stablehlo.dot_general %29, %32, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %34 = stablehlo.reshape %33 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %35 = stablehlo.transpose %34, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %36 = stablehlo.reshape %arg5 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %37 = stablehlo.reshape %36 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %38 = stablehlo.transpose %37, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %39 = stablehlo.dot_general %29, %38, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %40 = stablehlo.reshape %39 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %41 = stablehlo.transpose %40, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %42 = stablehlo.reshape %arg4 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %43 = stablehlo.reshape %42 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %44 = stablehlo.dot_general %43, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %45 = stablehlo.transpose %44, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %46 = stablehlo.concatenate %45, %45, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %47 = stablehlo.cosine %46 : tensor<1x7x128xf32>
    %48 = stablehlo.convert %47 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %49 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %50 = stablehlo.multiply %41, %49 : tensor<1x8x7x128xbf16>
    %51 = stablehlo.slice %41 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %52 = stablehlo.negate %51 : tensor<1x8x7x64xbf16>
    %53 = stablehlo.slice %41 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %54 = stablehlo.concatenate %52, %53, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %55 = stablehlo.sine %46 : tensor<1x7x128xf32>
    %56 = stablehlo.convert %55 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %57 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.multiply %54, %57 : tensor<1x8x7x128xbf16>
    %59 = stablehlo.add %50, %58 : tensor<1x8x7x128xbf16>
    %60 = stablehlo.reshape %arg14 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %61 = stablehlo.reshape %60 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %62 = stablehlo.broadcast_in_dim %61, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %63 = stablehlo.reshape %arg11 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %64 = stablehlo.reshape %63 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %65 = stablehlo.transpose %64, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %66 = stablehlo.dot_general %29, %65, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %67 = stablehlo.reshape %66 : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
    %68 = stablehlo.transpose %67, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
    %69 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %70 = stablehlo.multiply %68, %69 : tensor<1x64x7x128xbf16>
    %71 = stablehlo.slice %68 [0:1, 0:64, 0:7, 64:128] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %72 = stablehlo.negate %71 : tensor<1x64x7x64xbf16>
    %73 = stablehlo.slice %68 [0:1, 0:64, 0:7, 0:64] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %74 = stablehlo.concatenate %72, %73, dim = 3 : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
    %75 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %76 = stablehlo.multiply %74, %75 : tensor<1x64x7x128xbf16>
    %77 = stablehlo.add %70, %76 : tensor<1x64x7x128xbf16>
    %78 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %79 = stablehlo.reshape %78 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %80 = stablehlo.transpose %79, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
    %81 = stablehlo.dot_general %77, %80, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
    %82 = stablehlo.multiply %81, %5 : tensor<1x64x7x7xbf16>
    %83 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %84 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %85 = stablehlo.subtract %83, %84 : tensor<7x7xi64>
    %86 = stablehlo.compare  GE, %85, %4 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %87 = stablehlo.select %86, %3, %2 : tensor<7x7xi1>, tensor<7x7xbf16>
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %89 = stablehlo.convert %88 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
    %90 = stablehlo.multiply %87, %89 : tensor<7x7xbf16>
    %91 = stablehlo.reshape %90 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
    %92 = stablehlo.reshape %arg10 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %93 = stablehlo.reshape %92 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
    %94 = stablehlo.convert %93 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
    %95 = stablehlo.reshape %94 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
    %97 = stablehlo.add %91, %96 : tensor<1x1x7x7xbf16>
    %98 = stablehlo.compare  EQ, %97, %1 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
    %99 = stablehlo.select %98, %0, %91 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
    %100 = stablehlo.reshape %99 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
    %102 = stablehlo.add %82, %101 : tensor<1x64x7x7xbf16>
    %103 = stablehlo.convert %102 : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
    %104 = stablehlo.reduce(%103 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %106 = stablehlo.subtract %103, %105 : tensor<1x64x7x7xf32>
    %107 = stablehlo.exponential %106 : tensor<1x64x7x7xf32>
    %108 = stablehlo.reduce(%107 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %110 = stablehlo.divide %107, %109 : tensor<1x64x7x7xf32>
    %111 = stablehlo.convert %110 : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
    %112 = stablehlo.broadcast_in_dim %35, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %113 = stablehlo.reshape %112 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %114 = stablehlo.dot_general %111, %113, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %115 = stablehlo.transpose %114, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
    %116 = stablehlo.reshape %115 : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
    %117 = stablehlo.reshape %arg9 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %118 = stablehlo.reshape %117 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %119 = stablehlo.transpose %118, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %120 = stablehlo.dot_general %116, %119, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %121 = stablehlo.reshape %120 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %122 = stablehlo.add %16, %121 : tensor<1x7x8192xbf16>
    %123 = stablehlo.reshape %arg12 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %124 = stablehlo.reshape %123 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %125 = stablehlo.broadcast_in_dim %124, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %126 = stablehlo.convert %122 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %127 = stablehlo.power %126, %6 : tensor<1x7x8192xf32>
    %128 = stablehlo.reduce(%127 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %129 = stablehlo.multiply %128, %cst_2 : tensor<1x7xf32>
    %130 = stablehlo.reshape %129 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %131 = stablehlo.add %130, %cst_3 : tensor<1x7x1xf32>
    %132 = stablehlo.rsqrt %131 : tensor<1x7x1xf32>
    %133 = stablehlo.reshape %132 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %135 = stablehlo.multiply %126, %134 : tensor<1x7x8192xf32>
    %136 = stablehlo.convert %135 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %137 = stablehlo.multiply %125, %136 : tensor<1x7x8192xbf16>
    %138 = stablehlo.reshape %137 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %139 = stablehlo.reshape %arg13 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %141 = stablehlo.transpose %140, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %142 = stablehlo.dot_general %138, %141, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %143 = stablehlo.reshape %142 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %144 = stablehlo.logistic %143 : tensor<1x7x28672xbf16>
    %145 = stablehlo.multiply %143, %144 : tensor<1x7x28672xbf16>
    %146 = stablehlo.reshape %arg8 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %148 = stablehlo.transpose %147, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %149 = stablehlo.dot_general %138, %148, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %150 = stablehlo.reshape %149 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %151 = stablehlo.multiply %145, %150 : tensor<1x7x28672xbf16>
    %152 = stablehlo.reshape %151 : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
    %153 = stablehlo.reshape %arg7 : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
    %155 = stablehlo.transpose %154, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
    %156 = stablehlo.dot_general %152, %155, contracting_dims = [1] x [0] : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
    %157 = stablehlo.reshape %156 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %158 = stablehlo.add %122, %157 : tensor<1x7x8192xbf16>
    %159 = stablehlo.convert %158 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %160 = stablehlo.power %159, %6 : tensor<1x7x8192xf32>
    %161 = stablehlo.reduce(%160 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %162 = stablehlo.multiply %161, %cst_2 : tensor<1x7xf32>
    %163 = stablehlo.reshape %162 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %164 = stablehlo.add %163, %cst_3 : tensor<1x7x1xf32>
    %165 = stablehlo.rsqrt %164 : tensor<1x7x1xf32>
    %166 = stablehlo.reshape %165 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %167 = stablehlo.broadcast_in_dim %166, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %168 = stablehlo.multiply %159, %167 : tensor<1x7x8192xf32>
    %169 = stablehlo.convert %168 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %170 = stablehlo.multiply %62, %169 : tensor<1x7x8192xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %172 = stablehlo.reshape %arg6 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
    %176 = stablehlo.reshape %175 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %35, %59, %176 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before AnalyzeMeshPass (analyze-mesh) ('builtin.module' operation: @SyncTensorsGraph.406) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8, "_axis_1"=4]>
  func.func @main(%arg0: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg1: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg2: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg3: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg4: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg5: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg6: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg7: tensor<8192x28672xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg8: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg9: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg10: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg11: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg12: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg13: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg14: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
    %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
    %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
    %c_6 = stablehlo.constant dense<1> : tensor<i64>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %4 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
    %5 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %7 = stablehlo.reshape %arg3 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %8 = stablehlo.reshape %7 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %9 = stablehlo.broadcast_in_dim %8, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %10 = stablehlo.reshape %arg2 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %12 = stablehlo.reshape %arg1 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %13 = stablehlo.reshape %12 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %14 = stablehlo.convert %13 : (tensor<7xi64>) -> tensor<7xui32>
    %15 = "stablehlo.gather"(%11, %14) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
    %16 = stablehlo.reshape %15 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %17 = stablehlo.convert %16 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %18 = stablehlo.power %17, %6 : tensor<1x7x8192xf32>
    %19 = stablehlo.reduce(%18 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %20 = stablehlo.multiply %19, %cst_2 : tensor<1x7xf32>
    %21 = stablehlo.reshape %20 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %22 = stablehlo.add %21, %cst_3 : tensor<1x7x1xf32>
    %23 = stablehlo.rsqrt %22 : tensor<1x7x1xf32>
    %24 = stablehlo.reshape %23 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %25 = stablehlo.broadcast_in_dim %24, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %26 = stablehlo.multiply %17, %25 : tensor<1x7x8192xf32>
    %27 = stablehlo.convert %26 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %28 = stablehlo.multiply %9, %27 : tensor<1x7x8192xbf16>
    %29 = stablehlo.reshape %28 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %30 = stablehlo.reshape %arg0 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %32 = stablehlo.transpose %31, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %33 = stablehlo.dot_general %29, %32, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %34 = stablehlo.reshape %33 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %35 = stablehlo.transpose %34, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %36 = stablehlo.reshape %arg5 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %37 = stablehlo.reshape %36 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %38 = stablehlo.transpose %37, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %39 = stablehlo.dot_general %29, %38, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %40 = stablehlo.reshape %39 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %41 = stablehlo.transpose %40, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %42 = stablehlo.reshape %arg4 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %43 = stablehlo.reshape %42 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %44 = stablehlo.dot_general %43, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %45 = stablehlo.transpose %44, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %46 = stablehlo.concatenate %45, %45, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %47 = stablehlo.cosine %46 : tensor<1x7x128xf32>
    %48 = stablehlo.convert %47 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %49 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %50 = stablehlo.multiply %41, %49 : tensor<1x8x7x128xbf16>
    %51 = stablehlo.slice %41 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %52 = stablehlo.negate %51 : tensor<1x8x7x64xbf16>
    %53 = stablehlo.slice %41 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %54 = stablehlo.concatenate %52, %53, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %55 = stablehlo.sine %46 : tensor<1x7x128xf32>
    %56 = stablehlo.convert %55 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %57 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.multiply %54, %57 : tensor<1x8x7x128xbf16>
    %59 = stablehlo.add %50, %58 : tensor<1x8x7x128xbf16>
    %60 = stablehlo.reshape %arg14 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %61 = stablehlo.reshape %60 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %62 = stablehlo.broadcast_in_dim %61, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %63 = stablehlo.reshape %arg11 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %64 = stablehlo.reshape %63 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %65 = stablehlo.transpose %64, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %66 = stablehlo.dot_general %29, %65, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %67 = stablehlo.reshape %66 : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
    %68 = stablehlo.transpose %67, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
    %69 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %70 = stablehlo.multiply %68, %69 : tensor<1x64x7x128xbf16>
    %71 = stablehlo.slice %68 [0:1, 0:64, 0:7, 64:128] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %72 = stablehlo.negate %71 : tensor<1x64x7x64xbf16>
    %73 = stablehlo.slice %68 [0:1, 0:64, 0:7, 0:64] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %74 = stablehlo.concatenate %72, %73, dim = 3 : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
    %75 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %76 = stablehlo.multiply %74, %75 : tensor<1x64x7x128xbf16>
    %77 = stablehlo.add %70, %76 : tensor<1x64x7x128xbf16>
    %78 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %79 = stablehlo.reshape %78 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %80 = stablehlo.transpose %79, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
    %81 = stablehlo.dot_general %77, %80, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
    %82 = stablehlo.multiply %81, %5 : tensor<1x64x7x7xbf16>
    %83 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %84 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %85 = stablehlo.subtract %83, %84 : tensor<7x7xi64>
    %86 = stablehlo.compare  GE, %85, %4 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %87 = stablehlo.select %86, %3, %2 : tensor<7x7xi1>, tensor<7x7xbf16>
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %89 = stablehlo.convert %88 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
    %90 = stablehlo.multiply %87, %89 : tensor<7x7xbf16>
    %91 = stablehlo.reshape %90 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
    %92 = stablehlo.reshape %arg10 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %93 = stablehlo.reshape %92 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
    %94 = stablehlo.convert %93 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
    %95 = stablehlo.reshape %94 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
    %97 = stablehlo.add %91, %96 : tensor<1x1x7x7xbf16>
    %98 = stablehlo.compare  EQ, %97, %1 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
    %99 = stablehlo.select %98, %0, %91 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
    %100 = stablehlo.reshape %99 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
    %102 = stablehlo.add %82, %101 : tensor<1x64x7x7xbf16>
    %103 = stablehlo.convert %102 : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
    %104 = stablehlo.reduce(%103 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %106 = stablehlo.subtract %103, %105 : tensor<1x64x7x7xf32>
    %107 = stablehlo.exponential %106 : tensor<1x64x7x7xf32>
    %108 = stablehlo.reduce(%107 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %110 = stablehlo.divide %107, %109 : tensor<1x64x7x7xf32>
    %111 = stablehlo.convert %110 : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
    %112 = stablehlo.broadcast_in_dim %35, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %113 = stablehlo.reshape %112 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %114 = stablehlo.dot_general %111, %113, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %115 = stablehlo.transpose %114, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
    %116 = stablehlo.reshape %115 : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
    %117 = stablehlo.reshape %arg9 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %118 = stablehlo.reshape %117 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %119 = stablehlo.transpose %118, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %120 = stablehlo.dot_general %116, %119, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %121 = stablehlo.reshape %120 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %122 = stablehlo.add %16, %121 : tensor<1x7x8192xbf16>
    %123 = stablehlo.reshape %arg12 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %124 = stablehlo.reshape %123 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %125 = stablehlo.broadcast_in_dim %124, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %126 = stablehlo.convert %122 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %127 = stablehlo.power %126, %6 : tensor<1x7x8192xf32>
    %128 = stablehlo.reduce(%127 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %129 = stablehlo.multiply %128, %cst_2 : tensor<1x7xf32>
    %130 = stablehlo.reshape %129 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %131 = stablehlo.add %130, %cst_3 : tensor<1x7x1xf32>
    %132 = stablehlo.rsqrt %131 : tensor<1x7x1xf32>
    %133 = stablehlo.reshape %132 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %135 = stablehlo.multiply %126, %134 : tensor<1x7x8192xf32>
    %136 = stablehlo.convert %135 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %137 = stablehlo.multiply %125, %136 : tensor<1x7x8192xbf16>
    %138 = stablehlo.reshape %137 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %139 = stablehlo.reshape %arg13 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %141 = stablehlo.transpose %140, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %142 = stablehlo.dot_general %138, %141, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %143 = stablehlo.reshape %142 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %144 = stablehlo.logistic %143 : tensor<1x7x28672xbf16>
    %145 = stablehlo.multiply %143, %144 : tensor<1x7x28672xbf16>
    %146 = stablehlo.reshape %arg8 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %148 = stablehlo.transpose %147, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %149 = stablehlo.dot_general %138, %148, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %150 = stablehlo.reshape %149 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %151 = stablehlo.multiply %145, %150 : tensor<1x7x28672xbf16>
    %152 = stablehlo.reshape %151 : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
    %153 = stablehlo.reshape %arg7 : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
    %155 = stablehlo.transpose %154, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
    %156 = stablehlo.dot_general %152, %155, contracting_dims = [1] x [0] : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
    %157 = stablehlo.reshape %156 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %158 = stablehlo.add %122, %157 : tensor<1x7x8192xbf16>
    %159 = stablehlo.convert %158 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %160 = stablehlo.power %159, %6 : tensor<1x7x8192xf32>
    %161 = stablehlo.reduce(%160 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %162 = stablehlo.multiply %161, %cst_2 : tensor<1x7xf32>
    %163 = stablehlo.reshape %162 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %164 = stablehlo.add %163, %cst_3 : tensor<1x7x1xf32>
    %165 = stablehlo.rsqrt %164 : tensor<1x7x1xf32>
    %166 = stablehlo.reshape %165 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %167 = stablehlo.broadcast_in_dim %166, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %168 = stablehlo.multiply %159, %167 : tensor<1x7x8192xf32>
    %169 = stablehlo.convert %168 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %170 = stablehlo.multiply %62, %169 : tensor<1x7x8192xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %172 = stablehlo.reshape %arg6 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
    %176 = stablehlo.reshape %175 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %35, %59, %176 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before DecoupleConstFanoutPass (decouple-const-fanout) ('builtin.module' operation: @SyncTensorsGraph.406) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8, "_axis_1"=4]>
  func.func @main(%arg0: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg1: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg2: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg3: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg4: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg5: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg6: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg7: tensor<8192x28672xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg8: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg9: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg10: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg11: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg12: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg13: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg14: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
    %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
    %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
    %c_6 = stablehlo.constant dense<1> : tensor<i64>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %4 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
    %5 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %7 = stablehlo.reshape %arg3 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %8 = stablehlo.reshape %7 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %9 = stablehlo.broadcast_in_dim %8, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %10 = stablehlo.reshape %arg2 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %12 = stablehlo.reshape %arg1 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %13 = stablehlo.reshape %12 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %14 = stablehlo.convert %13 : (tensor<7xi64>) -> tensor<7xui32>
    %15 = "stablehlo.gather"(%11, %14) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
    %16 = stablehlo.reshape %15 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %17 = stablehlo.convert %16 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %18 = stablehlo.power %17, %6 : tensor<1x7x8192xf32>
    %19 = stablehlo.reduce(%18 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %20 = stablehlo.multiply %19, %cst_2 : tensor<1x7xf32>
    %21 = stablehlo.reshape %20 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %22 = stablehlo.add %21, %cst_3 : tensor<1x7x1xf32>
    %23 = stablehlo.rsqrt %22 : tensor<1x7x1xf32>
    %24 = stablehlo.reshape %23 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %25 = stablehlo.broadcast_in_dim %24, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %26 = stablehlo.multiply %17, %25 : tensor<1x7x8192xf32>
    %27 = stablehlo.convert %26 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %28 = stablehlo.multiply %9, %27 : tensor<1x7x8192xbf16>
    %29 = stablehlo.reshape %28 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %30 = stablehlo.reshape %arg0 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %31 = stablehlo.reshape %30 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %32 = stablehlo.transpose %31, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %33 = stablehlo.dot_general %29, %32, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %34 = stablehlo.reshape %33 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %35 = stablehlo.transpose %34, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %36 = stablehlo.reshape %arg5 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %37 = stablehlo.reshape %36 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %38 = stablehlo.transpose %37, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %39 = stablehlo.dot_general %29, %38, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %40 = stablehlo.reshape %39 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %41 = stablehlo.transpose %40, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %42 = stablehlo.reshape %arg4 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %43 = stablehlo.reshape %42 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %44 = stablehlo.dot_general %43, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %45 = stablehlo.transpose %44, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %46 = stablehlo.concatenate %45, %45, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %47 = stablehlo.cosine %46 : tensor<1x7x128xf32>
    %48 = stablehlo.convert %47 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %49 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %50 = stablehlo.multiply %41, %49 : tensor<1x8x7x128xbf16>
    %51 = stablehlo.slice %41 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %52 = stablehlo.negate %51 : tensor<1x8x7x64xbf16>
    %53 = stablehlo.slice %41 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %54 = stablehlo.concatenate %52, %53, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %55 = stablehlo.sine %46 : tensor<1x7x128xf32>
    %56 = stablehlo.convert %55 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %57 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %58 = stablehlo.multiply %54, %57 : tensor<1x8x7x128xbf16>
    %59 = stablehlo.add %50, %58 : tensor<1x8x7x128xbf16>
    %60 = stablehlo.reshape %arg14 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %61 = stablehlo.reshape %60 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %62 = stablehlo.broadcast_in_dim %61, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %63 = stablehlo.reshape %arg11 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %64 = stablehlo.reshape %63 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %65 = stablehlo.transpose %64, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %66 = stablehlo.dot_general %29, %65, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %67 = stablehlo.reshape %66 : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
    %68 = stablehlo.transpose %67, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
    %69 = stablehlo.broadcast_in_dim %48, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %70 = stablehlo.multiply %68, %69 : tensor<1x64x7x128xbf16>
    %71 = stablehlo.slice %68 [0:1, 0:64, 0:7, 64:128] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %72 = stablehlo.negate %71 : tensor<1x64x7x64xbf16>
    %73 = stablehlo.slice %68 [0:1, 0:64, 0:7, 0:64] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %74 = stablehlo.concatenate %72, %73, dim = 3 : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
    %75 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %76 = stablehlo.multiply %74, %75 : tensor<1x64x7x128xbf16>
    %77 = stablehlo.add %70, %76 : tensor<1x64x7x128xbf16>
    %78 = stablehlo.broadcast_in_dim %59, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %79 = stablehlo.reshape %78 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %80 = stablehlo.transpose %79, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
    %81 = stablehlo.dot_general %77, %80, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
    %82 = stablehlo.multiply %81, %5 : tensor<1x64x7x7xbf16>
    %83 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %84 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %85 = stablehlo.subtract %83, %84 : tensor<7x7xi64>
    %86 = stablehlo.compare  GE, %85, %4 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %87 = stablehlo.select %86, %3, %2 : tensor<7x7xi1>, tensor<7x7xbf16>
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %89 = stablehlo.convert %88 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
    %90 = stablehlo.multiply %87, %89 : tensor<7x7xbf16>
    %91 = stablehlo.reshape %90 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
    %92 = stablehlo.reshape %arg10 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %93 = stablehlo.reshape %92 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
    %94 = stablehlo.convert %93 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
    %95 = stablehlo.reshape %94 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
    %97 = stablehlo.add %91, %96 : tensor<1x1x7x7xbf16>
    %98 = stablehlo.compare  EQ, %97, %1 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
    %99 = stablehlo.select %98, %0, %91 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
    %100 = stablehlo.reshape %99 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
    %102 = stablehlo.add %82, %101 : tensor<1x64x7x7xbf16>
    %103 = stablehlo.convert %102 : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
    %104 = stablehlo.reduce(%103 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %106 = stablehlo.subtract %103, %105 : tensor<1x64x7x7xf32>
    %107 = stablehlo.exponential %106 : tensor<1x64x7x7xf32>
    %108 = stablehlo.reduce(%107 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %110 = stablehlo.divide %107, %109 : tensor<1x64x7x7xf32>
    %111 = stablehlo.convert %110 : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
    %112 = stablehlo.broadcast_in_dim %35, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %113 = stablehlo.reshape %112 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %114 = stablehlo.dot_general %111, %113, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %115 = stablehlo.transpose %114, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
    %116 = stablehlo.reshape %115 : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
    %117 = stablehlo.reshape %arg9 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %118 = stablehlo.reshape %117 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %119 = stablehlo.transpose %118, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %120 = stablehlo.dot_general %116, %119, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %121 = stablehlo.reshape %120 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %122 = stablehlo.add %16, %121 : tensor<1x7x8192xbf16>
    %123 = stablehlo.reshape %arg12 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %124 = stablehlo.reshape %123 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %125 = stablehlo.broadcast_in_dim %124, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %126 = stablehlo.convert %122 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %127 = stablehlo.power %126, %6 : tensor<1x7x8192xf32>
    %128 = stablehlo.reduce(%127 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %129 = stablehlo.multiply %128, %cst_2 : tensor<1x7xf32>
    %130 = stablehlo.reshape %129 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %131 = stablehlo.add %130, %cst_3 : tensor<1x7x1xf32>
    %132 = stablehlo.rsqrt %131 : tensor<1x7x1xf32>
    %133 = stablehlo.reshape %132 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %134 = stablehlo.broadcast_in_dim %133, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %135 = stablehlo.multiply %126, %134 : tensor<1x7x8192xf32>
    %136 = stablehlo.convert %135 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %137 = stablehlo.multiply %125, %136 : tensor<1x7x8192xbf16>
    %138 = stablehlo.reshape %137 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %139 = stablehlo.reshape %arg13 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %140 = stablehlo.reshape %139 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %141 = stablehlo.transpose %140, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %142 = stablehlo.dot_general %138, %141, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %143 = stablehlo.reshape %142 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %144 = stablehlo.logistic %143 : tensor<1x7x28672xbf16>
    %145 = stablehlo.multiply %143, %144 : tensor<1x7x28672xbf16>
    %146 = stablehlo.reshape %arg8 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %148 = stablehlo.transpose %147, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %149 = stablehlo.dot_general %138, %148, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %150 = stablehlo.reshape %149 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %151 = stablehlo.multiply %145, %150 : tensor<1x7x28672xbf16>
    %152 = stablehlo.reshape %151 : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
    %153 = stablehlo.reshape %arg7 : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
    %155 = stablehlo.transpose %154, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
    %156 = stablehlo.dot_general %152, %155, contracting_dims = [1] x [0] : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
    %157 = stablehlo.reshape %156 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %158 = stablehlo.add %122, %157 : tensor<1x7x8192xbf16>
    %159 = stablehlo.convert %158 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %160 = stablehlo.power %159, %6 : tensor<1x7x8192xf32>
    %161 = stablehlo.reduce(%160 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %162 = stablehlo.multiply %161, %cst_2 : tensor<1x7xf32>
    %163 = stablehlo.reshape %162 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %164 = stablehlo.add %163, %cst_3 : tensor<1x7x1xf32>
    %165 = stablehlo.rsqrt %164 : tensor<1x7x1xf32>
    %166 = stablehlo.reshape %165 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %167 = stablehlo.broadcast_in_dim %166, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %168 = stablehlo.multiply %159, %167 : tensor<1x7x8192xf32>
    %169 = stablehlo.convert %168 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %170 = stablehlo.multiply %62, %169 : tensor<1x7x8192xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %172 = stablehlo.reshape %arg6 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %173 = stablehlo.reshape %172 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
    %176 = stablehlo.reshape %175 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %35, %59, %176 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump After DecoupleConstFanoutPass (decouple-const-fanout) ('builtin.module' operation: @SyncTensorsGraph.406) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8, "_axis_1"=4]>
  func.func @main(%arg0: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg1: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg2: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg3: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg4: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg5: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg6: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg7: tensor<8192x28672xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg8: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg9: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg10: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg11: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg12: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg13: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg14: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
    %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
    %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
    %c_6 = stablehlo.constant dense<1> : tensor<i64>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %4 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
    %5 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %7 = stablehlo.reshape %arg3 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %8 = stablehlo.reshape %7 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %9 = stablehlo.broadcast_in_dim %8, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %10 = stablehlo.reshape %arg2 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %12 = stablehlo.reshape %arg1 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %13 = stablehlo.reshape %12 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %14 = stablehlo.convert %13 : (tensor<7xi64>) -> tensor<7xui32>
    %15 = "stablehlo.gather"(%11, %14) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
    %16 = stablehlo.reshape %15 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %17 = stablehlo.convert %16 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %18 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %19 = stablehlo.power %17, %18 : tensor<1x7x8192xf32>
    %20 = stablehlo.reduce(%19 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %21 = stablehlo.multiply %20, %cst_2 : tensor<1x7xf32>
    %22 = stablehlo.reshape %21 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %23 = stablehlo.add %22, %cst_3 : tensor<1x7x1xf32>
    %24 = stablehlo.rsqrt %23 : tensor<1x7x1xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %26 = stablehlo.broadcast_in_dim %25, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %27 = stablehlo.multiply %17, %26 : tensor<1x7x8192xf32>
    %28 = stablehlo.convert %27 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %29 = stablehlo.multiply %9, %28 : tensor<1x7x8192xbf16>
    %30 = stablehlo.reshape %29 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %31 = stablehlo.reshape %arg0 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %32 = stablehlo.reshape %31 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %33 = stablehlo.transpose %32, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %34 = stablehlo.dot_general %30, %33, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %35 = stablehlo.reshape %34 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %36 = stablehlo.transpose %35, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %37 = stablehlo.reshape %arg5 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %38 = stablehlo.reshape %37 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %40 = stablehlo.dot_general %30, %39, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.reshape %arg4 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %45 = stablehlo.dot_general %44, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %46 = stablehlo.transpose %45, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %47 = stablehlo.concatenate %46, %46, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %48 = stablehlo.cosine %47 : tensor<1x7x128xf32>
    %49 = stablehlo.convert %48 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %50 = stablehlo.broadcast_in_dim %49, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %51 = stablehlo.multiply %42, %50 : tensor<1x8x7x128xbf16>
    %52 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %53 = stablehlo.negate %52 : tensor<1x8x7x64xbf16>
    %54 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %55 = stablehlo.concatenate %53, %54, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %56 = stablehlo.sine %47 : tensor<1x7x128xf32>
    %57 = stablehlo.convert %56 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %58 = stablehlo.broadcast_in_dim %57, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %59 = stablehlo.multiply %55, %58 : tensor<1x8x7x128xbf16>
    %60 = stablehlo.add %51, %59 : tensor<1x8x7x128xbf16>
    %61 = stablehlo.reshape %arg14 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %63 = stablehlo.broadcast_in_dim %62, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %64 = stablehlo.reshape %arg11 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %66 = stablehlo.transpose %65, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %67 = stablehlo.dot_general %30, %66, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %68 = stablehlo.reshape %67 : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
    %70 = stablehlo.broadcast_in_dim %49, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %71 = stablehlo.multiply %69, %70 : tensor<1x64x7x128xbf16>
    %72 = stablehlo.slice %69 [0:1, 0:64, 0:7, 64:128] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %73 = stablehlo.negate %72 : tensor<1x64x7x64xbf16>
    %74 = stablehlo.slice %69 [0:1, 0:64, 0:7, 0:64] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %75 = stablehlo.concatenate %73, %74, dim = 3 : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
    %76 = stablehlo.broadcast_in_dim %57, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %77 = stablehlo.multiply %75, %76 : tensor<1x64x7x128xbf16>
    %78 = stablehlo.add %71, %77 : tensor<1x64x7x128xbf16>
    %79 = stablehlo.broadcast_in_dim %60, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %80 = stablehlo.reshape %79 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %81 = stablehlo.transpose %80, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
    %82 = stablehlo.dot_general %78, %81, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
    %83 = stablehlo.multiply %82, %5 : tensor<1x64x7x7xbf16>
    %84 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %85 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %86 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %87 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %88 = stablehlo.subtract %86, %87 : tensor<7x7xi64>
    %89 = stablehlo.compare  GE, %88, %4 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %90 = stablehlo.select %89, %3, %2 : tensor<7x7xi1>, tensor<7x7xbf16>
    %91 = stablehlo.compare  GT, %84, %85 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %92 = stablehlo.convert %91 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
    %93 = stablehlo.multiply %90, %92 : tensor<7x7xbf16>
    %94 = stablehlo.reshape %93 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
    %95 = stablehlo.reshape %arg10 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %96 = stablehlo.reshape %95 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
    %97 = stablehlo.convert %96 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
    %98 = stablehlo.reshape %97 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
    %99 = stablehlo.broadcast_in_dim %98, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
    %100 = stablehlo.add %94, %99 : tensor<1x1x7x7xbf16>
    %101 = stablehlo.compare  EQ, %100, %1 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
    %102 = stablehlo.select %101, %0, %94 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
    %103 = stablehlo.reshape %102 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0, 2, 3] : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
    %105 = stablehlo.add %83, %104 : tensor<1x64x7x7xbf16>
    %106 = stablehlo.convert %105 : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
    %107 = stablehlo.reduce(%106 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %108 = stablehlo.broadcast_in_dim %107, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %109 = stablehlo.subtract %106, %108 : tensor<1x64x7x7xf32>
    %110 = stablehlo.exponential %109 : tensor<1x64x7x7xf32>
    %111 = stablehlo.reduce(%110 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %112 = stablehlo.broadcast_in_dim %111, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %113 = stablehlo.divide %110, %112 : tensor<1x64x7x7xf32>
    %114 = stablehlo.convert %113 : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
    %115 = stablehlo.broadcast_in_dim %36, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %116 = stablehlo.reshape %115 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %117 = stablehlo.dot_general %114, %116, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %118 = stablehlo.transpose %117, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
    %120 = stablehlo.reshape %arg9 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %121 = stablehlo.reshape %120 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %122 = stablehlo.transpose %121, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %123 = stablehlo.dot_general %119, %122, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %124 = stablehlo.reshape %123 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %125 = stablehlo.add %16, %124 : tensor<1x7x8192xbf16>
    %126 = stablehlo.reshape %arg12 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %127 = stablehlo.reshape %126 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %128 = stablehlo.broadcast_in_dim %127, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %129 = stablehlo.convert %125 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %130 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %131 = stablehlo.power %129, %130 : tensor<1x7x8192xf32>
    %132 = stablehlo.reduce(%131 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %133 = stablehlo.multiply %132, %cst_2 : tensor<1x7xf32>
    %134 = stablehlo.reshape %133 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %135 = stablehlo.add %134, %cst_3 : tensor<1x7x1xf32>
    %136 = stablehlo.rsqrt %135 : tensor<1x7x1xf32>
    %137 = stablehlo.reshape %136 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %139 = stablehlo.multiply %129, %138 : tensor<1x7x8192xf32>
    %140 = stablehlo.convert %139 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %141 = stablehlo.multiply %128, %140 : tensor<1x7x8192xbf16>
    %142 = stablehlo.reshape %141 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %143 = stablehlo.reshape %arg13 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %144 = stablehlo.reshape %143 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %145 = stablehlo.transpose %144, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %146 = stablehlo.dot_general %142, %145, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %147 = stablehlo.reshape %146 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %148 = stablehlo.logistic %147 : tensor<1x7x28672xbf16>
    %149 = stablehlo.multiply %147, %148 : tensor<1x7x28672xbf16>
    %150 = stablehlo.reshape %arg8 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %152 = stablehlo.transpose %151, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %153 = stablehlo.dot_general %142, %152, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %154 = stablehlo.reshape %153 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %155 = stablehlo.multiply %149, %154 : tensor<1x7x28672xbf16>
    %156 = stablehlo.reshape %155 : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
    %157 = stablehlo.reshape %arg7 : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
    %158 = stablehlo.reshape %157 : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
    %159 = stablehlo.transpose %158, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
    %160 = stablehlo.dot_general %156, %159, contracting_dims = [1] x [0] : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
    %161 = stablehlo.reshape %160 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %162 = stablehlo.add %125, %161 : tensor<1x7x8192xbf16>
    %163 = stablehlo.convert %162 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %164 = stablehlo.power %163, %6 : tensor<1x7x8192xf32>
    %165 = stablehlo.reduce(%164 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %166 = stablehlo.multiply %165, %cst_2 : tensor<1x7xf32>
    %167 = stablehlo.reshape %166 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %168 = stablehlo.add %167, %cst_3 : tensor<1x7x1xf32>
    %169 = stablehlo.rsqrt %168 : tensor<1x7x1xf32>
    %170 = stablehlo.reshape %169 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %171 = stablehlo.broadcast_in_dim %170, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %172 = stablehlo.multiply %163, %171 : tensor<1x7x8192xf32>
    %173 = stablehlo.convert %172 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %174 = stablehlo.multiply %63, %173 : tensor<1x7x8192xbf16>
    %175 = stablehlo.reshape %174 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = stablehlo.reshape %arg6 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %177 = stablehlo.reshape %176 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %178 = stablehlo.transpose %177, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
    %179 = stablehlo.dot_general %175, %178, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
    %180 = stablehlo.reshape %179 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %36, %60, %180 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before FlattenCompositePass (flatten-composite) ('builtin.module' operation: @SyncTensorsGraph.406) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8, "_axis_1"=4]>
  func.func @main(%arg0: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg1: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg2: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg3: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg4: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg5: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg6: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg7: tensor<8192x28672xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg8: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg9: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg10: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg11: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg12: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg13: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg14: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
    %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
    %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
    %c_6 = stablehlo.constant dense<1> : tensor<i64>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %4 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
    %5 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %7 = stablehlo.reshape %arg3 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %8 = stablehlo.reshape %7 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %9 = stablehlo.broadcast_in_dim %8, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %10 = stablehlo.reshape %arg2 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %12 = stablehlo.reshape %arg1 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %13 = stablehlo.reshape %12 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %14 = stablehlo.convert %13 : (tensor<7xi64>) -> tensor<7xui32>
    %15 = "stablehlo.gather"(%11, %14) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
    %16 = stablehlo.reshape %15 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %17 = stablehlo.convert %16 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %18 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %19 = stablehlo.power %17, %18 : tensor<1x7x8192xf32>
    %20 = stablehlo.reduce(%19 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %21 = stablehlo.multiply %20, %cst_2 : tensor<1x7xf32>
    %22 = stablehlo.reshape %21 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %23 = stablehlo.add %22, %cst_3 : tensor<1x7x1xf32>
    %24 = stablehlo.rsqrt %23 : tensor<1x7x1xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %26 = stablehlo.broadcast_in_dim %25, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %27 = stablehlo.multiply %17, %26 : tensor<1x7x8192xf32>
    %28 = stablehlo.convert %27 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %29 = stablehlo.multiply %9, %28 : tensor<1x7x8192xbf16>
    %30 = stablehlo.reshape %29 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %31 = stablehlo.reshape %arg0 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %32 = stablehlo.reshape %31 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %33 = stablehlo.transpose %32, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %34 = stablehlo.dot_general %30, %33, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %35 = stablehlo.reshape %34 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %36 = stablehlo.transpose %35, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %37 = stablehlo.reshape %arg5 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %38 = stablehlo.reshape %37 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %40 = stablehlo.dot_general %30, %39, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.reshape %arg4 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %45 = stablehlo.dot_general %44, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %46 = stablehlo.transpose %45, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %47 = stablehlo.concatenate %46, %46, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %48 = stablehlo.cosine %47 : tensor<1x7x128xf32>
    %49 = stablehlo.convert %48 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %50 = stablehlo.broadcast_in_dim %49, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %51 = stablehlo.multiply %42, %50 : tensor<1x8x7x128xbf16>
    %52 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %53 = stablehlo.negate %52 : tensor<1x8x7x64xbf16>
    %54 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %55 = stablehlo.concatenate %53, %54, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %56 = stablehlo.sine %47 : tensor<1x7x128xf32>
    %57 = stablehlo.convert %56 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %58 = stablehlo.broadcast_in_dim %57, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %59 = stablehlo.multiply %55, %58 : tensor<1x8x7x128xbf16>
    %60 = stablehlo.add %51, %59 : tensor<1x8x7x128xbf16>
    %61 = stablehlo.reshape %arg14 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %63 = stablehlo.broadcast_in_dim %62, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %64 = stablehlo.reshape %arg11 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %66 = stablehlo.transpose %65, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %67 = stablehlo.dot_general %30, %66, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %68 = stablehlo.reshape %67 : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
    %70 = stablehlo.broadcast_in_dim %49, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %71 = stablehlo.multiply %69, %70 : tensor<1x64x7x128xbf16>
    %72 = stablehlo.slice %69 [0:1, 0:64, 0:7, 64:128] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %73 = stablehlo.negate %72 : tensor<1x64x7x64xbf16>
    %74 = stablehlo.slice %69 [0:1, 0:64, 0:7, 0:64] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %75 = stablehlo.concatenate %73, %74, dim = 3 : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
    %76 = stablehlo.broadcast_in_dim %57, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %77 = stablehlo.multiply %75, %76 : tensor<1x64x7x128xbf16>
    %78 = stablehlo.add %71, %77 : tensor<1x64x7x128xbf16>
    %79 = stablehlo.broadcast_in_dim %60, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %80 = stablehlo.reshape %79 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %81 = stablehlo.transpose %80, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
    %82 = stablehlo.dot_general %78, %81, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
    %83 = stablehlo.multiply %82, %5 : tensor<1x64x7x7xbf16>
    %84 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %85 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %86 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %87 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %88 = stablehlo.subtract %86, %87 : tensor<7x7xi64>
    %89 = stablehlo.compare  GE, %88, %4 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %90 = stablehlo.select %89, %3, %2 : tensor<7x7xi1>, tensor<7x7xbf16>
    %91 = stablehlo.compare  GT, %84, %85 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %92 = stablehlo.convert %91 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
    %93 = stablehlo.multiply %90, %92 : tensor<7x7xbf16>
    %94 = stablehlo.reshape %93 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
    %95 = stablehlo.reshape %arg10 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %96 = stablehlo.reshape %95 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
    %97 = stablehlo.convert %96 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
    %98 = stablehlo.reshape %97 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
    %99 = stablehlo.broadcast_in_dim %98, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
    %100 = stablehlo.add %94, %99 : tensor<1x1x7x7xbf16>
    %101 = stablehlo.compare  EQ, %100, %1 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
    %102 = stablehlo.select %101, %0, %94 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
    %103 = stablehlo.reshape %102 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0, 2, 3] : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
    %105 = stablehlo.add %83, %104 : tensor<1x64x7x7xbf16>
    %106 = stablehlo.convert %105 : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
    %107 = stablehlo.reduce(%106 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %108 = stablehlo.broadcast_in_dim %107, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %109 = stablehlo.subtract %106, %108 : tensor<1x64x7x7xf32>
    %110 = stablehlo.exponential %109 : tensor<1x64x7x7xf32>
    %111 = stablehlo.reduce(%110 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %112 = stablehlo.broadcast_in_dim %111, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %113 = stablehlo.divide %110, %112 : tensor<1x64x7x7xf32>
    %114 = stablehlo.convert %113 : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
    %115 = stablehlo.broadcast_in_dim %36, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %116 = stablehlo.reshape %115 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %117 = stablehlo.dot_general %114, %116, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %118 = stablehlo.transpose %117, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
    %120 = stablehlo.reshape %arg9 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %121 = stablehlo.reshape %120 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %122 = stablehlo.transpose %121, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %123 = stablehlo.dot_general %119, %122, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %124 = stablehlo.reshape %123 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %125 = stablehlo.add %16, %124 : tensor<1x7x8192xbf16>
    %126 = stablehlo.reshape %arg12 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %127 = stablehlo.reshape %126 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %128 = stablehlo.broadcast_in_dim %127, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %129 = stablehlo.convert %125 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %130 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %131 = stablehlo.power %129, %130 : tensor<1x7x8192xf32>
    %132 = stablehlo.reduce(%131 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %133 = stablehlo.multiply %132, %cst_2 : tensor<1x7xf32>
    %134 = stablehlo.reshape %133 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %135 = stablehlo.add %134, %cst_3 : tensor<1x7x1xf32>
    %136 = stablehlo.rsqrt %135 : tensor<1x7x1xf32>
    %137 = stablehlo.reshape %136 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %139 = stablehlo.multiply %129, %138 : tensor<1x7x8192xf32>
    %140 = stablehlo.convert %139 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %141 = stablehlo.multiply %128, %140 : tensor<1x7x8192xbf16>
    %142 = stablehlo.reshape %141 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %143 = stablehlo.reshape %arg13 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %144 = stablehlo.reshape %143 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %145 = stablehlo.transpose %144, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %146 = stablehlo.dot_general %142, %145, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %147 = stablehlo.reshape %146 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %148 = stablehlo.logistic %147 : tensor<1x7x28672xbf16>
    %149 = stablehlo.multiply %147, %148 : tensor<1x7x28672xbf16>
    %150 = stablehlo.reshape %arg8 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %152 = stablehlo.transpose %151, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %153 = stablehlo.dot_general %142, %152, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %154 = stablehlo.reshape %153 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %155 = stablehlo.multiply %149, %154 : tensor<1x7x28672xbf16>
    %156 = stablehlo.reshape %155 : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
    %157 = stablehlo.reshape %arg7 : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
    %158 = stablehlo.reshape %157 : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
    %159 = stablehlo.transpose %158, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
    %160 = stablehlo.dot_general %156, %159, contracting_dims = [1] x [0] : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
    %161 = stablehlo.reshape %160 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %162 = stablehlo.add %125, %161 : tensor<1x7x8192xbf16>
    %163 = stablehlo.convert %162 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %164 = stablehlo.power %163, %6 : tensor<1x7x8192xf32>
    %165 = stablehlo.reduce(%164 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %166 = stablehlo.multiply %165, %cst_2 : tensor<1x7xf32>
    %167 = stablehlo.reshape %166 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %168 = stablehlo.add %167, %cst_3 : tensor<1x7x1xf32>
    %169 = stablehlo.rsqrt %168 : tensor<1x7x1xf32>
    %170 = stablehlo.reshape %169 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %171 = stablehlo.broadcast_in_dim %170, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %172 = stablehlo.multiply %163, %171 : tensor<1x7x8192xf32>
    %173 = stablehlo.convert %172 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %174 = stablehlo.multiply %63, %173 : tensor<1x7x8192xbf16>
    %175 = stablehlo.reshape %174 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = stablehlo.reshape %arg6 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %177 = stablehlo.reshape %176 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %178 = stablehlo.transpose %177, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
    %179 = stablehlo.dot_general %175, %178, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
    %180 = stablehlo.reshape %179 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %36, %60, %180 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before RegisterCustomShardingRulePass (register-custom-sharding-rule) ('builtin.module' operation: @SyncTensorsGraph.406) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8, "_axis_1"=4]>
  func.func @main(%arg0: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg1: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg2: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg3: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg4: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg5: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg6: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg7: tensor<8192x28672xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg8: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg9: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg10: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg11: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg12: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg13: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg14: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
    %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
    %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
    %c_6 = stablehlo.constant dense<1> : tensor<i64>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %4 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
    %5 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %7 = stablehlo.reshape %arg3 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %8 = stablehlo.reshape %7 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %9 = stablehlo.broadcast_in_dim %8, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %10 = stablehlo.reshape %arg2 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %12 = stablehlo.reshape %arg1 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %13 = stablehlo.reshape %12 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %14 = stablehlo.convert %13 : (tensor<7xi64>) -> tensor<7xui32>
    %15 = "stablehlo.gather"(%11, %14) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
    %16 = stablehlo.reshape %15 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %17 = stablehlo.convert %16 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %18 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %19 = stablehlo.power %17, %18 : tensor<1x7x8192xf32>
    %20 = stablehlo.reduce(%19 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %21 = stablehlo.multiply %20, %cst_2 : tensor<1x7xf32>
    %22 = stablehlo.reshape %21 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %23 = stablehlo.add %22, %cst_3 : tensor<1x7x1xf32>
    %24 = stablehlo.rsqrt %23 : tensor<1x7x1xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %26 = stablehlo.broadcast_in_dim %25, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %27 = stablehlo.multiply %17, %26 : tensor<1x7x8192xf32>
    %28 = stablehlo.convert %27 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %29 = stablehlo.multiply %9, %28 : tensor<1x7x8192xbf16>
    %30 = stablehlo.reshape %29 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %31 = stablehlo.reshape %arg0 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %32 = stablehlo.reshape %31 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %33 = stablehlo.transpose %32, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %34 = stablehlo.dot_general %30, %33, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %35 = stablehlo.reshape %34 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %36 = stablehlo.transpose %35, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %37 = stablehlo.reshape %arg5 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %38 = stablehlo.reshape %37 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %40 = stablehlo.dot_general %30, %39, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.reshape %arg4 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %45 = stablehlo.dot_general %44, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %46 = stablehlo.transpose %45, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %47 = stablehlo.concatenate %46, %46, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %48 = stablehlo.cosine %47 : tensor<1x7x128xf32>
    %49 = stablehlo.convert %48 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %50 = stablehlo.broadcast_in_dim %49, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %51 = stablehlo.multiply %42, %50 : tensor<1x8x7x128xbf16>
    %52 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %53 = stablehlo.negate %52 : tensor<1x8x7x64xbf16>
    %54 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %55 = stablehlo.concatenate %53, %54, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %56 = stablehlo.sine %47 : tensor<1x7x128xf32>
    %57 = stablehlo.convert %56 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %58 = stablehlo.broadcast_in_dim %57, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %59 = stablehlo.multiply %55, %58 : tensor<1x8x7x128xbf16>
    %60 = stablehlo.add %51, %59 : tensor<1x8x7x128xbf16>
    %61 = stablehlo.reshape %arg14 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %63 = stablehlo.broadcast_in_dim %62, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %64 = stablehlo.reshape %arg11 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %66 = stablehlo.transpose %65, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %67 = stablehlo.dot_general %30, %66, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %68 = stablehlo.reshape %67 : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
    %70 = stablehlo.broadcast_in_dim %49, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %71 = stablehlo.multiply %69, %70 : tensor<1x64x7x128xbf16>
    %72 = stablehlo.slice %69 [0:1, 0:64, 0:7, 64:128] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %73 = stablehlo.negate %72 : tensor<1x64x7x64xbf16>
    %74 = stablehlo.slice %69 [0:1, 0:64, 0:7, 0:64] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %75 = stablehlo.concatenate %73, %74, dim = 3 : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
    %76 = stablehlo.broadcast_in_dim %57, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %77 = stablehlo.multiply %75, %76 : tensor<1x64x7x128xbf16>
    %78 = stablehlo.add %71, %77 : tensor<1x64x7x128xbf16>
    %79 = stablehlo.broadcast_in_dim %60, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %80 = stablehlo.reshape %79 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %81 = stablehlo.transpose %80, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
    %82 = stablehlo.dot_general %78, %81, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
    %83 = stablehlo.multiply %82, %5 : tensor<1x64x7x7xbf16>
    %84 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %85 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %86 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %87 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %88 = stablehlo.subtract %86, %87 : tensor<7x7xi64>
    %89 = stablehlo.compare  GE, %88, %4 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %90 = stablehlo.select %89, %3, %2 : tensor<7x7xi1>, tensor<7x7xbf16>
    %91 = stablehlo.compare  GT, %84, %85 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %92 = stablehlo.convert %91 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
    %93 = stablehlo.multiply %90, %92 : tensor<7x7xbf16>
    %94 = stablehlo.reshape %93 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
    %95 = stablehlo.reshape %arg10 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %96 = stablehlo.reshape %95 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
    %97 = stablehlo.convert %96 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
    %98 = stablehlo.reshape %97 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
    %99 = stablehlo.broadcast_in_dim %98, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
    %100 = stablehlo.add %94, %99 : tensor<1x1x7x7xbf16>
    %101 = stablehlo.compare  EQ, %100, %1 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
    %102 = stablehlo.select %101, %0, %94 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
    %103 = stablehlo.reshape %102 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0, 2, 3] : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
    %105 = stablehlo.add %83, %104 : tensor<1x64x7x7xbf16>
    %106 = stablehlo.convert %105 : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
    %107 = stablehlo.reduce(%106 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %108 = stablehlo.broadcast_in_dim %107, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %109 = stablehlo.subtract %106, %108 : tensor<1x64x7x7xf32>
    %110 = stablehlo.exponential %109 : tensor<1x64x7x7xf32>
    %111 = stablehlo.reduce(%110 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %112 = stablehlo.broadcast_in_dim %111, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %113 = stablehlo.divide %110, %112 : tensor<1x64x7x7xf32>
    %114 = stablehlo.convert %113 : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
    %115 = stablehlo.broadcast_in_dim %36, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %116 = stablehlo.reshape %115 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %117 = stablehlo.dot_general %114, %116, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %118 = stablehlo.transpose %117, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
    %120 = stablehlo.reshape %arg9 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %121 = stablehlo.reshape %120 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %122 = stablehlo.transpose %121, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %123 = stablehlo.dot_general %119, %122, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %124 = stablehlo.reshape %123 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %125 = stablehlo.add %16, %124 : tensor<1x7x8192xbf16>
    %126 = stablehlo.reshape %arg12 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %127 = stablehlo.reshape %126 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %128 = stablehlo.broadcast_in_dim %127, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %129 = stablehlo.convert %125 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %130 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %131 = stablehlo.power %129, %130 : tensor<1x7x8192xf32>
    %132 = stablehlo.reduce(%131 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %133 = stablehlo.multiply %132, %cst_2 : tensor<1x7xf32>
    %134 = stablehlo.reshape %133 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %135 = stablehlo.add %134, %cst_3 : tensor<1x7x1xf32>
    %136 = stablehlo.rsqrt %135 : tensor<1x7x1xf32>
    %137 = stablehlo.reshape %136 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %139 = stablehlo.multiply %129, %138 : tensor<1x7x8192xf32>
    %140 = stablehlo.convert %139 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %141 = stablehlo.multiply %128, %140 : tensor<1x7x8192xbf16>
    %142 = stablehlo.reshape %141 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %143 = stablehlo.reshape %arg13 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %144 = stablehlo.reshape %143 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %145 = stablehlo.transpose %144, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %146 = stablehlo.dot_general %142, %145, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %147 = stablehlo.reshape %146 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %148 = stablehlo.logistic %147 : tensor<1x7x28672xbf16>
    %149 = stablehlo.multiply %147, %148 : tensor<1x7x28672xbf16>
    %150 = stablehlo.reshape %arg8 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %152 = stablehlo.transpose %151, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %153 = stablehlo.dot_general %142, %152, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %154 = stablehlo.reshape %153 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %155 = stablehlo.multiply %149, %154 : tensor<1x7x28672xbf16>
    %156 = stablehlo.reshape %155 : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
    %157 = stablehlo.reshape %arg7 : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
    %158 = stablehlo.reshape %157 : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
    %159 = stablehlo.transpose %158, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
    %160 = stablehlo.dot_general %156, %159, contracting_dims = [1] x [0] : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
    %161 = stablehlo.reshape %160 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %162 = stablehlo.add %125, %161 : tensor<1x7x8192xbf16>
    %163 = stablehlo.convert %162 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %164 = stablehlo.power %163, %6 : tensor<1x7x8192xf32>
    %165 = stablehlo.reduce(%164 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %166 = stablehlo.multiply %165, %cst_2 : tensor<1x7xf32>
    %167 = stablehlo.reshape %166 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %168 = stablehlo.add %167, %cst_3 : tensor<1x7x1xf32>
    %169 = stablehlo.rsqrt %168 : tensor<1x7x1xf32>
    %170 = stablehlo.reshape %169 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %171 = stablehlo.broadcast_in_dim %170, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %172 = stablehlo.multiply %163, %171 : tensor<1x7x8192xf32>
    %173 = stablehlo.convert %172 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %174 = stablehlo.multiply %63, %173 : tensor<1x7x8192xbf16>
    %175 = stablehlo.reshape %174 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = stablehlo.reshape %arg6 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %177 = stablehlo.reshape %176 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %178 = stablehlo.transpose %177, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
    %179 = stablehlo.dot_general %175, %178, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
    %180 = stablehlo.reshape %179 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %36, %60, %180 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before ApplyShardingConstraintsPass (sdy-apply-sharding-constraints) ('builtin.module' operation: @SyncTensorsGraph.406) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8, "_axis_1"=4]>
  func.func @main(%arg0: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg1: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg2: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg3: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg4: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg5: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg6: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg7: tensor<8192x28672xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg8: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg9: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg10: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg11: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg12: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg13: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg14: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
    %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
    %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
    %c_6 = stablehlo.constant dense<1> : tensor<i64>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %4 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
    %5 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %7 = stablehlo.reshape %arg3 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %8 = stablehlo.reshape %7 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %9 = stablehlo.broadcast_in_dim %8, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %10 = stablehlo.reshape %arg2 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %12 = stablehlo.reshape %arg1 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %13 = stablehlo.reshape %12 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %14 = stablehlo.convert %13 : (tensor<7xi64>) -> tensor<7xui32>
    %15 = "stablehlo.gather"(%11, %14) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
    %16 = stablehlo.reshape %15 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %17 = stablehlo.convert %16 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %18 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %19 = stablehlo.power %17, %18 : tensor<1x7x8192xf32>
    %20 = stablehlo.reduce(%19 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %21 = stablehlo.multiply %20, %cst_2 : tensor<1x7xf32>
    %22 = stablehlo.reshape %21 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %23 = stablehlo.add %22, %cst_3 : tensor<1x7x1xf32>
    %24 = stablehlo.rsqrt %23 : tensor<1x7x1xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %26 = stablehlo.broadcast_in_dim %25, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %27 = stablehlo.multiply %17, %26 : tensor<1x7x8192xf32>
    %28 = stablehlo.convert %27 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %29 = stablehlo.multiply %9, %28 : tensor<1x7x8192xbf16>
    %30 = stablehlo.reshape %29 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %31 = stablehlo.reshape %arg0 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %32 = stablehlo.reshape %31 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %33 = stablehlo.transpose %32, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %34 = stablehlo.dot_general %30, %33, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %35 = stablehlo.reshape %34 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %36 = stablehlo.transpose %35, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %37 = stablehlo.reshape %arg5 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %38 = stablehlo.reshape %37 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %40 = stablehlo.dot_general %30, %39, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.reshape %arg4 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %45 = stablehlo.dot_general %44, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %46 = stablehlo.transpose %45, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %47 = stablehlo.concatenate %46, %46, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %48 = stablehlo.cosine %47 : tensor<1x7x128xf32>
    %49 = stablehlo.convert %48 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %50 = stablehlo.broadcast_in_dim %49, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %51 = stablehlo.multiply %42, %50 : tensor<1x8x7x128xbf16>
    %52 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %53 = stablehlo.negate %52 : tensor<1x8x7x64xbf16>
    %54 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %55 = stablehlo.concatenate %53, %54, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %56 = stablehlo.sine %47 : tensor<1x7x128xf32>
    %57 = stablehlo.convert %56 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %58 = stablehlo.broadcast_in_dim %57, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %59 = stablehlo.multiply %55, %58 : tensor<1x8x7x128xbf16>
    %60 = stablehlo.add %51, %59 : tensor<1x8x7x128xbf16>
    %61 = stablehlo.reshape %arg14 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %63 = stablehlo.broadcast_in_dim %62, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %64 = stablehlo.reshape %arg11 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %66 = stablehlo.transpose %65, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %67 = stablehlo.dot_general %30, %66, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %68 = stablehlo.reshape %67 : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
    %70 = stablehlo.broadcast_in_dim %49, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %71 = stablehlo.multiply %69, %70 : tensor<1x64x7x128xbf16>
    %72 = stablehlo.slice %69 [0:1, 0:64, 0:7, 64:128] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %73 = stablehlo.negate %72 : tensor<1x64x7x64xbf16>
    %74 = stablehlo.slice %69 [0:1, 0:64, 0:7, 0:64] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %75 = stablehlo.concatenate %73, %74, dim = 3 : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
    %76 = stablehlo.broadcast_in_dim %57, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %77 = stablehlo.multiply %75, %76 : tensor<1x64x7x128xbf16>
    %78 = stablehlo.add %71, %77 : tensor<1x64x7x128xbf16>
    %79 = stablehlo.broadcast_in_dim %60, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %80 = stablehlo.reshape %79 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %81 = stablehlo.transpose %80, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
    %82 = stablehlo.dot_general %78, %81, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
    %83 = stablehlo.multiply %82, %5 : tensor<1x64x7x7xbf16>
    %84 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %85 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %86 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %87 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %88 = stablehlo.subtract %86, %87 : tensor<7x7xi64>
    %89 = stablehlo.compare  GE, %88, %4 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %90 = stablehlo.select %89, %3, %2 : tensor<7x7xi1>, tensor<7x7xbf16>
    %91 = stablehlo.compare  GT, %84, %85 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %92 = stablehlo.convert %91 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
    %93 = stablehlo.multiply %90, %92 : tensor<7x7xbf16>
    %94 = stablehlo.reshape %93 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
    %95 = stablehlo.reshape %arg10 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %96 = stablehlo.reshape %95 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
    %97 = stablehlo.convert %96 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
    %98 = stablehlo.reshape %97 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
    %99 = stablehlo.broadcast_in_dim %98, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
    %100 = stablehlo.add %94, %99 : tensor<1x1x7x7xbf16>
    %101 = stablehlo.compare  EQ, %100, %1 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
    %102 = stablehlo.select %101, %0, %94 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
    %103 = stablehlo.reshape %102 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0, 2, 3] : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
    %105 = stablehlo.add %83, %104 : tensor<1x64x7x7xbf16>
    %106 = stablehlo.convert %105 : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
    %107 = stablehlo.reduce(%106 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %108 = stablehlo.broadcast_in_dim %107, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %109 = stablehlo.subtract %106, %108 : tensor<1x64x7x7xf32>
    %110 = stablehlo.exponential %109 : tensor<1x64x7x7xf32>
    %111 = stablehlo.reduce(%110 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %112 = stablehlo.broadcast_in_dim %111, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %113 = stablehlo.divide %110, %112 : tensor<1x64x7x7xf32>
    %114 = stablehlo.convert %113 : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
    %115 = stablehlo.broadcast_in_dim %36, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %116 = stablehlo.reshape %115 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %117 = stablehlo.dot_general %114, %116, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %118 = stablehlo.transpose %117, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
    %120 = stablehlo.reshape %arg9 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %121 = stablehlo.reshape %120 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %122 = stablehlo.transpose %121, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %123 = stablehlo.dot_general %119, %122, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %124 = stablehlo.reshape %123 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %125 = stablehlo.add %16, %124 : tensor<1x7x8192xbf16>
    %126 = stablehlo.reshape %arg12 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %127 = stablehlo.reshape %126 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %128 = stablehlo.broadcast_in_dim %127, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %129 = stablehlo.convert %125 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %130 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %131 = stablehlo.power %129, %130 : tensor<1x7x8192xf32>
    %132 = stablehlo.reduce(%131 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %133 = stablehlo.multiply %132, %cst_2 : tensor<1x7xf32>
    %134 = stablehlo.reshape %133 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %135 = stablehlo.add %134, %cst_3 : tensor<1x7x1xf32>
    %136 = stablehlo.rsqrt %135 : tensor<1x7x1xf32>
    %137 = stablehlo.reshape %136 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %139 = stablehlo.multiply %129, %138 : tensor<1x7x8192xf32>
    %140 = stablehlo.convert %139 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %141 = stablehlo.multiply %128, %140 : tensor<1x7x8192xbf16>
    %142 = stablehlo.reshape %141 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %143 = stablehlo.reshape %arg13 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %144 = stablehlo.reshape %143 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %145 = stablehlo.transpose %144, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %146 = stablehlo.dot_general %142, %145, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %147 = stablehlo.reshape %146 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %148 = stablehlo.logistic %147 : tensor<1x7x28672xbf16>
    %149 = stablehlo.multiply %147, %148 : tensor<1x7x28672xbf16>
    %150 = stablehlo.reshape %arg8 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %152 = stablehlo.transpose %151, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %153 = stablehlo.dot_general %142, %152, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %154 = stablehlo.reshape %153 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %155 = stablehlo.multiply %149, %154 : tensor<1x7x28672xbf16>
    %156 = stablehlo.reshape %155 : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
    %157 = stablehlo.reshape %arg7 : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
    %158 = stablehlo.reshape %157 : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
    %159 = stablehlo.transpose %158, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
    %160 = stablehlo.dot_general %156, %159, contracting_dims = [1] x [0] : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
    %161 = stablehlo.reshape %160 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %162 = stablehlo.add %125, %161 : tensor<1x7x8192xbf16>
    %163 = stablehlo.convert %162 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %164 = stablehlo.power %163, %6 : tensor<1x7x8192xf32>
    %165 = stablehlo.reduce(%164 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %166 = stablehlo.multiply %165, %cst_2 : tensor<1x7xf32>
    %167 = stablehlo.reshape %166 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %168 = stablehlo.add %167, %cst_3 : tensor<1x7x1xf32>
    %169 = stablehlo.rsqrt %168 : tensor<1x7x1xf32>
    %170 = stablehlo.reshape %169 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %171 = stablehlo.broadcast_in_dim %170, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %172 = stablehlo.multiply %163, %171 : tensor<1x7x8192xf32>
    %173 = stablehlo.convert %172 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %174 = stablehlo.multiply %63, %173 : tensor<1x7x8192xbf16>
    %175 = stablehlo.reshape %174 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = stablehlo.reshape %arg6 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %177 = stablehlo.reshape %176 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %178 = stablehlo.transpose %177, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
    %179 = stablehlo.dot_general %175, %178, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
    %180 = stablehlo.reshape %179 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %36, %60, %180 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @SyncTensorsGraph.406) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8, "_axis_1"=4]>
  func.func @main(%arg0: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg1: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg2: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg3: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg4: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg5: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg6: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg7: tensor<8192x28672xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg8: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg9: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg10: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg11: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg12: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg13: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg14: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
    %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
    %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
    %c_6 = stablehlo.constant dense<1> : tensor<i64>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %4 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
    %5 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %7 = stablehlo.reshape %arg3 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %8 = stablehlo.reshape %7 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %9 = stablehlo.broadcast_in_dim %8, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %10 = stablehlo.reshape %arg2 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %12 = stablehlo.reshape %arg1 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %13 = stablehlo.reshape %12 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %14 = stablehlo.convert %13 : (tensor<7xi64>) -> tensor<7xui32>
    %15 = "stablehlo.gather"(%11, %14) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
    %16 = stablehlo.reshape %15 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %17 = stablehlo.convert %16 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %18 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %19 = stablehlo.power %17, %18 : tensor<1x7x8192xf32>
    %20 = stablehlo.reduce(%19 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %21 = stablehlo.multiply %20, %cst_2 : tensor<1x7xf32>
    %22 = stablehlo.reshape %21 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %23 = stablehlo.add %22, %cst_3 : tensor<1x7x1xf32>
    %24 = stablehlo.rsqrt %23 : tensor<1x7x1xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %26 = stablehlo.broadcast_in_dim %25, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %27 = stablehlo.multiply %17, %26 : tensor<1x7x8192xf32>
    %28 = stablehlo.convert %27 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %29 = stablehlo.multiply %9, %28 : tensor<1x7x8192xbf16>
    %30 = stablehlo.reshape %29 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %31 = stablehlo.reshape %arg0 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %32 = stablehlo.reshape %31 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %33 = stablehlo.transpose %32, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %34 = stablehlo.dot_general %30, %33, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %35 = stablehlo.reshape %34 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %36 = stablehlo.transpose %35, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %37 = stablehlo.reshape %arg5 : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %38 = stablehlo.reshape %37 : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %40 = stablehlo.dot_general %30, %39, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.reshape %arg4 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %45 = stablehlo.dot_general %44, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %46 = stablehlo.transpose %45, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %47 = stablehlo.concatenate %46, %46, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %48 = stablehlo.cosine %47 : tensor<1x7x128xf32>
    %49 = stablehlo.convert %48 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %50 = stablehlo.broadcast_in_dim %49, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %51 = stablehlo.multiply %42, %50 : tensor<1x8x7x128xbf16>
    %52 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %53 = stablehlo.negate %52 : tensor<1x8x7x64xbf16>
    %54 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %55 = stablehlo.concatenate %53, %54, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %56 = stablehlo.sine %47 : tensor<1x7x128xf32>
    %57 = stablehlo.convert %56 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %58 = stablehlo.broadcast_in_dim %57, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %59 = stablehlo.multiply %55, %58 : tensor<1x8x7x128xbf16>
    %60 = stablehlo.add %51, %59 : tensor<1x8x7x128xbf16>
    %61 = stablehlo.reshape %arg14 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %63 = stablehlo.broadcast_in_dim %62, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %64 = stablehlo.reshape %arg11 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %65 = stablehlo.reshape %64 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %66 = stablehlo.transpose %65, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %67 = stablehlo.dot_general %30, %66, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %68 = stablehlo.reshape %67 : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
    %70 = stablehlo.broadcast_in_dim %49, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %71 = stablehlo.multiply %69, %70 : tensor<1x64x7x128xbf16>
    %72 = stablehlo.slice %69 [0:1, 0:64, 0:7, 64:128] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %73 = stablehlo.negate %72 : tensor<1x64x7x64xbf16>
    %74 = stablehlo.slice %69 [0:1, 0:64, 0:7, 0:64] : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %75 = stablehlo.concatenate %73, %74, dim = 3 : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
    %76 = stablehlo.broadcast_in_dim %57, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %77 = stablehlo.multiply %75, %76 : tensor<1x64x7x128xbf16>
    %78 = stablehlo.add %71, %77 : tensor<1x64x7x128xbf16>
    %79 = stablehlo.broadcast_in_dim %60, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %80 = stablehlo.reshape %79 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %81 = stablehlo.transpose %80, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
    %82 = stablehlo.dot_general %78, %81, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
    %83 = stablehlo.multiply %82, %5 : tensor<1x64x7x7xbf16>
    %84 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %85 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %86 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %87 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %88 = stablehlo.subtract %86, %87 : tensor<7x7xi64>
    %89 = stablehlo.compare  GE, %88, %4 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %90 = stablehlo.select %89, %3, %2 : tensor<7x7xi1>, tensor<7x7xbf16>
    %91 = stablehlo.compare  GT, %84, %85 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %92 = stablehlo.convert %91 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
    %93 = stablehlo.multiply %90, %92 : tensor<7x7xbf16>
    %94 = stablehlo.reshape %93 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
    %95 = stablehlo.reshape %arg10 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %96 = stablehlo.reshape %95 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
    %97 = stablehlo.convert %96 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
    %98 = stablehlo.reshape %97 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
    %99 = stablehlo.broadcast_in_dim %98, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
    %100 = stablehlo.add %94, %99 : tensor<1x1x7x7xbf16>
    %101 = stablehlo.compare  EQ, %100, %1 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
    %102 = stablehlo.select %101, %0, %94 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
    %103 = stablehlo.reshape %102 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0, 2, 3] : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
    %105 = stablehlo.add %83, %104 : tensor<1x64x7x7xbf16>
    %106 = stablehlo.convert %105 : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
    %107 = stablehlo.reduce(%106 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %108 = stablehlo.broadcast_in_dim %107, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %109 = stablehlo.subtract %106, %108 : tensor<1x64x7x7xf32>
    %110 = stablehlo.exponential %109 : tensor<1x64x7x7xf32>
    %111 = stablehlo.reduce(%110 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %112 = stablehlo.broadcast_in_dim %111, dims = [0, 1, 2] : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %113 = stablehlo.divide %110, %112 : tensor<1x64x7x7xf32>
    %114 = stablehlo.convert %113 : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
    %115 = stablehlo.broadcast_in_dim %36, dims = [0, 1, 3, 4] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %116 = stablehlo.reshape %115 : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %117 = stablehlo.dot_general %114, %116, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %118 = stablehlo.transpose %117, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
    %120 = stablehlo.reshape %arg9 : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %121 = stablehlo.reshape %120 : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %122 = stablehlo.transpose %121, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %123 = stablehlo.dot_general %119, %122, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %124 = stablehlo.reshape %123 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %125 = stablehlo.add %16, %124 : tensor<1x7x8192xbf16>
    %126 = stablehlo.reshape %arg12 : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %127 = stablehlo.reshape %126 : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %128 = stablehlo.broadcast_in_dim %127, dims = [2] : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %129 = stablehlo.convert %125 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %130 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1x7x8192xf32>
    %131 = stablehlo.power %129, %130 : tensor<1x7x8192xf32>
    %132 = stablehlo.reduce(%131 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %133 = stablehlo.multiply %132, %cst_2 : tensor<1x7xf32>
    %134 = stablehlo.reshape %133 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %135 = stablehlo.add %134, %cst_3 : tensor<1x7x1xf32>
    %136 = stablehlo.rsqrt %135 : tensor<1x7x1xf32>
    %137 = stablehlo.reshape %136 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %139 = stablehlo.multiply %129, %138 : tensor<1x7x8192xf32>
    %140 = stablehlo.convert %139 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %141 = stablehlo.multiply %128, %140 : tensor<1x7x8192xbf16>
    %142 = stablehlo.reshape %141 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %143 = stablehlo.reshape %arg13 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %144 = stablehlo.reshape %143 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %145 = stablehlo.transpose %144, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %146 = stablehlo.dot_general %142, %145, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %147 = stablehlo.reshape %146 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %148 = stablehlo.logistic %147 : tensor<1x7x28672xbf16>
    %149 = stablehlo.multiply %147, %148 : tensor<1x7x28672xbf16>
    %150 = stablehlo.reshape %arg8 : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %152 = stablehlo.transpose %151, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %153 = stablehlo.dot_general %142, %152, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %154 = stablehlo.reshape %153 : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %155 = stablehlo.multiply %149, %154 : tensor<1x7x28672xbf16>
    %156 = stablehlo.reshape %155 : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
    %157 = stablehlo.reshape %arg7 : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
    %158 = stablehlo.reshape %157 : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
    %159 = stablehlo.transpose %158, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
    %160 = stablehlo.dot_general %156, %159, contracting_dims = [1] x [0] : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
    %161 = stablehlo.reshape %160 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %162 = stablehlo.add %125, %161 : tensor<1x7x8192xbf16>
    %163 = stablehlo.convert %162 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %164 = stablehlo.power %163, %6 : tensor<1x7x8192xf32>
    %165 = stablehlo.reduce(%164 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %166 = stablehlo.multiply %165, %cst_2 : tensor<1x7xf32>
    %167 = stablehlo.reshape %166 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %168 = stablehlo.add %167, %cst_3 : tensor<1x7x1xf32>
    %169 = stablehlo.rsqrt %168 : tensor<1x7x1xf32>
    %170 = stablehlo.reshape %169 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %171 = stablehlo.broadcast_in_dim %170, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %172 = stablehlo.multiply %163, %171 : tensor<1x7x8192xf32>
    %173 = stablehlo.convert %172 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %174 = stablehlo.multiply %63, %173 : tensor<1x7x8192xbf16>
    %175 = stablehlo.reshape %174 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = stablehlo.reshape %arg6 : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %177 = stablehlo.reshape %176 : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %178 = stablehlo.transpose %177, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
    %179 = stablehlo.dot_general %175, %178, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
    %180 = stablehlo.reshape %179 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %36, %60, %180 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump After AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @SyncTensorsGraph.406) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8, "_axis_1"=4]>
  func.func @main(%arg0: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg1: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg2: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg3: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg4: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg5: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg6: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg7: tensor<8192x28672xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg8: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg9: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg10: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg11: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg12: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg13: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg14: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x7x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x7x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
    %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
    %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
    %c_6 = stablehlo.constant dense<1> : tensor<i64>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %4 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
    %5 = stablehlo.broadcast_in_dim %cst_5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
    %7 = stablehlo.reshape %arg3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %8 = stablehlo.reshape %7 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %9 = stablehlo.broadcast_in_dim %8, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %10 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %11 = stablehlo.reshape %10 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %12 = stablehlo.reshape %arg1 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %13 = stablehlo.reshape %12 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %14 = stablehlo.convert %13 : (tensor<7xi64>) -> tensor<7xui32>
    %15 = "stablehlo.gather"(%11, %14) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
    %16 = stablehlo.reshape %15 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %17 = stablehlo.convert %16 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %18 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
    %19 = stablehlo.power %17, %18 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %20 = stablehlo.reduce(%19 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %21 = stablehlo.multiply %20, %cst_2 : tensor<1x7xf32>
    %22 = stablehlo.reshape %21 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %23 = stablehlo.add %22, %cst_3 : tensor<1x7x1xf32>
    %24 = stablehlo.rsqrt %23 : tensor<1x7x1xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %26 = stablehlo.broadcast_in_dim %25, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %27 = stablehlo.multiply %17, %26 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %28 = stablehlo.convert %27 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %29 = stablehlo.multiply %9, %28 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xbf16>
    %30 = stablehlo.reshape %29 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %31 = stablehlo.reshape %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %32 = stablehlo.reshape %31 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %33 = stablehlo.transpose %32, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %34 = stablehlo.dot_general %30, %33, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %35 = stablehlo.reshape %34 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %36 = stablehlo.transpose %35, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %37 = stablehlo.reshape %arg5 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %38 = stablehlo.reshape %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %40 = stablehlo.dot_general %30, %39, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.reshape %arg4 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %45 = stablehlo.dot_general %44, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %46 = stablehlo.transpose %45, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %47 = stablehlo.concatenate %46, %46, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %48 = stablehlo.cosine %47 : tensor<1x7x128xf32>
    %49 = stablehlo.convert %48 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %50 = stablehlo.broadcast_in_dim %49, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %51 = stablehlo.multiply %42, %50 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %52 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %53 = stablehlo.negate %52 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x64xbf16>
    %54 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %55 = stablehlo.concatenate %53, %54, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %56 = stablehlo.sine %47 : tensor<1x7x128xf32>
    %57 = stablehlo.convert %56 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %58 = stablehlo.broadcast_in_dim %57, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %59 = stablehlo.multiply %55, %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %60 = stablehlo.add %51, %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %61 = stablehlo.reshape %arg14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %62 = stablehlo.reshape %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %63 = stablehlo.broadcast_in_dim %62, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %64 = stablehlo.reshape %arg11 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %65 = stablehlo.reshape %64 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %66 = stablehlo.transpose %65, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %67 = stablehlo.dot_general %30, %66, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %68 = stablehlo.reshape %67 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
    %70 = stablehlo.broadcast_in_dim %49, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %71 = stablehlo.multiply %69, %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
    %72 = stablehlo.slice %69 [0:1, 0:64, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %73 = stablehlo.negate %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x64xbf16>
    %74 = stablehlo.slice %69 [0:1, 0:64, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %75 = stablehlo.concatenate %73, %74, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
    %76 = stablehlo.broadcast_in_dim %57, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %77 = stablehlo.multiply %75, %76 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
    %78 = stablehlo.add %71, %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
    %79 = stablehlo.broadcast_in_dim %60, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %80 = stablehlo.reshape %79 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %81 = stablehlo.transpose %80, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
    %82 = stablehlo.dot_general %78, %81, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
    %83 = stablehlo.multiply %82, %5 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xbf16>
    %84 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %85 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %86 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %87 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %88 = stablehlo.subtract %86, %87 : tensor<7x7xi64>
    %89 = stablehlo.compare  GE, %88, %4 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %90 = stablehlo.select %89, %3, %2 : tensor<7x7xi1>, tensor<7x7xbf16>
    %91 = stablehlo.compare  GT, %84, %85 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %92 = stablehlo.convert %91 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
    %93 = stablehlo.multiply %90, %92 : tensor<7x7xbf16>
    %94 = stablehlo.reshape %93 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
    %95 = stablehlo.reshape %arg10 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %96 = stablehlo.reshape %95 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
    %97 = stablehlo.convert %96 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
    %98 = stablehlo.reshape %97 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
    %99 = stablehlo.broadcast_in_dim %98, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
    %100 = stablehlo.add %94, %99 : tensor<1x1x7x7xbf16>
    %101 = stablehlo.compare  EQ, %100, %1 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
    %102 = stablehlo.select %101, %0, %94 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
    %103 = stablehlo.reshape %102 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
    %105 = stablehlo.add %83, %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xbf16>
    %106 = stablehlo.convert %105 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
    %107 = stablehlo.reduce(%106 init: %cst_0) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %108 = stablehlo.broadcast_in_dim %107, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %109 = stablehlo.subtract %106, %108 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
    %110 = stablehlo.exponential %109 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
    %111 = stablehlo.reduce(%110 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %112 = stablehlo.broadcast_in_dim %111, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %113 = stablehlo.divide %110, %112 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
    %114 = stablehlo.convert %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
    %115 = stablehlo.broadcast_in_dim %36, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %116 = stablehlo.reshape %115 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %117 = stablehlo.dot_general %114, %116, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %118 = stablehlo.transpose %117, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
    %119 = stablehlo.reshape %118 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
    %120 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %121 = stablehlo.reshape %120 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %122 = stablehlo.transpose %121, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %123 = stablehlo.dot_general %119, %122, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %124 = stablehlo.reshape %123 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %125 = stablehlo.add %16, %124 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
    %126 = stablehlo.reshape %arg12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %127 = stablehlo.reshape %126 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %128 = stablehlo.broadcast_in_dim %127, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %129 = stablehlo.convert %125 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %130 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
    %131 = stablehlo.power %129, %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
    %132 = stablehlo.reduce(%131 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %133 = stablehlo.multiply %132, %cst_2 : tensor<1x7xf32>
    %134 = stablehlo.reshape %133 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %135 = stablehlo.add %134, %cst_3 : tensor<1x7x1xf32>
    %136 = stablehlo.rsqrt %135 : tensor<1x7x1xf32>
    %137 = stablehlo.reshape %136 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %139 = stablehlo.multiply %129, %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
    %140 = stablehlo.convert %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %141 = stablehlo.multiply %128, %140 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
    %142 = stablehlo.reshape %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %143 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %145 = stablehlo.transpose %144, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %146 = stablehlo.dot_general %142, %145, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %147 = stablehlo.reshape %146 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %148 = stablehlo.logistic %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
    %149 = stablehlo.multiply %147, %148 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
    %150 = stablehlo.reshape %arg8 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %151 = stablehlo.reshape %150 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %152 = stablehlo.transpose %151, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %153 = stablehlo.dot_general %142, %152, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %154 = stablehlo.reshape %153 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %155 = stablehlo.multiply %149, %154 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
    %156 = stablehlo.reshape %155 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
    %157 = stablehlo.reshape %arg7 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
    %158 = stablehlo.reshape %157 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
    %159 = stablehlo.transpose %158, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
    %160 = stablehlo.dot_general %156, %159, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
    %161 = stablehlo.reshape %160 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %162 = stablehlo.add %125, %161 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
    %163 = stablehlo.convert %162 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %164 = stablehlo.power %163, %6 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
    %165 = stablehlo.reduce(%164 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %166 = stablehlo.multiply %165, %cst_2 : tensor<1x7xf32>
    %167 = stablehlo.reshape %166 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %168 = stablehlo.add %167, %cst_3 : tensor<1x7x1xf32>
    %169 = stablehlo.rsqrt %168 : tensor<1x7x1xf32>
    %170 = stablehlo.reshape %169 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %171 = stablehlo.broadcast_in_dim %170, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %172 = stablehlo.multiply %163, %171 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
    %173 = stablehlo.convert %172 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %174 = stablehlo.multiply %63, %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
    %175 = stablehlo.reshape %174 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = stablehlo.reshape %arg6 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %177 = stablehlo.reshape %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %178 = stablehlo.transpose %177, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
    %179 = stablehlo.dot_general %175, %178, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
    %180 = stablehlo.reshape %179 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %36, %60, %180 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before ShardingConstraintToReshardPass (sdy-sharding-constraint-to-reshard) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8, "_axis_1"=4]>
  func.func @main(%arg0: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg1: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg2: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg3: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg4: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg5: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg6: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg7: tensor<8192x28672xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg8: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg9: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg10: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg11: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg12: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg13: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg14: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x7x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x7x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
    %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
    %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
    %c_6 = stablehlo.constant dense<1> : tensor<i64>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %4 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
    %5 = stablehlo.broadcast_in_dim %cst_5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
    %7 = stablehlo.reshape %arg3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %8 = stablehlo.reshape %7 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %9 = stablehlo.broadcast_in_dim %8, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %10 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %11 = stablehlo.reshape %10 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %12 = stablehlo.reshape %arg1 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %13 = stablehlo.reshape %12 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %14 = stablehlo.convert %13 : (tensor<7xi64>) -> tensor<7xui32>
    %15 = "stablehlo.gather"(%11, %14) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
    %16 = stablehlo.reshape %15 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %17 = stablehlo.convert %16 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %18 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
    %19 = stablehlo.power %17, %18 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %20 = stablehlo.reduce(%19 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %21 = stablehlo.multiply %20, %cst_2 : tensor<1x7xf32>
    %22 = stablehlo.reshape %21 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %23 = stablehlo.add %22, %cst_3 : tensor<1x7x1xf32>
    %24 = stablehlo.rsqrt %23 : tensor<1x7x1xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %26 = stablehlo.broadcast_in_dim %25, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %27 = stablehlo.multiply %17, %26 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %28 = stablehlo.convert %27 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %29 = stablehlo.multiply %9, %28 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xbf16>
    %30 = stablehlo.reshape %29 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %31 = stablehlo.reshape %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %32 = stablehlo.reshape %31 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %33 = stablehlo.transpose %32, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %34 = stablehlo.dot_general %30, %33, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %35 = stablehlo.reshape %34 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %36 = stablehlo.transpose %35, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %37 = stablehlo.reshape %arg5 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %38 = stablehlo.reshape %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %40 = stablehlo.dot_general %30, %39, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.reshape %arg4 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %45 = stablehlo.dot_general %44, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %46 = stablehlo.transpose %45, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %47 = stablehlo.concatenate %46, %46, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %48 = stablehlo.cosine %47 : tensor<1x7x128xf32>
    %49 = stablehlo.convert %48 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %50 = stablehlo.broadcast_in_dim %49, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %51 = stablehlo.multiply %42, %50 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %52 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %53 = stablehlo.negate %52 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x64xbf16>
    %54 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %55 = stablehlo.concatenate %53, %54, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %56 = stablehlo.sine %47 : tensor<1x7x128xf32>
    %57 = stablehlo.convert %56 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %58 = stablehlo.broadcast_in_dim %57, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %59 = stablehlo.multiply %55, %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %60 = stablehlo.add %51, %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %61 = stablehlo.reshape %arg14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %62 = stablehlo.reshape %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %63 = stablehlo.broadcast_in_dim %62, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %64 = stablehlo.reshape %arg11 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %65 = stablehlo.reshape %64 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %66 = stablehlo.transpose %65, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %67 = stablehlo.dot_general %30, %66, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %68 = stablehlo.reshape %67 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
    %70 = stablehlo.broadcast_in_dim %49, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %71 = stablehlo.multiply %69, %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
    %72 = stablehlo.slice %69 [0:1, 0:64, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %73 = stablehlo.negate %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x64xbf16>
    %74 = stablehlo.slice %69 [0:1, 0:64, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %75 = stablehlo.concatenate %73, %74, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
    %76 = stablehlo.broadcast_in_dim %57, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %77 = stablehlo.multiply %75, %76 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
    %78 = stablehlo.add %71, %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
    %79 = stablehlo.broadcast_in_dim %60, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %80 = stablehlo.reshape %79 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %81 = stablehlo.transpose %80, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
    %82 = stablehlo.dot_general %78, %81, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
    %83 = stablehlo.multiply %82, %5 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xbf16>
    %84 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %85 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %86 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %87 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %88 = stablehlo.subtract %86, %87 : tensor<7x7xi64>
    %89 = stablehlo.compare  GE, %88, %4 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %90 = stablehlo.select %89, %3, %2 : tensor<7x7xi1>, tensor<7x7xbf16>
    %91 = stablehlo.compare  GT, %84, %85 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %92 = stablehlo.convert %91 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
    %93 = stablehlo.multiply %90, %92 : tensor<7x7xbf16>
    %94 = stablehlo.reshape %93 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
    %95 = stablehlo.reshape %arg10 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %96 = stablehlo.reshape %95 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
    %97 = stablehlo.convert %96 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
    %98 = stablehlo.reshape %97 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
    %99 = stablehlo.broadcast_in_dim %98, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
    %100 = stablehlo.add %94, %99 : tensor<1x1x7x7xbf16>
    %101 = stablehlo.compare  EQ, %100, %1 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
    %102 = stablehlo.select %101, %0, %94 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
    %103 = stablehlo.reshape %102 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
    %105 = stablehlo.add %83, %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xbf16>
    %106 = stablehlo.convert %105 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
    %107 = stablehlo.reduce(%106 init: %cst_0) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %108 = stablehlo.broadcast_in_dim %107, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %109 = stablehlo.subtract %106, %108 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
    %110 = stablehlo.exponential %109 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
    %111 = stablehlo.reduce(%110 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %112 = stablehlo.broadcast_in_dim %111, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %113 = stablehlo.divide %110, %112 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
    %114 = stablehlo.convert %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
    %115 = stablehlo.broadcast_in_dim %36, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %116 = stablehlo.reshape %115 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %117 = stablehlo.dot_general %114, %116, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %118 = stablehlo.transpose %117, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
    %119 = stablehlo.reshape %118 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
    %120 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %121 = stablehlo.reshape %120 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %122 = stablehlo.transpose %121, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %123 = stablehlo.dot_general %119, %122, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %124 = stablehlo.reshape %123 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %125 = stablehlo.add %16, %124 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
    %126 = stablehlo.reshape %arg12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %127 = stablehlo.reshape %126 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %128 = stablehlo.broadcast_in_dim %127, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %129 = stablehlo.convert %125 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %130 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
    %131 = stablehlo.power %129, %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
    %132 = stablehlo.reduce(%131 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %133 = stablehlo.multiply %132, %cst_2 : tensor<1x7xf32>
    %134 = stablehlo.reshape %133 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %135 = stablehlo.add %134, %cst_3 : tensor<1x7x1xf32>
    %136 = stablehlo.rsqrt %135 : tensor<1x7x1xf32>
    %137 = stablehlo.reshape %136 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %139 = stablehlo.multiply %129, %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
    %140 = stablehlo.convert %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %141 = stablehlo.multiply %128, %140 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
    %142 = stablehlo.reshape %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %143 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %145 = stablehlo.transpose %144, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %146 = stablehlo.dot_general %142, %145, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %147 = stablehlo.reshape %146 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %148 = stablehlo.logistic %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
    %149 = stablehlo.multiply %147, %148 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
    %150 = stablehlo.reshape %arg8 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %151 = stablehlo.reshape %150 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %152 = stablehlo.transpose %151, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %153 = stablehlo.dot_general %142, %152, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %154 = stablehlo.reshape %153 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %155 = stablehlo.multiply %149, %154 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
    %156 = stablehlo.reshape %155 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
    %157 = stablehlo.reshape %arg7 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
    %158 = stablehlo.reshape %157 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
    %159 = stablehlo.transpose %158, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
    %160 = stablehlo.dot_general %156, %159, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
    %161 = stablehlo.reshape %160 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %162 = stablehlo.add %125, %161 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
    %163 = stablehlo.convert %162 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %164 = stablehlo.power %163, %6 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
    %165 = stablehlo.reduce(%164 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %166 = stablehlo.multiply %165, %cst_2 : tensor<1x7xf32>
    %167 = stablehlo.reshape %166 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %168 = stablehlo.add %167, %cst_3 : tensor<1x7x1xf32>
    %169 = stablehlo.rsqrt %168 : tensor<1x7x1xf32>
    %170 = stablehlo.reshape %169 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %171 = stablehlo.broadcast_in_dim %170, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %172 = stablehlo.multiply %163, %171 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
    %173 = stablehlo.convert %172 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %174 = stablehlo.multiply %63, %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
    %175 = stablehlo.reshape %174 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = stablehlo.reshape %arg6 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %177 = stablehlo.reshape %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %178 = stablehlo.transpose %177, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
    %179 = stablehlo.dot_general %175, %178, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
    %180 = stablehlo.reshape %179 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %36, %60, %180 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before InsertExplicitReshardsPass (insert-explicit-reshards) ('builtin.module' operation: @SyncTensorsGraph.406) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8, "_axis_1"=4]>
  func.func @main(%arg0: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg1: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg2: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg3: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg4: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg5: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg6: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg7: tensor<8192x28672xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg8: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg9: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg10: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg11: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg12: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg13: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg14: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x7x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x7x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
    %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
    %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
    %c_6 = stablehlo.constant dense<1> : tensor<i64>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %4 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
    %5 = stablehlo.broadcast_in_dim %cst_5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
    %7 = stablehlo.reshape %arg3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %8 = stablehlo.reshape %7 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %9 = stablehlo.broadcast_in_dim %8, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %10 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %11 = stablehlo.reshape %10 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %12 = stablehlo.reshape %arg1 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %13 = stablehlo.reshape %12 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %14 = stablehlo.convert %13 : (tensor<7xi64>) -> tensor<7xui32>
    %15 = "stablehlo.gather"(%11, %14) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
    %16 = stablehlo.reshape %15 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %17 = stablehlo.convert %16 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %18 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
    %19 = stablehlo.power %17, %18 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %20 = stablehlo.reduce(%19 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %21 = stablehlo.multiply %20, %cst_2 : tensor<1x7xf32>
    %22 = stablehlo.reshape %21 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %23 = stablehlo.add %22, %cst_3 : tensor<1x7x1xf32>
    %24 = stablehlo.rsqrt %23 : tensor<1x7x1xf32>
    %25 = stablehlo.reshape %24 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %26 = stablehlo.broadcast_in_dim %25, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %27 = stablehlo.multiply %17, %26 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %28 = stablehlo.convert %27 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %29 = stablehlo.multiply %9, %28 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xbf16>
    %30 = stablehlo.reshape %29 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %31 = stablehlo.reshape %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %32 = stablehlo.reshape %31 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %33 = stablehlo.transpose %32, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %34 = stablehlo.dot_general %30, %33, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %35 = stablehlo.reshape %34 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %36 = stablehlo.transpose %35, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %37 = stablehlo.reshape %arg5 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %38 = stablehlo.reshape %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %39 = stablehlo.transpose %38, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %40 = stablehlo.dot_general %30, %39, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.reshape %arg4 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %44 = stablehlo.reshape %43 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %45 = stablehlo.dot_general %44, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %46 = stablehlo.transpose %45, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %47 = stablehlo.concatenate %46, %46, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %48 = stablehlo.cosine %47 : tensor<1x7x128xf32>
    %49 = stablehlo.convert %48 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %50 = stablehlo.broadcast_in_dim %49, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %51 = stablehlo.multiply %42, %50 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %52 = stablehlo.slice %42 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %53 = stablehlo.negate %52 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x64xbf16>
    %54 = stablehlo.slice %42 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %55 = stablehlo.concatenate %53, %54, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %56 = stablehlo.sine %47 : tensor<1x7x128xf32>
    %57 = stablehlo.convert %56 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %58 = stablehlo.broadcast_in_dim %57, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %59 = stablehlo.multiply %55, %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %60 = stablehlo.add %51, %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %61 = stablehlo.reshape %arg14 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %62 = stablehlo.reshape %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %63 = stablehlo.broadcast_in_dim %62, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %64 = stablehlo.reshape %arg11 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %65 = stablehlo.reshape %64 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %66 = stablehlo.transpose %65, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %67 = stablehlo.dot_general %30, %66, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %68 = stablehlo.reshape %67 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
    %69 = stablehlo.transpose %68, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
    %70 = stablehlo.broadcast_in_dim %49, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %71 = stablehlo.multiply %69, %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
    %72 = stablehlo.slice %69 [0:1, 0:64, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %73 = stablehlo.negate %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x64xbf16>
    %74 = stablehlo.slice %69 [0:1, 0:64, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %75 = stablehlo.concatenate %73, %74, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
    %76 = stablehlo.broadcast_in_dim %57, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %77 = stablehlo.multiply %75, %76 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
    %78 = stablehlo.add %71, %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
    %79 = stablehlo.broadcast_in_dim %60, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %80 = stablehlo.reshape %79 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %81 = stablehlo.transpose %80, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
    %82 = stablehlo.dot_general %78, %81, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
    %83 = stablehlo.multiply %82, %5 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xbf16>
    %84 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %85 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %86 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %87 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %88 = stablehlo.subtract %86, %87 : tensor<7x7xi64>
    %89 = stablehlo.compare  GE, %88, %4 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %90 = stablehlo.select %89, %3, %2 : tensor<7x7xi1>, tensor<7x7xbf16>
    %91 = stablehlo.compare  GT, %84, %85 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %92 = stablehlo.convert %91 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
    %93 = stablehlo.multiply %90, %92 : tensor<7x7xbf16>
    %94 = stablehlo.reshape %93 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
    %95 = stablehlo.reshape %arg10 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %96 = stablehlo.reshape %95 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
    %97 = stablehlo.convert %96 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
    %98 = stablehlo.reshape %97 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
    %99 = stablehlo.broadcast_in_dim %98, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
    %100 = stablehlo.add %94, %99 : tensor<1x1x7x7xbf16>
    %101 = stablehlo.compare  EQ, %100, %1 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
    %102 = stablehlo.select %101, %0, %94 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
    %103 = stablehlo.reshape %102 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
    %105 = stablehlo.add %83, %104 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xbf16>
    %106 = stablehlo.convert %105 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
    %107 = stablehlo.reduce(%106 init: %cst_0) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %108 = stablehlo.broadcast_in_dim %107, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %109 = stablehlo.subtract %106, %108 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
    %110 = stablehlo.exponential %109 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
    %111 = stablehlo.reduce(%110 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %112 = stablehlo.broadcast_in_dim %111, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %113 = stablehlo.divide %110, %112 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
    %114 = stablehlo.convert %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
    %115 = stablehlo.broadcast_in_dim %36, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %116 = stablehlo.reshape %115 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %117 = stablehlo.dot_general %114, %116, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %118 = stablehlo.transpose %117, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
    %119 = stablehlo.reshape %118 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
    %120 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %121 = stablehlo.reshape %120 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %122 = stablehlo.transpose %121, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %123 = stablehlo.dot_general %119, %122, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %124 = stablehlo.reshape %123 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %125 = stablehlo.add %16, %124 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
    %126 = stablehlo.reshape %arg12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %127 = stablehlo.reshape %126 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %128 = stablehlo.broadcast_in_dim %127, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %129 = stablehlo.convert %125 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %130 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
    %131 = stablehlo.power %129, %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
    %132 = stablehlo.reduce(%131 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %133 = stablehlo.multiply %132, %cst_2 : tensor<1x7xf32>
    %134 = stablehlo.reshape %133 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %135 = stablehlo.add %134, %cst_3 : tensor<1x7x1xf32>
    %136 = stablehlo.rsqrt %135 : tensor<1x7x1xf32>
    %137 = stablehlo.reshape %136 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %138 = stablehlo.broadcast_in_dim %137, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %139 = stablehlo.multiply %129, %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
    %140 = stablehlo.convert %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %141 = stablehlo.multiply %128, %140 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
    %142 = stablehlo.reshape %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %143 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %145 = stablehlo.transpose %144, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %146 = stablehlo.dot_general %142, %145, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %147 = stablehlo.reshape %146 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %148 = stablehlo.logistic %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
    %149 = stablehlo.multiply %147, %148 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
    %150 = stablehlo.reshape %arg8 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %151 = stablehlo.reshape %150 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %152 = stablehlo.transpose %151, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %153 = stablehlo.dot_general %142, %152, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %154 = stablehlo.reshape %153 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %155 = stablehlo.multiply %149, %154 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
    %156 = stablehlo.reshape %155 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
    %157 = stablehlo.reshape %arg7 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
    %158 = stablehlo.reshape %157 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
    %159 = stablehlo.transpose %158, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
    %160 = stablehlo.dot_general %156, %159, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
    %161 = stablehlo.reshape %160 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %162 = stablehlo.add %125, %161 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
    %163 = stablehlo.convert %162 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %164 = stablehlo.power %163, %6 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
    %165 = stablehlo.reduce(%164 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %166 = stablehlo.multiply %165, %cst_2 : tensor<1x7xf32>
    %167 = stablehlo.reshape %166 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %168 = stablehlo.add %167, %cst_3 : tensor<1x7x1xf32>
    %169 = stablehlo.rsqrt %168 : tensor<1x7x1xf32>
    %170 = stablehlo.reshape %169 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %171 = stablehlo.broadcast_in_dim %170, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %172 = stablehlo.multiply %163, %171 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
    %173 = stablehlo.convert %172 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %174 = stablehlo.multiply %63, %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
    %175 = stablehlo.reshape %174 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = stablehlo.reshape %arg6 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %177 = stablehlo.reshape %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %178 = stablehlo.transpose %177, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
    %179 = stablehlo.dot_general %175, %178, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
    %180 = stablehlo.reshape %179 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %36, %60, %180 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump After InsertExplicitReshardsPass (insert-explicit-reshards) ('builtin.module' operation: @SyncTensorsGraph.406) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8, "_axis_1"=4]>
  func.func @main(%arg0: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg1: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg2: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg3: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg4: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg5: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg6: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg7: tensor<8192x28672xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg8: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg9: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg10: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg11: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg12: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg13: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg14: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x7x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x7x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
    %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
    %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
    %c_6 = stablehlo.constant dense<1> : tensor<i64>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %4 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
    %5 = stablehlo.broadcast_in_dim %cst_5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
    %7 = sdy.reshard %arg3 <@mesh, [{"_axis_0"}]> : tensor<8192xbf16>
    %8 = stablehlo.reshape %7 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %9 = stablehlo.reshape %8 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %10 = stablehlo.broadcast_in_dim %9, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %11 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %12 = stablehlo.reshape %11 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %13 = stablehlo.reshape %arg1 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %14 = stablehlo.reshape %13 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %15 = stablehlo.convert %14 : (tensor<7xi64>) -> tensor<7xui32>
    %16 = "stablehlo.gather"(%12, %15) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
    %17 = stablehlo.reshape %16 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %18 = stablehlo.convert %17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %19 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
    %20 = stablehlo.power %18, %19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %22 = sdy.all_reduce {"_axis_0"} %21 out_sharding=<@mesh, [{}, {}]> : tensor<1x7xf32>
    %23 = stablehlo.multiply %22, %cst_2 : tensor<1x7xf32>
    %24 = stablehlo.reshape %23 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %25 = stablehlo.add %24, %cst_3 : tensor<1x7x1xf32>
    %26 = stablehlo.rsqrt %25 : tensor<1x7x1xf32>
    %27 = stablehlo.reshape %26 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %29 = stablehlo.multiply %18, %28 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %30 = stablehlo.convert %29 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %31 = stablehlo.multiply %10, %30 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xbf16>
    %32 = stablehlo.reshape %31 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %33 = stablehlo.reshape %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %34 = stablehlo.reshape %33 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %35 = stablehlo.transpose %34, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %36 = sdy.reshard %32 <@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
    %37 = stablehlo.dot_general %36, %35, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %38 = sdy.all_reduce {"_axis_1"} %37 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x1024xbf16>
    %39 = stablehlo.reshape %38 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %40 = stablehlo.transpose %39, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %41 = stablehlo.reshape %arg5 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %42 = stablehlo.reshape %41 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %43 = stablehlo.transpose %42, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %44 = sdy.reshard %32 <@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
    %45 = stablehlo.dot_general %44, %43, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %46 = sdy.all_reduce {"_axis_1"} %45 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x1024xbf16>
    %47 = stablehlo.reshape %46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %49 = stablehlo.reshape %arg4 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %50 = stablehlo.reshape %49 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %51 = stablehlo.dot_general %50, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %52 = stablehlo.transpose %51, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %53 = stablehlo.concatenate %52, %52, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %54 = stablehlo.cosine %53 : tensor<1x7x128xf32>
    %55 = stablehlo.convert %54 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %57 = stablehlo.multiply %48, %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %48 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %48 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.sine %53 : tensor<1x7x128xf32>
    %63 = stablehlo.convert %62 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %64 = stablehlo.broadcast_in_dim %63, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %65 = stablehlo.multiply %61, %64 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %66 = stablehlo.add %57, %65 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %67 = sdy.reshard %arg14 <@mesh, [{"_axis_1"}]> : tensor<8192xbf16>
    %68 = stablehlo.reshape %67 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %69 = stablehlo.reshape %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %70 = stablehlo.broadcast_in_dim %69, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %71 = stablehlo.reshape %arg11 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %72 = stablehlo.reshape %71 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %73 = stablehlo.transpose %72, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %74 = sdy.reshard %32 <@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
    %75 = stablehlo.dot_general %74, %73, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %76 = sdy.all_reduce {"_axis_1"} %75 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x8192xbf16>
    %77 = stablehlo.reshape %76 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
    %78 = stablehlo.transpose %77, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
    %79 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %80 = stablehlo.multiply %78, %79 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
    %81 = stablehlo.slice %78 [0:1, 0:64, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %82 = stablehlo.negate %81 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x64xbf16>
    %83 = stablehlo.slice %78 [0:1, 0:64, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %84 = stablehlo.concatenate %82, %83, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
    %85 = stablehlo.broadcast_in_dim %63, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %86 = stablehlo.multiply %84, %85 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
    %87 = stablehlo.add %80, %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
    %88 = stablehlo.broadcast_in_dim %66, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %89 = stablehlo.reshape %88 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %90 = stablehlo.transpose %89, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
    %91 = stablehlo.dot_general %87, %90, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
    %92 = stablehlo.multiply %91, %5 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xbf16>
    %93 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %94 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %95 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %96 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %97 = stablehlo.subtract %95, %96 : tensor<7x7xi64>
    %98 = stablehlo.compare  GE, %97, %4 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %99 = stablehlo.select %98, %3, %2 : tensor<7x7xi1>, tensor<7x7xbf16>
    %100 = stablehlo.compare  GT, %93, %94 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %101 = stablehlo.convert %100 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
    %102 = stablehlo.multiply %99, %101 : tensor<7x7xbf16>
    %103 = stablehlo.reshape %102 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
    %104 = stablehlo.reshape %arg10 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %105 = stablehlo.reshape %104 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
    %106 = stablehlo.convert %105 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
    %107 = stablehlo.reshape %106 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
    %108 = stablehlo.broadcast_in_dim %107, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
    %109 = stablehlo.add %103, %108 : tensor<1x1x7x7xbf16>
    %110 = stablehlo.compare  EQ, %109, %1 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
    %111 = stablehlo.select %110, %0, %103 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
    %112 = stablehlo.reshape %111 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
    %113 = stablehlo.broadcast_in_dim %112, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
    %114 = stablehlo.add %92, %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xbf16>
    %115 = stablehlo.convert %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
    %116 = stablehlo.reduce(%115 init: %cst_0) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %118 = stablehlo.subtract %115, %117 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
    %119 = stablehlo.exponential %118 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
    %120 = stablehlo.reduce(%119 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %121 = stablehlo.broadcast_in_dim %120, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %122 = stablehlo.divide %119, %121 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
    %123 = stablehlo.convert %122 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
    %124 = stablehlo.broadcast_in_dim %40, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %125 = stablehlo.reshape %124 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %126 = stablehlo.dot_general %123, %125, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
    %128 = stablehlo.reshape %127 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
    %129 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %130 = stablehlo.reshape %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %131 = stablehlo.transpose %130, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %132 = stablehlo.dot_general %128, %131, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %133 = sdy.all_reduce {"_axis_0"} %132 out_sharding=<@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
    %134 = stablehlo.reshape %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %135 = sdy.reshard %17 <@mesh, [{?}, {?}, {"_axis_1", ?}]> : tensor<1x7x8192xbf16>
    %136 = stablehlo.add %135, %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
    %137 = sdy.reshard %arg12 <@mesh, [{"_axis_1"}]> : tensor<8192xbf16>
    %138 = stablehlo.reshape %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %139 = stablehlo.reshape %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %140 = stablehlo.broadcast_in_dim %139, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %141 = stablehlo.convert %136 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %142 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
    %143 = stablehlo.power %141, %142 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
    %144 = stablehlo.reduce(%143 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %145 = sdy.all_reduce {"_axis_1"} %144 out_sharding=<@mesh, [{}, {}]> : tensor<1x7xf32>
    %146 = stablehlo.multiply %145, %cst_2 : tensor<1x7xf32>
    %147 = stablehlo.reshape %146 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %148 = stablehlo.add %147, %cst_3 : tensor<1x7x1xf32>
    %149 = stablehlo.rsqrt %148 : tensor<1x7x1xf32>
    %150 = stablehlo.reshape %149 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %151 = stablehlo.broadcast_in_dim %150, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %152 = stablehlo.multiply %141, %151 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
    %153 = stablehlo.convert %152 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %154 = stablehlo.multiply %140, %153 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
    %155 = stablehlo.reshape %154 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %156 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %157 = stablehlo.reshape %156 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %158 = stablehlo.transpose %157, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %159 = stablehlo.dot_general %155, %158, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %160 = sdy.all_reduce {"_axis_1"} %159 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x28672xbf16>
    %161 = stablehlo.reshape %160 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %162 = stablehlo.logistic %161 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
    %163 = stablehlo.multiply %161, %162 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
    %164 = stablehlo.reshape %arg8 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %165 = stablehlo.reshape %164 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %166 = stablehlo.transpose %165, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %167 = stablehlo.dot_general %155, %166, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %168 = sdy.all_reduce {"_axis_1"} %167 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x28672xbf16>
    %169 = stablehlo.reshape %168 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %170 = stablehlo.multiply %163, %169 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
    %171 = stablehlo.reshape %170 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
    %172 = stablehlo.reshape %arg7 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
    %173 = stablehlo.reshape %172 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = sdy.all_reduce {"_axis_0"} %175 out_sharding=<@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
    %177 = stablehlo.reshape %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %178 = stablehlo.add %136, %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
    %179 = stablehlo.convert %178 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %180 = stablehlo.power %179, %6 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
    %181 = stablehlo.reduce(%180 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %182 = sdy.all_reduce {"_axis_1"} %181 out_sharding=<@mesh, [{}, {}]> : tensor<1x7xf32>
    %183 = stablehlo.multiply %182, %cst_2 : tensor<1x7xf32>
    %184 = stablehlo.reshape %183 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %185 = stablehlo.add %184, %cst_3 : tensor<1x7x1xf32>
    %186 = stablehlo.rsqrt %185 : tensor<1x7x1xf32>
    %187 = stablehlo.reshape %186 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %188 = stablehlo.broadcast_in_dim %187, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %189 = stablehlo.multiply %179, %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
    %190 = stablehlo.convert %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %191 = stablehlo.multiply %70, %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
    %192 = stablehlo.reshape %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %193 = stablehlo.reshape %arg6 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %194 = stablehlo.reshape %193 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %195 = stablehlo.transpose %194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
    %196 = sdy.reshard %192 <@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x8192xbf16>
    %197 = stablehlo.dot_general %196, %195, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
    %198 = sdy.all_reduce {"_axis_0"} %197 out_sharding=<@mesh, [{}, {}]> : tensor<7x128256xbf16>
    %199 = stablehlo.reshape %198 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %40, %66, %199 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before WrapUnderManualComputationPass (wrap-under-manual-computation) ('builtin.module' operation: @SyncTensorsGraph.406) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8, "_axis_1"=4]>
  func.func @main(%arg0: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg1: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg2: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg3: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg4: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg5: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg6: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg7: tensor<8192x28672xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg8: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg9: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg10: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg11: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg12: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg13: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg14: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x7x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x7x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
    %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
    %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
    %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
    %c_6 = stablehlo.constant dense<1> : tensor<i64>
    %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
    %4 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
    %5 = stablehlo.broadcast_in_dim %cst_5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
    %7 = sdy.reshard %arg3 <@mesh, [{"_axis_0"}]> : tensor<8192xbf16>
    %8 = stablehlo.reshape %7 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %9 = stablehlo.reshape %8 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %10 = stablehlo.broadcast_in_dim %9, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %11 = stablehlo.reshape %arg2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %12 = stablehlo.reshape %11 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %13 = stablehlo.reshape %arg1 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %14 = stablehlo.reshape %13 : (tensor<1x1x7xi64>) -> tensor<7xi64>
    %15 = stablehlo.convert %14 : (tensor<7xi64>) -> tensor<7xui32>
    %16 = "stablehlo.gather"(%12, %15) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
    %17 = stablehlo.reshape %16 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %18 = stablehlo.convert %17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %19 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
    %20 = stablehlo.power %18, %19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %21 = stablehlo.reduce(%20 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %22 = sdy.all_reduce {"_axis_0"} %21 out_sharding=<@mesh, [{}, {}]> : tensor<1x7xf32>
    %23 = stablehlo.multiply %22, %cst_2 : tensor<1x7xf32>
    %24 = stablehlo.reshape %23 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %25 = stablehlo.add %24, %cst_3 : tensor<1x7x1xf32>
    %26 = stablehlo.rsqrt %25 : tensor<1x7x1xf32>
    %27 = stablehlo.reshape %26 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %28 = stablehlo.broadcast_in_dim %27, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %29 = stablehlo.multiply %18, %28 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
    %30 = stablehlo.convert %29 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %31 = stablehlo.multiply %10, %30 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xbf16>
    %32 = stablehlo.reshape %31 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %33 = stablehlo.reshape %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %34 = stablehlo.reshape %33 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %35 = stablehlo.transpose %34, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %36 = sdy.reshard %32 <@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
    %37 = stablehlo.dot_general %36, %35, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %38 = sdy.all_reduce {"_axis_1"} %37 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x1024xbf16>
    %39 = stablehlo.reshape %38 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %40 = stablehlo.transpose %39, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %41 = stablehlo.reshape %arg5 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
    %42 = stablehlo.reshape %41 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
    %43 = stablehlo.transpose %42, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
    %44 = sdy.reshard %32 <@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
    %45 = stablehlo.dot_general %44, %43, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
    %46 = sdy.all_reduce {"_axis_1"} %45 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x1024xbf16>
    %47 = stablehlo.reshape %46 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %48 = stablehlo.transpose %47, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %49 = stablehlo.reshape %arg4 : (tensor<64xf32>) -> tensor<1x1x64xf32>
    %50 = stablehlo.reshape %49 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
    %51 = stablehlo.dot_general %50, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %52 = stablehlo.transpose %51, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %53 = stablehlo.concatenate %52, %52, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %54 = stablehlo.cosine %53 : tensor<1x7x128xf32>
    %55 = stablehlo.convert %54 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %56 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %57 = stablehlo.multiply %48, %56 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %58 = stablehlo.slice %48 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %59 = stablehlo.negate %58 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x64xbf16>
    %60 = stablehlo.slice %48 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %61 = stablehlo.concatenate %59, %60, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %62 = stablehlo.sine %53 : tensor<1x7x128xf32>
    %63 = stablehlo.convert %62 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %64 = stablehlo.broadcast_in_dim %63, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %65 = stablehlo.multiply %61, %64 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %66 = stablehlo.add %57, %65 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
    %67 = sdy.reshard %arg14 <@mesh, [{"_axis_1"}]> : tensor<8192xbf16>
    %68 = stablehlo.reshape %67 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %69 = stablehlo.reshape %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %70 = stablehlo.broadcast_in_dim %69, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %71 = stablehlo.reshape %arg11 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %72 = stablehlo.reshape %71 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %73 = stablehlo.transpose %72, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %74 = sdy.reshard %32 <@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
    %75 = stablehlo.dot_general %74, %73, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %76 = sdy.all_reduce {"_axis_1"} %75 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x8192xbf16>
    %77 = stablehlo.reshape %76 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
    %78 = stablehlo.transpose %77, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
    %79 = stablehlo.broadcast_in_dim %55, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %80 = stablehlo.multiply %78, %79 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
    %81 = stablehlo.slice %78 [0:1, 0:64, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %82 = stablehlo.negate %81 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x64xbf16>
    %83 = stablehlo.slice %78 [0:1, 0:64, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
    %84 = stablehlo.concatenate %82, %83, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
    %85 = stablehlo.broadcast_in_dim %63, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %86 = stablehlo.multiply %84, %85 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
    %87 = stablehlo.add %80, %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
    %88 = stablehlo.broadcast_in_dim %66, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %89 = stablehlo.reshape %88 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %90 = stablehlo.transpose %89, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
    %91 = stablehlo.dot_general %87, %90, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
    %92 = stablehlo.multiply %91, %5 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xbf16>
    %93 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %94 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %95 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
    %96 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
    %97 = stablehlo.subtract %95, %96 : tensor<7x7xi64>
    %98 = stablehlo.compare  GE, %97, %4 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %99 = stablehlo.select %98, %3, %2 : tensor<7x7xi1>, tensor<7x7xbf16>
    %100 = stablehlo.compare  GT, %93, %94 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
    %101 = stablehlo.convert %100 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
    %102 = stablehlo.multiply %99, %101 : tensor<7x7xbf16>
    %103 = stablehlo.reshape %102 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
    %104 = stablehlo.reshape %arg10 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
    %105 = stablehlo.reshape %104 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
    %106 = stablehlo.convert %105 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
    %107 = stablehlo.reshape %106 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
    %108 = stablehlo.broadcast_in_dim %107, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
    %109 = stablehlo.add %103, %108 : tensor<1x1x7x7xbf16>
    %110 = stablehlo.compare  EQ, %109, %1 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
    %111 = stablehlo.select %110, %0, %103 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
    %112 = stablehlo.reshape %111 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
    %113 = stablehlo.broadcast_in_dim %112, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
    %114 = stablehlo.add %92, %113 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xbf16>
    %115 = stablehlo.convert %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
    %116 = stablehlo.reduce(%115 init: %cst_0) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %117 = stablehlo.broadcast_in_dim %116, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %118 = stablehlo.subtract %115, %117 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
    %119 = stablehlo.exponential %118 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
    %120 = stablehlo.reduce(%119 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
    %121 = stablehlo.broadcast_in_dim %120, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
    %122 = stablehlo.divide %119, %121 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
    %123 = stablehlo.convert %122 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
    %124 = stablehlo.broadcast_in_dim %40, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
    %125 = stablehlo.reshape %124 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %126 = stablehlo.dot_general %123, %125, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
    %128 = stablehlo.reshape %127 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
    %129 = stablehlo.reshape %arg9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
    %130 = stablehlo.reshape %129 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %131 = stablehlo.transpose %130, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
    %132 = stablehlo.dot_general %128, %131, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
    %133 = sdy.all_reduce {"_axis_0"} %132 out_sharding=<@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
    %134 = stablehlo.reshape %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %135 = sdy.reshard %17 <@mesh, [{?}, {?}, {"_axis_1", ?}]> : tensor<1x7x8192xbf16>
    %136 = stablehlo.add %135, %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
    %137 = sdy.reshard %arg12 <@mesh, [{"_axis_1"}]> : tensor<8192xbf16>
    %138 = stablehlo.reshape %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
    %139 = stablehlo.reshape %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
    %140 = stablehlo.broadcast_in_dim %139, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
    %141 = stablehlo.convert %136 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %142 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
    %143 = stablehlo.power %141, %142 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
    %144 = stablehlo.reduce(%143 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %145 = sdy.all_reduce {"_axis_1"} %144 out_sharding=<@mesh, [{}, {}]> : tensor<1x7xf32>
    %146 = stablehlo.multiply %145, %cst_2 : tensor<1x7xf32>
    %147 = stablehlo.reshape %146 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %148 = stablehlo.add %147, %cst_3 : tensor<1x7x1xf32>
    %149 = stablehlo.rsqrt %148 : tensor<1x7x1xf32>
    %150 = stablehlo.reshape %149 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %151 = stablehlo.broadcast_in_dim %150, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %152 = stablehlo.multiply %141, %151 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
    %153 = stablehlo.convert %152 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %154 = stablehlo.multiply %140, %153 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
    %155 = stablehlo.reshape %154 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %156 = stablehlo.reshape %arg13 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %157 = stablehlo.reshape %156 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %158 = stablehlo.transpose %157, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %159 = stablehlo.dot_general %155, %158, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %160 = sdy.all_reduce {"_axis_1"} %159 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x28672xbf16>
    %161 = stablehlo.reshape %160 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %162 = stablehlo.logistic %161 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
    %163 = stablehlo.multiply %161, %162 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
    %164 = stablehlo.reshape %arg8 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
    %165 = stablehlo.reshape %164 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
    %166 = stablehlo.transpose %165, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
    %167 = stablehlo.dot_general %155, %166, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
    %168 = sdy.all_reduce {"_axis_1"} %167 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x28672xbf16>
    %169 = stablehlo.reshape %168 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
    %170 = stablehlo.multiply %163, %169 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
    %171 = stablehlo.reshape %170 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
    %172 = stablehlo.reshape %arg7 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
    %173 = stablehlo.reshape %172 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
    %174 = stablehlo.transpose %173, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
    %175 = stablehlo.dot_general %171, %174, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
    %176 = sdy.all_reduce {"_axis_0"} %175 out_sharding=<@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
    %177 = stablehlo.reshape %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %178 = stablehlo.add %136, %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
    %179 = stablehlo.convert %178 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %180 = stablehlo.power %179, %6 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
    %181 = stablehlo.reduce(%180 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
    %182 = sdy.all_reduce {"_axis_1"} %181 out_sharding=<@mesh, [{}, {}]> : tensor<1x7xf32>
    %183 = stablehlo.multiply %182, %cst_2 : tensor<1x7xf32>
    %184 = stablehlo.reshape %183 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %185 = stablehlo.add %184, %cst_3 : tensor<1x7x1xf32>
    %186 = stablehlo.rsqrt %185 : tensor<1x7x1xf32>
    %187 = stablehlo.reshape %186 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %188 = stablehlo.broadcast_in_dim %187, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
    %189 = stablehlo.multiply %179, %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
    %190 = stablehlo.convert %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %191 = stablehlo.multiply %70, %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
    %192 = stablehlo.reshape %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %193 = stablehlo.reshape %arg6 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
    %194 = stablehlo.reshape %193 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
    %195 = stablehlo.transpose %194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
    %196 = sdy.reshard %192 <@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x8192xbf16>
    %197 = stablehlo.dot_general %196, %195, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
    %198 = sdy.all_reduce {"_axis_0"} %197 out_sharding=<@mesh, [{}, {}]> : tensor<7x128256xbf16>
    %199 = stablehlo.reshape %198 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %40, %66, %199 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump After WrapUnderManualComputationPass (wrap-under-manual-computation) ('builtin.module' operation: @SyncTensorsGraph.406) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8, "_axis_1"=4]>
  func.func @main(%arg0: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg1: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg2: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg3: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg4: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg5: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg6: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg7: tensor<8192x28672xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg8: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg9: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg10: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg11: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg12: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg13: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg14: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x7x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x7x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:3 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14) in_shardings=[<@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>] manual_axes={} (%arg15: tensor<1024x8192xbf16>, %arg16: tensor<1x7xi64>, %arg17: tensor<128256x8192xbf16>, %arg18: tensor<8192xbf16>, %arg19: tensor<64xf32>, %arg20: tensor<1024x8192xbf16>, %arg21: tensor<128256x8192xbf16>, %arg22: tensor<8192x28672xbf16>, %arg23: tensor<28672x8192xbf16>, %arg24: tensor<8192x8192xbf16>, %arg25: tensor<1x7xi64>, %arg26: tensor<8192x8192xbf16>, %arg27: tensor<8192xbf16>, %arg28: tensor<28672x8192xbf16>, %arg29: tensor<8192xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
      %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
      %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
      %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
      %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
      %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
      %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
      %c_6 = stablehlo.constant dense<1> : tensor<i64>
      %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
      %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
      %3 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
      %4 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
      %5 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
      %6 = stablehlo.broadcast_in_dim %cst_5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
      %7 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
      %8 = sdy.reshard %arg18 <@mesh, [{"_axis_0"}]> : tensor<8192xbf16>
      %9 = stablehlo.reshape %8 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
      %10 = stablehlo.reshape %9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
      %11 = stablehlo.broadcast_in_dim %10, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
      %12 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
      %13 = stablehlo.reshape %12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
      %14 = stablehlo.reshape %arg16 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
      %15 = stablehlo.reshape %14 : (tensor<1x1x7xi64>) -> tensor<7xi64>
      %16 = stablehlo.convert %15 : (tensor<7xi64>) -> tensor<7xui32>
      %17 = "stablehlo.gather"(%13, %16) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
      %18 = stablehlo.reshape %17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
      %19 = stablehlo.convert %18 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
      %20 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
      %21 = stablehlo.power %19, %20 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
      %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
      %23 = sdy.all_reduce {"_axis_0"} %22 out_sharding=<@mesh, [{}, {}]> : tensor<1x7xf32>
      %24 = stablehlo.multiply %23, %cst_2 : tensor<1x7xf32>
      %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %26 = stablehlo.add %25, %cst_3 : tensor<1x7x1xf32>
      %27 = stablehlo.rsqrt %26 : tensor<1x7x1xf32>
      %28 = stablehlo.reshape %27 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %29 = stablehlo.broadcast_in_dim %28, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
      %30 = stablehlo.multiply %19, %29 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
      %31 = stablehlo.convert %30 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
      %32 = stablehlo.multiply %11, %31 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xbf16>
      %33 = stablehlo.reshape %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
      %34 = stablehlo.reshape %arg15 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
      %35 = stablehlo.reshape %34 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
      %36 = stablehlo.transpose %35, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
      %37 = sdy.reshard %33 <@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
      %38 = stablehlo.dot_general %37, %36, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
      %39 = sdy.all_reduce {"_axis_1"} %38 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x1024xbf16>
      %40 = stablehlo.reshape %39 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
      %41 = stablehlo.transpose %40, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
      %42 = stablehlo.reshape %arg20 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
      %43 = stablehlo.reshape %42 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
      %44 = stablehlo.transpose %43, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
      %45 = sdy.reshard %33 <@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
      %46 = stablehlo.dot_general %45, %44, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
      %47 = sdy.all_reduce {"_axis_1"} %46 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x1024xbf16>
      %48 = stablehlo.reshape %47 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
      %49 = stablehlo.transpose %48, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
      %50 = stablehlo.reshape %arg19 : (tensor<64xf32>) -> tensor<1x1x64xf32>
      %51 = stablehlo.reshape %50 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
      %52 = stablehlo.dot_general %51, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
      %53 = stablehlo.transpose %52, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
      %54 = stablehlo.concatenate %53, %53, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
      %55 = stablehlo.cosine %54 : tensor<1x7x128xf32>
      %56 = stablehlo.convert %55 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
      %57 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
      %58 = stablehlo.multiply %49, %57 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
      %59 = stablehlo.slice %49 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
      %60 = stablehlo.negate %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x64xbf16>
      %61 = stablehlo.slice %49 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
      %62 = stablehlo.concatenate %60, %61, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
      %63 = stablehlo.sine %54 : tensor<1x7x128xf32>
      %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
      %65 = stablehlo.broadcast_in_dim %64, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
      %66 = stablehlo.multiply %62, %65 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
      %67 = stablehlo.add %58, %66 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
      %68 = sdy.reshard %arg29 <@mesh, [{"_axis_1"}]> : tensor<8192xbf16>
      %69 = stablehlo.reshape %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
      %70 = stablehlo.reshape %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
      %71 = stablehlo.broadcast_in_dim %70, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
      %72 = stablehlo.reshape %arg26 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
      %73 = stablehlo.reshape %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
      %74 = stablehlo.transpose %73, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
      %75 = sdy.reshard %33 <@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
      %76 = stablehlo.dot_general %75, %74, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
      %77 = sdy.all_reduce {"_axis_1"} %76 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x8192xbf16>
      %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
      %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
      %80 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
      %81 = stablehlo.multiply %79, %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
      %82 = stablehlo.slice %79 [0:1, 0:64, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
      %83 = stablehlo.negate %82 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x64xbf16>
      %84 = stablehlo.slice %79 [0:1, 0:64, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
      %85 = stablehlo.concatenate %83, %84, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
      %86 = stablehlo.broadcast_in_dim %64, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
      %87 = stablehlo.multiply %85, %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
      %88 = stablehlo.add %81, %87 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
      %89 = stablehlo.broadcast_in_dim %67, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
      %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
      %91 = stablehlo.transpose %90, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
      %92 = stablehlo.dot_general %88, %91, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
      %93 = stablehlo.multiply %92, %6 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xbf16>
      %94 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
      %95 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
      %96 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
      %97 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
      %98 = stablehlo.subtract %96, %97 : tensor<7x7xi64>
      %99 = stablehlo.compare  GE, %98, %5 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
      %100 = stablehlo.select %99, %4, %3 : tensor<7x7xi1>, tensor<7x7xbf16>
      %101 = stablehlo.compare  GT, %94, %95 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
      %102 = stablehlo.convert %101 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
      %103 = stablehlo.multiply %100, %102 : tensor<7x7xbf16>
      %104 = stablehlo.reshape %103 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
      %105 = stablehlo.reshape %arg25 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
      %106 = stablehlo.reshape %105 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
      %107 = stablehlo.convert %106 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
      %108 = stablehlo.reshape %107 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
      %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
      %110 = stablehlo.add %104, %109 : tensor<1x1x7x7xbf16>
      %111 = stablehlo.compare  EQ, %110, %2 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
      %112 = stablehlo.select %111, %1, %104 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
      %113 = stablehlo.reshape %112 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
      %114 = stablehlo.broadcast_in_dim %113, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
      %115 = stablehlo.add %93, %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xbf16>
      %116 = stablehlo.convert %115 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
      %117 = stablehlo.reduce(%116 init: %cst_0) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
      %118 = stablehlo.broadcast_in_dim %117, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
      %119 = stablehlo.subtract %116, %118 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
      %120 = stablehlo.exponential %119 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
      %121 = stablehlo.reduce(%120 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
      %122 = stablehlo.broadcast_in_dim %121, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
      %123 = stablehlo.divide %120, %122 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
      %124 = stablehlo.convert %123 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
      %125 = stablehlo.broadcast_in_dim %41, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
      %126 = stablehlo.reshape %125 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
      %127 = stablehlo.dot_general %124, %126, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
      %128 = stablehlo.transpose %127, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
      %129 = stablehlo.reshape %128 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
      %130 = stablehlo.reshape %arg24 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
      %131 = stablehlo.reshape %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
      %132 = stablehlo.transpose %131, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
      %133 = stablehlo.dot_general %129, %132, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
      %134 = sdy.all_reduce {"_axis_0"} %133 out_sharding=<@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
      %135 = stablehlo.reshape %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
      %136 = sdy.reshard %18 <@mesh, [{?}, {?}, {"_axis_1", ?}]> : tensor<1x7x8192xbf16>
      %137 = stablehlo.add %136, %135 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
      %138 = sdy.reshard %arg27 <@mesh, [{"_axis_1"}]> : tensor<8192xbf16>
      %139 = stablehlo.reshape %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
      %140 = stablehlo.reshape %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
      %141 = stablehlo.broadcast_in_dim %140, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
      %142 = stablehlo.convert %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
      %143 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
      %144 = stablehlo.power %142, %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
      %145 = stablehlo.reduce(%144 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
      %146 = sdy.all_reduce {"_axis_1"} %145 out_sharding=<@mesh, [{}, {}]> : tensor<1x7xf32>
      %147 = stablehlo.multiply %146, %cst_2 : tensor<1x7xf32>
      %148 = stablehlo.reshape %147 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %149 = stablehlo.add %148, %cst_3 : tensor<1x7x1xf32>
      %150 = stablehlo.rsqrt %149 : tensor<1x7x1xf32>
      %151 = stablehlo.reshape %150 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %152 = stablehlo.broadcast_in_dim %151, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
      %153 = stablehlo.multiply %142, %152 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
      %154 = stablehlo.convert %153 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
      %155 = stablehlo.multiply %141, %154 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
      %156 = stablehlo.reshape %155 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
      %157 = stablehlo.reshape %arg28 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
      %158 = stablehlo.reshape %157 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
      %159 = stablehlo.transpose %158, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
      %160 = stablehlo.dot_general %156, %159, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
      %161 = sdy.all_reduce {"_axis_1"} %160 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x28672xbf16>
      %162 = stablehlo.reshape %161 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
      %163 = stablehlo.logistic %162 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
      %164 = stablehlo.multiply %162, %163 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
      %165 = stablehlo.reshape %arg23 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
      %166 = stablehlo.reshape %165 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
      %167 = stablehlo.transpose %166, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
      %168 = stablehlo.dot_general %156, %167, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
      %169 = sdy.all_reduce {"_axis_1"} %168 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x28672xbf16>
      %170 = stablehlo.reshape %169 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
      %171 = stablehlo.multiply %164, %170 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
      %172 = stablehlo.reshape %171 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
      %173 = stablehlo.reshape %arg22 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
      %174 = stablehlo.reshape %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
      %175 = stablehlo.transpose %174, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
      %176 = stablehlo.dot_general %172, %175, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
      %177 = sdy.all_reduce {"_axis_0"} %176 out_sharding=<@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
      %178 = stablehlo.reshape %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
      %179 = stablehlo.add %137, %178 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
      %180 = stablehlo.convert %179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
      %181 = stablehlo.power %180, %7 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
      %182 = stablehlo.reduce(%181 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
      %183 = sdy.all_reduce {"_axis_1"} %182 out_sharding=<@mesh, [{}, {}]> : tensor<1x7xf32>
      %184 = stablehlo.multiply %183, %cst_2 : tensor<1x7xf32>
      %185 = stablehlo.reshape %184 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %186 = stablehlo.add %185, %cst_3 : tensor<1x7x1xf32>
      %187 = stablehlo.rsqrt %186 : tensor<1x7x1xf32>
      %188 = stablehlo.reshape %187 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %189 = stablehlo.broadcast_in_dim %188, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
      %190 = stablehlo.multiply %180, %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
      %191 = stablehlo.convert %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
      %192 = stablehlo.multiply %71, %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
      %193 = stablehlo.reshape %192 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
      %194 = stablehlo.reshape %arg21 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
      %195 = stablehlo.reshape %194 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
      %196 = stablehlo.transpose %195, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
      %197 = sdy.reshard %193 <@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x8192xbf16>
      %198 = stablehlo.dot_general %197, %196, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
      %199 = sdy.all_reduce {"_axis_0"} %198 out_sharding=<@mesh, [{}, {}]> : tensor<7x128256xbf16>
      %200 = stablehlo.reshape %199 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
      sdy.return %41, %67, %200 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
    } : (tensor<1024x8192xbf16>, tensor<1x7xi64>, tensor<128256x8192xbf16>, tensor<8192xbf16>, tensor<64xf32>, tensor<1024x8192xbf16>, tensor<128256x8192xbf16>, tensor<8192x28672xbf16>, tensor<28672x8192xbf16>, tensor<8192x8192xbf16>, tensor<1x7xi64>, tensor<8192x8192xbf16>, tensor<8192xbf16>, tensor<28672x8192xbf16>, tensor<8192xbf16>) -> (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>)
    return %0#0, %0#1, %0#2 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before ReshardToCollectivesPass (sdy-reshard-to-collectives) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8, "_axis_1"=4]>
  func.func @main(%arg0: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg1: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg2: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg3: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg4: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg5: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg6: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg7: tensor<8192x28672xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg8: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg9: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg10: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg11: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg12: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg13: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg14: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x7x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x7x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:3 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14) in_shardings=[<@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>] manual_axes={} (%arg15: tensor<1024x8192xbf16>, %arg16: tensor<1x7xi64>, %arg17: tensor<128256x8192xbf16>, %arg18: tensor<8192xbf16>, %arg19: tensor<64xf32>, %arg20: tensor<1024x8192xbf16>, %arg21: tensor<128256x8192xbf16>, %arg22: tensor<8192x28672xbf16>, %arg23: tensor<28672x8192xbf16>, %arg24: tensor<8192x8192xbf16>, %arg25: tensor<1x7xi64>, %arg26: tensor<8192x8192xbf16>, %arg27: tensor<8192xbf16>, %arg28: tensor<28672x8192xbf16>, %arg29: tensor<8192xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
      %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
      %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
      %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
      %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
      %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
      %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
      %c_6 = stablehlo.constant dense<1> : tensor<i64>
      %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
      %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
      %3 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
      %4 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
      %5 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
      %6 = stablehlo.broadcast_in_dim %cst_5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
      %7 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
      %8 = sdy.reshard %arg18 <@mesh, [{"_axis_0"}]> : tensor<8192xbf16>
      %9 = stablehlo.reshape %8 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
      %10 = stablehlo.reshape %9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
      %11 = stablehlo.broadcast_in_dim %10, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
      %12 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
      %13 = stablehlo.reshape %12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
      %14 = stablehlo.reshape %arg16 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
      %15 = stablehlo.reshape %14 : (tensor<1x1x7xi64>) -> tensor<7xi64>
      %16 = stablehlo.convert %15 : (tensor<7xi64>) -> tensor<7xui32>
      %17 = "stablehlo.gather"(%13, %16) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
      %18 = stablehlo.reshape %17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
      %19 = stablehlo.convert %18 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
      %20 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
      %21 = stablehlo.power %19, %20 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
      %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
      %23 = sdy.all_reduce {"_axis_0"} %22 out_sharding=<@mesh, [{}, {}]> : tensor<1x7xf32>
      %24 = stablehlo.multiply %23, %cst_2 : tensor<1x7xf32>
      %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %26 = stablehlo.add %25, %cst_3 : tensor<1x7x1xf32>
      %27 = stablehlo.rsqrt %26 : tensor<1x7x1xf32>
      %28 = stablehlo.reshape %27 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %29 = stablehlo.broadcast_in_dim %28, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
      %30 = stablehlo.multiply %19, %29 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
      %31 = stablehlo.convert %30 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
      %32 = stablehlo.multiply %11, %31 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xbf16>
      %33 = stablehlo.reshape %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
      %34 = stablehlo.reshape %arg15 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
      %35 = stablehlo.reshape %34 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
      %36 = stablehlo.transpose %35, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
      %37 = sdy.reshard %33 <@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
      %38 = stablehlo.dot_general %37, %36, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
      %39 = sdy.all_reduce {"_axis_1"} %38 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x1024xbf16>
      %40 = stablehlo.reshape %39 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
      %41 = stablehlo.transpose %40, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
      %42 = stablehlo.reshape %arg20 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
      %43 = stablehlo.reshape %42 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
      %44 = stablehlo.transpose %43, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
      %45 = sdy.reshard %33 <@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
      %46 = stablehlo.dot_general %45, %44, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
      %47 = sdy.all_reduce {"_axis_1"} %46 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x1024xbf16>
      %48 = stablehlo.reshape %47 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
      %49 = stablehlo.transpose %48, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
      %50 = stablehlo.reshape %arg19 : (tensor<64xf32>) -> tensor<1x1x64xf32>
      %51 = stablehlo.reshape %50 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
      %52 = stablehlo.dot_general %51, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
      %53 = stablehlo.transpose %52, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
      %54 = stablehlo.concatenate %53, %53, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
      %55 = stablehlo.cosine %54 : tensor<1x7x128xf32>
      %56 = stablehlo.convert %55 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
      %57 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
      %58 = stablehlo.multiply %49, %57 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
      %59 = stablehlo.slice %49 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
      %60 = stablehlo.negate %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x64xbf16>
      %61 = stablehlo.slice %49 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
      %62 = stablehlo.concatenate %60, %61, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
      %63 = stablehlo.sine %54 : tensor<1x7x128xf32>
      %64 = stablehlo.convert %63 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
      %65 = stablehlo.broadcast_in_dim %64, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
      %66 = stablehlo.multiply %62, %65 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
      %67 = stablehlo.add %58, %66 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
      %68 = sdy.reshard %arg29 <@mesh, [{"_axis_1"}]> : tensor<8192xbf16>
      %69 = stablehlo.reshape %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
      %70 = stablehlo.reshape %69 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
      %71 = stablehlo.broadcast_in_dim %70, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
      %72 = stablehlo.reshape %arg26 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
      %73 = stablehlo.reshape %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
      %74 = stablehlo.transpose %73, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
      %75 = sdy.reshard %33 <@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
      %76 = stablehlo.dot_general %75, %74, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
      %77 = sdy.all_reduce {"_axis_1"} %76 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x8192xbf16>
      %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
      %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
      %80 = stablehlo.broadcast_in_dim %56, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
      %81 = stablehlo.multiply %79, %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
      %82 = stablehlo.slice %79 [0:1, 0:64, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
      %83 = stablehlo.negate %82 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x64xbf16>
      %84 = stablehlo.slice %79 [0:1, 0:64, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
      %85 = stablehlo.concatenate %83, %84, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
      %86 = stablehlo.broadcast_in_dim %64, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
      %87 = stablehlo.multiply %85, %86 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
      %88 = stablehlo.add %81, %87 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
      %89 = stablehlo.broadcast_in_dim %67, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
      %90 = stablehlo.reshape %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
      %91 = stablehlo.transpose %90, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
      %92 = stablehlo.dot_general %88, %91, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
      %93 = stablehlo.multiply %92, %6 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xbf16>
      %94 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
      %95 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
      %96 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
      %97 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
      %98 = stablehlo.subtract %96, %97 : tensor<7x7xi64>
      %99 = stablehlo.compare  GE, %98, %5 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
      %100 = stablehlo.select %99, %4, %3 : tensor<7x7xi1>, tensor<7x7xbf16>
      %101 = stablehlo.compare  GT, %94, %95 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
      %102 = stablehlo.convert %101 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
      %103 = stablehlo.multiply %100, %102 : tensor<7x7xbf16>
      %104 = stablehlo.reshape %103 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
      %105 = stablehlo.reshape %arg25 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
      %106 = stablehlo.reshape %105 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
      %107 = stablehlo.convert %106 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
      %108 = stablehlo.reshape %107 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
      %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
      %110 = stablehlo.add %104, %109 : tensor<1x1x7x7xbf16>
      %111 = stablehlo.compare  EQ, %110, %2 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
      %112 = stablehlo.select %111, %1, %104 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
      %113 = stablehlo.reshape %112 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
      %114 = stablehlo.broadcast_in_dim %113, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
      %115 = stablehlo.add %93, %114 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xbf16>
      %116 = stablehlo.convert %115 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
      %117 = stablehlo.reduce(%116 init: %cst_0) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
      %118 = stablehlo.broadcast_in_dim %117, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
      %119 = stablehlo.subtract %116, %118 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
      %120 = stablehlo.exponential %119 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
      %121 = stablehlo.reduce(%120 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
      %122 = stablehlo.broadcast_in_dim %121, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
      %123 = stablehlo.divide %120, %122 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
      %124 = stablehlo.convert %123 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
      %125 = stablehlo.broadcast_in_dim %41, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
      %126 = stablehlo.reshape %125 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
      %127 = stablehlo.dot_general %124, %126, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
      %128 = stablehlo.transpose %127, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
      %129 = stablehlo.reshape %128 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
      %130 = stablehlo.reshape %arg24 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
      %131 = stablehlo.reshape %130 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
      %132 = stablehlo.transpose %131, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
      %133 = stablehlo.dot_general %129, %132, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
      %134 = sdy.all_reduce {"_axis_0"} %133 out_sharding=<@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
      %135 = stablehlo.reshape %134 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
      %136 = sdy.reshard %18 <@mesh, [{?}, {?}, {"_axis_1", ?}]> : tensor<1x7x8192xbf16>
      %137 = stablehlo.add %136, %135 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
      %138 = sdy.reshard %arg27 <@mesh, [{"_axis_1"}]> : tensor<8192xbf16>
      %139 = stablehlo.reshape %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
      %140 = stablehlo.reshape %139 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
      %141 = stablehlo.broadcast_in_dim %140, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
      %142 = stablehlo.convert %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
      %143 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
      %144 = stablehlo.power %142, %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
      %145 = stablehlo.reduce(%144 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
      %146 = sdy.all_reduce {"_axis_1"} %145 out_sharding=<@mesh, [{}, {}]> : tensor<1x7xf32>
      %147 = stablehlo.multiply %146, %cst_2 : tensor<1x7xf32>
      %148 = stablehlo.reshape %147 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %149 = stablehlo.add %148, %cst_3 : tensor<1x7x1xf32>
      %150 = stablehlo.rsqrt %149 : tensor<1x7x1xf32>
      %151 = stablehlo.reshape %150 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %152 = stablehlo.broadcast_in_dim %151, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
      %153 = stablehlo.multiply %142, %152 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
      %154 = stablehlo.convert %153 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
      %155 = stablehlo.multiply %141, %154 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
      %156 = stablehlo.reshape %155 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
      %157 = stablehlo.reshape %arg28 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
      %158 = stablehlo.reshape %157 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
      %159 = stablehlo.transpose %158, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
      %160 = stablehlo.dot_general %156, %159, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
      %161 = sdy.all_reduce {"_axis_1"} %160 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x28672xbf16>
      %162 = stablehlo.reshape %161 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
      %163 = stablehlo.logistic %162 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
      %164 = stablehlo.multiply %162, %163 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
      %165 = stablehlo.reshape %arg23 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
      %166 = stablehlo.reshape %165 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
      %167 = stablehlo.transpose %166, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
      %168 = stablehlo.dot_general %156, %167, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
      %169 = sdy.all_reduce {"_axis_1"} %168 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x28672xbf16>
      %170 = stablehlo.reshape %169 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
      %171 = stablehlo.multiply %164, %170 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
      %172 = stablehlo.reshape %171 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
      %173 = stablehlo.reshape %arg22 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
      %174 = stablehlo.reshape %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
      %175 = stablehlo.transpose %174, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
      %176 = stablehlo.dot_general %172, %175, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
      %177 = sdy.all_reduce {"_axis_0"} %176 out_sharding=<@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
      %178 = stablehlo.reshape %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
      %179 = stablehlo.add %137, %178 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
      %180 = stablehlo.convert %179 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
      %181 = stablehlo.power %180, %7 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
      %182 = stablehlo.reduce(%181 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
      %183 = sdy.all_reduce {"_axis_1"} %182 out_sharding=<@mesh, [{}, {}]> : tensor<1x7xf32>
      %184 = stablehlo.multiply %183, %cst_2 : tensor<1x7xf32>
      %185 = stablehlo.reshape %184 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %186 = stablehlo.add %185, %cst_3 : tensor<1x7x1xf32>
      %187 = stablehlo.rsqrt %186 : tensor<1x7x1xf32>
      %188 = stablehlo.reshape %187 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %189 = stablehlo.broadcast_in_dim %188, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
      %190 = stablehlo.multiply %180, %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
      %191 = stablehlo.convert %190 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
      %192 = stablehlo.multiply %71, %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
      %193 = stablehlo.reshape %192 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
      %194 = stablehlo.reshape %arg21 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
      %195 = stablehlo.reshape %194 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
      %196 = stablehlo.transpose %195, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
      %197 = sdy.reshard %193 <@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x8192xbf16>
      %198 = stablehlo.dot_general %197, %196, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
      %199 = sdy.all_reduce {"_axis_0"} %198 out_sharding=<@mesh, [{}, {}]> : tensor<7x128256xbf16>
      %200 = stablehlo.reshape %199 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
      sdy.return %41, %67, %200 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
    } : (tensor<1024x8192xbf16>, tensor<1x7xi64>, tensor<128256x8192xbf16>, tensor<8192xbf16>, tensor<64xf32>, tensor<1024x8192xbf16>, tensor<128256x8192xbf16>, tensor<8192x28672xbf16>, tensor<28672x8192xbf16>, tensor<8192x8192xbf16>, tensor<1x7xi64>, tensor<8192x8192xbf16>, tensor<8192xbf16>, tensor<28672x8192xbf16>, tensor<8192xbf16>) -> (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>)
    return %0#0, %0#1, %0#2 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump After ReshardToCollectivesPass (sdy-reshard-to-collectives) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8, "_axis_1"=4]>
  func.func @main(%arg0: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg1: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg2: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg3: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg4: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg5: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg6: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg7: tensor<8192x28672xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg8: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg9: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg10: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg11: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg12: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg13: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg14: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x7x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x7x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:3 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14) in_shardings=[<@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>] manual_axes={} (%arg15: tensor<1024x8192xbf16>, %arg16: tensor<1x7xi64>, %arg17: tensor<128256x8192xbf16>, %arg18: tensor<8192xbf16>, %arg19: tensor<64xf32>, %arg20: tensor<1024x8192xbf16>, %arg21: tensor<128256x8192xbf16>, %arg22: tensor<8192x28672xbf16>, %arg23: tensor<28672x8192xbf16>, %arg24: tensor<8192x8192xbf16>, %arg25: tensor<1x7xi64>, %arg26: tensor<8192x8192xbf16>, %arg27: tensor<8192xbf16>, %arg28: tensor<28672x8192xbf16>, %arg29: tensor<8192xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
      %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
      %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
      %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
      %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
      %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
      %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
      %c_6 = stablehlo.constant dense<1> : tensor<i64>
      %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
      %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
      %3 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
      %4 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
      %5 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
      %6 = stablehlo.broadcast_in_dim %cst_5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
      %7 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
      %8 = sdy.all_slice [{"_axis_0"}] %arg18 out_sharding=<@mesh, [{"_axis_0"}]> : tensor<8192xbf16>
      %9 = stablehlo.reshape %8 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
      %10 = stablehlo.reshape %9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
      %11 = stablehlo.broadcast_in_dim %10, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
      %12 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
      %13 = stablehlo.reshape %12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
      %14 = stablehlo.reshape %arg16 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
      %15 = stablehlo.reshape %14 : (tensor<1x1x7xi64>) -> tensor<7xi64>
      %16 = stablehlo.convert %15 : (tensor<7xi64>) -> tensor<7xui32>
      %17 = "stablehlo.gather"(%13, %16) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
      %18 = stablehlo.reshape %17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
      %19 = stablehlo.convert %18 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
      %20 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
      %21 = stablehlo.power %19, %20 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
      %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
      %23 = sdy.all_reduce {"_axis_0"} %22 out_sharding=<@mesh, [{}, {}]> : tensor<1x7xf32>
      %24 = stablehlo.multiply %23, %cst_2 : tensor<1x7xf32>
      %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %26 = stablehlo.add %25, %cst_3 : tensor<1x7x1xf32>
      %27 = stablehlo.rsqrt %26 : tensor<1x7x1xf32>
      %28 = stablehlo.reshape %27 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %29 = stablehlo.broadcast_in_dim %28, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
      %30 = stablehlo.multiply %19, %29 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
      %31 = stablehlo.convert %30 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
      %32 = stablehlo.multiply %11, %31 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xbf16>
      %33 = stablehlo.reshape %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
      %34 = stablehlo.reshape %arg15 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
      %35 = stablehlo.reshape %34 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
      %36 = stablehlo.transpose %35, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
      %37 = sdy.collective_permute %33 out_sharding=<@mesh, [{}, {"_axis_1", "_axis_0":(4)2}]> : tensor<7x8192xbf16>
      %38 = sdy.all_gather [{}, {"_axis_0":(4)2}] %37 out_sharding=<@mesh, [{}, {"_axis_1"}]> : tensor<7x8192xbf16>
      %39 = stablehlo.dot_general %38, %36, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
      %40 = sdy.all_reduce {"_axis_1"} %39 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x1024xbf16>
      %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
      %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
      %43 = stablehlo.reshape %arg20 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
      %44 = stablehlo.reshape %43 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
      %45 = stablehlo.transpose %44, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
      %46 = sdy.collective_permute %33 out_sharding=<@mesh, [{}, {"_axis_1", "_axis_0":(4)2}]> : tensor<7x8192xbf16>
      %47 = sdy.all_gather [{}, {"_axis_0":(4)2}] %46 out_sharding=<@mesh, [{}, {"_axis_1"}]> : tensor<7x8192xbf16>
      %48 = stablehlo.dot_general %47, %45, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
      %49 = sdy.all_reduce {"_axis_1"} %48 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x1024xbf16>
      %50 = stablehlo.reshape %49 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
      %51 = stablehlo.transpose %50, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
      %52 = stablehlo.reshape %arg19 : (tensor<64xf32>) -> tensor<1x1x64xf32>
      %53 = stablehlo.reshape %52 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
      %54 = stablehlo.dot_general %53, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
      %55 = stablehlo.transpose %54, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
      %56 = stablehlo.concatenate %55, %55, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
      %57 = stablehlo.cosine %56 : tensor<1x7x128xf32>
      %58 = stablehlo.convert %57 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
      %59 = stablehlo.broadcast_in_dim %58, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
      %60 = stablehlo.multiply %51, %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
      %61 = stablehlo.slice %51 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
      %62 = stablehlo.negate %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x64xbf16>
      %63 = stablehlo.slice %51 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
      %64 = stablehlo.concatenate %62, %63, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
      %65 = stablehlo.sine %56 : tensor<1x7x128xf32>
      %66 = stablehlo.convert %65 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
      %67 = stablehlo.broadcast_in_dim %66, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
      %68 = stablehlo.multiply %64, %67 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
      %69 = stablehlo.add %60, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
      %70 = sdy.all_slice [{"_axis_1"}] %arg29 out_sharding=<@mesh, [{"_axis_1"}]> : tensor<8192xbf16>
      %71 = stablehlo.reshape %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
      %72 = stablehlo.reshape %71 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
      %73 = stablehlo.broadcast_in_dim %72, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
      %74 = stablehlo.reshape %arg26 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
      %75 = stablehlo.reshape %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
      %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
      %77 = sdy.collective_permute %33 out_sharding=<@mesh, [{}, {"_axis_1", "_axis_0":(4)2}]> : tensor<7x8192xbf16>
      %78 = sdy.all_gather [{}, {"_axis_0":(4)2}] %77 out_sharding=<@mesh, [{}, {"_axis_1"}]> : tensor<7x8192xbf16>
      %79 = stablehlo.dot_general %78, %76, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
      %80 = sdy.all_reduce {"_axis_1"} %79 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x8192xbf16>
      %81 = stablehlo.reshape %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
      %82 = stablehlo.transpose %81, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
      %83 = stablehlo.broadcast_in_dim %58, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
      %84 = stablehlo.multiply %82, %83 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
      %85 = stablehlo.slice %82 [0:1, 0:64, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
      %86 = stablehlo.negate %85 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x64xbf16>
      %87 = stablehlo.slice %82 [0:1, 0:64, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
      %88 = stablehlo.concatenate %86, %87, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
      %89 = stablehlo.broadcast_in_dim %66, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
      %90 = stablehlo.multiply %88, %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
      %91 = stablehlo.add %84, %90 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
      %92 = stablehlo.broadcast_in_dim %69, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
      %93 = stablehlo.reshape %92 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
      %94 = stablehlo.transpose %93, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
      %95 = stablehlo.dot_general %91, %94, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
      %96 = stablehlo.multiply %95, %6 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xbf16>
      %97 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
      %98 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
      %99 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
      %100 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
      %101 = stablehlo.subtract %99, %100 : tensor<7x7xi64>
      %102 = stablehlo.compare  GE, %101, %5 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
      %103 = stablehlo.select %102, %4, %3 : tensor<7x7xi1>, tensor<7x7xbf16>
      %104 = stablehlo.compare  GT, %97, %98 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
      %105 = stablehlo.convert %104 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
      %106 = stablehlo.multiply %103, %105 : tensor<7x7xbf16>
      %107 = stablehlo.reshape %106 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
      %108 = stablehlo.reshape %arg25 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
      %109 = stablehlo.reshape %108 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
      %110 = stablehlo.convert %109 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
      %111 = stablehlo.reshape %110 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
      %112 = stablehlo.broadcast_in_dim %111, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
      %113 = stablehlo.add %107, %112 : tensor<1x1x7x7xbf16>
      %114 = stablehlo.compare  EQ, %113, %2 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
      %115 = stablehlo.select %114, %1, %107 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
      %116 = stablehlo.reshape %115 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
      %117 = stablehlo.broadcast_in_dim %116, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
      %118 = stablehlo.add %96, %117 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xbf16>
      %119 = stablehlo.convert %118 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
      %120 = stablehlo.reduce(%119 init: %cst_0) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
      %121 = stablehlo.broadcast_in_dim %120, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
      %122 = stablehlo.subtract %119, %121 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
      %123 = stablehlo.exponential %122 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
      %124 = stablehlo.reduce(%123 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
      %125 = stablehlo.broadcast_in_dim %124, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
      %126 = stablehlo.divide %123, %125 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
      %127 = stablehlo.convert %126 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
      %128 = stablehlo.broadcast_in_dim %42, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
      %129 = stablehlo.reshape %128 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
      %130 = stablehlo.dot_general %127, %129, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
      %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
      %132 = stablehlo.reshape %131 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
      %133 = stablehlo.reshape %arg24 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
      %134 = stablehlo.reshape %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
      %135 = stablehlo.transpose %134, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
      %136 = stablehlo.dot_general %132, %135, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
      %137 = sdy.all_reduce {"_axis_0"} %136 out_sharding=<@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
      %138 = stablehlo.reshape %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
      %139 = sdy.collective_permute %18 out_sharding=<@mesh, [{}, {}, {"_axis_1", "_axis_0":(4)2}]> : tensor<1x7x8192xbf16>
      %140 = sdy.all_gather [{}, {}, {"_axis_0":(4)2}] %139 out_sharding=<@mesh, [{}, {}, {"_axis_1"}]> : tensor<1x7x8192xbf16>
      %141 = stablehlo.add %140, %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
      %142 = sdy.all_slice [{"_axis_1"}] %arg27 out_sharding=<@mesh, [{"_axis_1"}]> : tensor<8192xbf16>
      %143 = stablehlo.reshape %142 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
      %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
      %145 = stablehlo.broadcast_in_dim %144, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
      %146 = stablehlo.convert %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
      %147 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
      %148 = stablehlo.power %146, %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
      %149 = stablehlo.reduce(%148 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
      %150 = sdy.all_reduce {"_axis_1"} %149 out_sharding=<@mesh, [{}, {}]> : tensor<1x7xf32>
      %151 = stablehlo.multiply %150, %cst_2 : tensor<1x7xf32>
      %152 = stablehlo.reshape %151 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %153 = stablehlo.add %152, %cst_3 : tensor<1x7x1xf32>
      %154 = stablehlo.rsqrt %153 : tensor<1x7x1xf32>
      %155 = stablehlo.reshape %154 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %156 = stablehlo.broadcast_in_dim %155, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
      %157 = stablehlo.multiply %146, %156 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
      %158 = stablehlo.convert %157 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
      %159 = stablehlo.multiply %145, %158 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
      %160 = stablehlo.reshape %159 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
      %161 = stablehlo.reshape %arg28 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
      %162 = stablehlo.reshape %161 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
      %163 = stablehlo.transpose %162, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
      %164 = stablehlo.dot_general %160, %163, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
      %165 = sdy.all_reduce {"_axis_1"} %164 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x28672xbf16>
      %166 = stablehlo.reshape %165 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
      %167 = stablehlo.logistic %166 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
      %168 = stablehlo.multiply %166, %167 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
      %169 = stablehlo.reshape %arg23 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
      %170 = stablehlo.reshape %169 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
      %171 = stablehlo.transpose %170, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
      %172 = stablehlo.dot_general %160, %171, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
      %173 = sdy.all_reduce {"_axis_1"} %172 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x28672xbf16>
      %174 = stablehlo.reshape %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
      %175 = stablehlo.multiply %168, %174 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
      %176 = stablehlo.reshape %175 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
      %177 = stablehlo.reshape %arg22 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
      %178 = stablehlo.reshape %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
      %179 = stablehlo.transpose %178, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
      %180 = stablehlo.dot_general %176, %179, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
      %181 = sdy.all_reduce {"_axis_0"} %180 out_sharding=<@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
      %182 = stablehlo.reshape %181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
      %183 = stablehlo.add %141, %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
      %184 = stablehlo.convert %183 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
      %185 = stablehlo.power %184, %7 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
      %186 = stablehlo.reduce(%185 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
      %187 = sdy.all_reduce {"_axis_1"} %186 out_sharding=<@mesh, [{}, {}]> : tensor<1x7xf32>
      %188 = stablehlo.multiply %187, %cst_2 : tensor<1x7xf32>
      %189 = stablehlo.reshape %188 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %190 = stablehlo.add %189, %cst_3 : tensor<1x7x1xf32>
      %191 = stablehlo.rsqrt %190 : tensor<1x7x1xf32>
      %192 = stablehlo.reshape %191 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %193 = stablehlo.broadcast_in_dim %192, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
      %194 = stablehlo.multiply %184, %193 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
      %195 = stablehlo.convert %194 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
      %196 = stablehlo.multiply %73, %195 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
      %197 = stablehlo.reshape %196 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
      %198 = stablehlo.reshape %arg21 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
      %199 = stablehlo.reshape %198 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
      %200 = stablehlo.transpose %199, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
      %201 = sdy.all_slice [{}, {"_axis_0":(1)2}] %197 out_sharding=<@mesh, [{}, {"_axis_1", "_axis_0":(1)2}]> : tensor<7x8192xbf16>
      %202 = sdy.collective_permute %201 out_sharding=<@mesh, [{}, {"_axis_0"}]> : tensor<7x8192xbf16>
      %203 = stablehlo.dot_general %202, %200, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
      %204 = sdy.all_reduce {"_axis_0"} %203 out_sharding=<@mesh, [{}, {}]> : tensor<7x128256xbf16>
      %205 = stablehlo.reshape %204 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
      sdy.return %42, %69, %205 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
    } : (tensor<1024x8192xbf16>, tensor<1x7xi64>, tensor<128256x8192xbf16>, tensor<8192xbf16>, tensor<64xf32>, tensor<1024x8192xbf16>, tensor<128256x8192xbf16>, tensor<8192x28672xbf16>, tensor<28672x8192xbf16>, tensor<8192x8192xbf16>, tensor<1x7xi64>, tensor<8192x8192xbf16>, tensor<8192xbf16>, tensor<28672x8192xbf16>, tensor<8192xbf16>) -> (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>)
    return %0#0, %0#1, %0#2 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


// -----// IR Dump Before UpdateGlobalToLocalShapesPass (update-global-to-local-shapes) ('builtin.module' operation: @SyncTensorsGraph.406) //----- //
module @SyncTensorsGraph.406 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=8, "_axis_1"=4]>
  func.func @main(%arg0: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, %arg1: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg2: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, %arg3: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, %arg4: tensor<64xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, %arg5: tensor<1024x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, %arg6: tensor<128256x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, %arg7: tensor<8192x28672xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, %arg8: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, %arg9: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_1"}, {"_axis_0"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, %arg10: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg11: tensor<8192x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, %arg12: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, %arg13: tensor<28672x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {"_axis_1"}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, %arg14: tensor<8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}) -> (tensor<1x8x7x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x7x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:3 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14) in_shardings=[<@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>] out_shardings=[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>] manual_axes={} (%arg15: tensor<1024x8192xbf16>, %arg16: tensor<1x7xi64>, %arg17: tensor<128256x8192xbf16>, %arg18: tensor<8192xbf16>, %arg19: tensor<64xf32>, %arg20: tensor<1024x8192xbf16>, %arg21: tensor<128256x8192xbf16>, %arg22: tensor<8192x28672xbf16>, %arg23: tensor<28672x8192xbf16>, %arg24: tensor<8192x8192xbf16>, %arg25: tensor<1x7xi64>, %arg26: tensor<8192x8192xbf16>, %arg27: tensor<8192xbf16>, %arg28: tensor<28672x8192xbf16>, %arg29: tensor<8192xbf16>) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
      %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>
      %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
      %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %cst_2 = stablehlo.constant dense<1.22070313E-4> : tensor<1x7xf32>
      %cst_3 = stablehlo.constant dense<9.99999974E-6> : tensor<1x7x1xf32>
      %cst_4 = stablehlo.constant dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>
      %cst_5 = stablehlo.constant dense<8.837890e-02> : tensor<bf16>
      %c_6 = stablehlo.constant dense<1> : tensor<i64>
      %cst_7 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
      %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
      %1 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
      %2 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
      %3 = stablehlo.broadcast_in_dim %cst_7, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
      %4 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<7x7xbf16>
      %5 = stablehlo.broadcast_in_dim %c_6, dims = [] : (tensor<i64>) -> tensor<7x7xi64>
      %6 = stablehlo.broadcast_in_dim %cst_5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<1x64x7x7xbf16>
      %7 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
      %8 = sdy.all_slice [{"_axis_0"}] %arg18 out_sharding=<@mesh, [{"_axis_0"}]> : tensor<8192xbf16>
      %9 = stablehlo.reshape %8 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
      %10 = stablehlo.reshape %9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
      %11 = stablehlo.broadcast_in_dim %10, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
      %12 = stablehlo.reshape %arg17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
      %13 = stablehlo.reshape %12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
      %14 = stablehlo.reshape %arg16 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
      %15 = stablehlo.reshape %14 : (tensor<1x1x7xi64>) -> tensor<7xi64>
      %16 = stablehlo.convert %15 : (tensor<7xi64>) -> tensor<7xui32>
      %17 = "stablehlo.gather"(%13, %16) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 8192>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>, tensor<7xui32>) -> tensor<7x8192xbf16>
      %18 = stablehlo.reshape %17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
      %19 = stablehlo.convert %18 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
      %20 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
      %21 = stablehlo.power %19, %20 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
      %22 = stablehlo.reduce(%21 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
      %23 = sdy.all_reduce {"_axis_0"} %22 out_sharding=<@mesh, [{}, {}]> : tensor<1x7xf32>
      %24 = stablehlo.multiply %23, %cst_2 : tensor<1x7xf32>
      %25 = stablehlo.reshape %24 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %26 = stablehlo.add %25, %cst_3 : tensor<1x7x1xf32>
      %27 = stablehlo.rsqrt %26 : tensor<1x7x1xf32>
      %28 = stablehlo.reshape %27 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %29 = stablehlo.broadcast_in_dim %28, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
      %30 = stablehlo.multiply %19, %29 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xf32>
      %31 = stablehlo.convert %30 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
      %32 = stablehlo.multiply %11, %31 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x8192xbf16>
      %33 = stablehlo.reshape %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
      %34 = stablehlo.reshape %arg15 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
      %35 = stablehlo.reshape %34 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
      %36 = stablehlo.transpose %35, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
      %37 = sdy.collective_permute %33 out_sharding=<@mesh, [{}, {"_axis_1", "_axis_0":(4)2}]> : tensor<7x8192xbf16>
      %38 = sdy.all_gather [{}, {"_axis_0":(4)2}] %37 out_sharding=<@mesh, [{}, {"_axis_1"}]> : tensor<7x8192xbf16>
      %39 = stablehlo.dot_general %38, %36, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
      %40 = sdy.all_reduce {"_axis_1"} %39 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x1024xbf16>
      %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
      %42 = stablehlo.transpose %41, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
      %43 = stablehlo.reshape %arg20 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1024x8192xbf16>) -> tensor<1x1024x8192xbf16>
      %44 = stablehlo.reshape %43 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x1024x8192xbf16>) -> tensor<1024x8192xbf16>
      %45 = stablehlo.transpose %44, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<1024x8192xbf16>) -> tensor<8192x1024xbf16>
      %46 = sdy.collective_permute %33 out_sharding=<@mesh, [{}, {"_axis_1", "_axis_0":(4)2}]> : tensor<7x8192xbf16>
      %47 = sdy.all_gather [{}, {"_axis_0":(4)2}] %46 out_sharding=<@mesh, [{}, {"_axis_1"}]> : tensor<7x8192xbf16>
      %48 = stablehlo.dot_general %47, %45, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x1024xbf16>) -> tensor<7x1024xbf16>
      %49 = sdy.all_reduce {"_axis_1"} %48 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x1024xbf16>
      %50 = stablehlo.reshape %49 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
      %51 = stablehlo.transpose %50, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
      %52 = stablehlo.reshape %arg19 : (tensor<64xf32>) -> tensor<1x1x64xf32>
      %53 = stablehlo.reshape %52 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
      %54 = stablehlo.dot_general %53, %cst_4, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
      %55 = stablehlo.transpose %54, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
      %56 = stablehlo.concatenate %55, %55, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
      %57 = stablehlo.cosine %56 : tensor<1x7x128xf32>
      %58 = stablehlo.convert %57 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
      %59 = stablehlo.broadcast_in_dim %58, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
      %60 = stablehlo.multiply %51, %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
      %61 = stablehlo.slice %51 [0:1, 0:8, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
      %62 = stablehlo.negate %61 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x64xbf16>
      %63 = stablehlo.slice %51 [0:1, 0:8, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
      %64 = stablehlo.concatenate %62, %63, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
      %65 = stablehlo.sine %56 : tensor<1x7x128xf32>
      %66 = stablehlo.convert %65 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
      %67 = stablehlo.broadcast_in_dim %66, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
      %68 = stablehlo.multiply %64, %67 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
      %69 = stablehlo.add %60, %68 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x8x7x128xbf16>
      %70 = sdy.all_slice [{"_axis_1"}] %arg29 out_sharding=<@mesh, [{"_axis_1"}]> : tensor<8192xbf16>
      %71 = stablehlo.reshape %70 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
      %72 = stablehlo.reshape %71 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
      %73 = stablehlo.broadcast_in_dim %72, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
      %74 = stablehlo.reshape %arg26 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
      %75 = stablehlo.reshape %74 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
      %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
      %77 = sdy.collective_permute %33 out_sharding=<@mesh, [{}, {"_axis_1", "_axis_0":(4)2}]> : tensor<7x8192xbf16>
      %78 = sdy.all_gather [{}, {"_axis_0":(4)2}] %77 out_sharding=<@mesh, [{}, {"_axis_1"}]> : tensor<7x8192xbf16>
      %79 = stablehlo.dot_general %78, %76, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
      %80 = sdy.all_reduce {"_axis_1"} %79 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x8192xbf16>
      %81 = stablehlo.reshape %80 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x64x128xbf16>
      %82 = stablehlo.transpose %81, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x64x128xbf16>) -> tensor<1x64x7x128xbf16>
      %83 = stablehlo.broadcast_in_dim %58, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
      %84 = stablehlo.multiply %82, %83 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
      %85 = stablehlo.slice %82 [0:1, 0:64, 0:7, 64:128] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
      %86 = stablehlo.negate %85 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x64xbf16>
      %87 = stablehlo.slice %82 [0:1, 0:64, 0:7, 0:64] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x7x64xbf16>
      %88 = stablehlo.concatenate %86, %87, dim = 3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x64xbf16>, tensor<1x64x7x64xbf16>) -> tensor<1x64x7x128xbf16>
      %89 = stablehlo.broadcast_in_dim %66, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x128xbf16>) -> tensor<1x64x7x128xbf16>
      %90 = stablehlo.multiply %88, %89 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
      %91 = stablehlo.add %84, %90 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x128xbf16>
      %92 = stablehlo.broadcast_in_dim %69, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
      %93 = stablehlo.reshape %92 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
      %94 = stablehlo.transpose %93, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x64x128x7xbf16>
      %95 = stablehlo.dot_general %91, %94, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x128xbf16>, tensor<1x64x128x7xbf16>) -> tensor<1x64x7x7xbf16>
      %96 = stablehlo.multiply %95, %6 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xbf16>
      %97 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
      %98 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
      %99 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<7xi64>) -> tensor<7x7xi64>
      %100 = stablehlo.broadcast_in_dim %c, dims = [0] : (tensor<7xi64>) -> tensor<7x7xi64>
      %101 = stablehlo.subtract %99, %100 : tensor<7x7xi64>
      %102 = stablehlo.compare  GE, %101, %5 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
      %103 = stablehlo.select %102, %4, %3 : tensor<7x7xi1>, tensor<7x7xbf16>
      %104 = stablehlo.compare  GT, %97, %98 : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
      %105 = stablehlo.convert %104 : (tensor<7x7xi1>) -> tensor<7x7xbf16>
      %106 = stablehlo.multiply %103, %105 : tensor<7x7xbf16>
      %107 = stablehlo.reshape %106 : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
      %108 = stablehlo.reshape %arg25 : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
      %109 = stablehlo.reshape %108 : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
      %110 = stablehlo.convert %109 : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
      %111 = stablehlo.reshape %110 : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
      %112 = stablehlo.broadcast_in_dim %111, dims = [0, 1, 3] : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
      %113 = stablehlo.add %107, %112 : tensor<1x1x7x7xbf16>
      %114 = stablehlo.compare  EQ, %113, %2 : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
      %115 = stablehlo.select %114, %1, %107 : tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>
      %116 = stablehlo.reshape %115 : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
      %117 = stablehlo.broadcast_in_dim %116, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x7x7xbf16>) -> tensor<1x64x7x7xbf16>
      %118 = stablehlo.add %96, %117 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xbf16>
      %119 = stablehlo.convert %118 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xbf16>) -> tensor<1x64x7x7xf32>
      %120 = stablehlo.reduce(%119 init: %cst_0) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
      %121 = stablehlo.broadcast_in_dim %120, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
      %122 = stablehlo.subtract %119, %121 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
      %123 = stablehlo.exponential %122 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
      %124 = stablehlo.reduce(%123 init: %cst) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<1x64x7x7xf32>, tensor<f32>) -> tensor<1x64x7xf32>
      %125 = stablehlo.broadcast_in_dim %124, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7xf32>) -> tensor<1x64x7x7xf32>
      %126 = stablehlo.divide %123, %125 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : tensor<1x64x7x7xf32>
      %127 = stablehlo.convert %126 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xf32>) -> tensor<1x64x7x7xbf16>
      %128 = stablehlo.broadcast_in_dim %42, dims = [0, 1, 3, 4] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x8x7x128xbf16>
      %129 = stablehlo.reshape %128 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x8x8x7x128xbf16>) -> tensor<1x64x7x128xbf16>
      %130 = stablehlo.dot_general %127, %129, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>]>} : (tensor<1x64x7x7xbf16>, tensor<1x64x7x128xbf16>) -> tensor<1x64x7x128xbf16>
      %131 = stablehlo.transpose %130, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x64x7x128xbf16>) -> tensor<1x7x64x128xbf16>
      %132 = stablehlo.reshape %131 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x64x128xbf16>) -> tensor<7x8192xbf16>
      %133 = stablehlo.reshape %arg24 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<8192x8192xbf16>) -> tensor<1x8192x8192xbf16>
      %134 = stablehlo.reshape %133 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<1x8192x8192xbf16>) -> tensor<8192x8192xbf16>
      %135 = stablehlo.transpose %134, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<8192x8192xbf16>) -> tensor<8192x8192xbf16>
      %136 = stablehlo.dot_general %132, %135, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x8192xbf16>) -> tensor<7x8192xbf16>
      %137 = sdy.all_reduce {"_axis_0"} %136 out_sharding=<@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
      %138 = stablehlo.reshape %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
      %139 = sdy.collective_permute %18 out_sharding=<@mesh, [{}, {}, {"_axis_1", "_axis_0":(4)2}]> : tensor<1x7x8192xbf16>
      %140 = sdy.all_gather [{}, {}, {"_axis_0":(4)2}] %139 out_sharding=<@mesh, [{}, {}, {"_axis_1"}]> : tensor<1x7x8192xbf16>
      %141 = stablehlo.add %140, %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
      %142 = sdy.all_slice [{"_axis_1"}] %arg27 out_sharding=<@mesh, [{"_axis_1"}]> : tensor<8192xbf16>
      %143 = stablehlo.reshape %142 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x1x8192xbf16>
      %144 = stablehlo.reshape %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}]>]>} : (tensor<1x1x8192xbf16>) -> tensor<8192xbf16>
      %145 = stablehlo.broadcast_in_dim %144, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<8192xbf16>) -> tensor<1x7x8192xbf16>
      %146 = stablehlo.convert %141 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
      %147 = stablehlo.broadcast_in_dim %cst_1, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<f32>) -> tensor<1x7x8192xf32>
      %148 = stablehlo.power %146, %147 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
      %149 = stablehlo.reduce(%148 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
      %150 = sdy.all_reduce {"_axis_1"} %149 out_sharding=<@mesh, [{}, {}]> : tensor<1x7xf32>
      %151 = stablehlo.multiply %150, %cst_2 : tensor<1x7xf32>
      %152 = stablehlo.reshape %151 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %153 = stablehlo.add %152, %cst_3 : tensor<1x7x1xf32>
      %154 = stablehlo.rsqrt %153 : tensor<1x7x1xf32>
      %155 = stablehlo.reshape %154 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %156 = stablehlo.broadcast_in_dim %155, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
      %157 = stablehlo.multiply %146, %156 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
      %158 = stablehlo.convert %157 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
      %159 = stablehlo.multiply %145, %158 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
      %160 = stablehlo.reshape %159 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
      %161 = stablehlo.reshape %arg28 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
      %162 = stablehlo.reshape %161 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
      %163 = stablehlo.transpose %162, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
      %164 = stablehlo.dot_general %160, %163, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
      %165 = sdy.all_reduce {"_axis_1"} %164 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x28672xbf16>
      %166 = stablehlo.reshape %165 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
      %167 = stablehlo.logistic %166 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
      %168 = stablehlo.multiply %166, %167 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
      %169 = stablehlo.reshape %arg23 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<28672x8192xbf16>) -> tensor<1x28672x8192xbf16>
      %170 = stablehlo.reshape %169 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>} : (tensor<1x28672x8192xbf16>) -> tensor<28672x8192xbf16>
      %171 = stablehlo.transpose %170, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<28672x8192xbf16>) -> tensor<8192x28672xbf16>
      %172 = stablehlo.dot_general %160, %171, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<7x8192xbf16>, tensor<8192x28672xbf16>) -> tensor<7x28672xbf16>
      %173 = sdy.all_reduce {"_axis_1"} %172 out_sharding=<@mesh, [{?}, {"_axis_0", ?}]> : tensor<7x28672xbf16>
      %174 = stablehlo.reshape %173 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<7x28672xbf16>) -> tensor<1x7x28672xbf16>
      %175 = stablehlo.multiply %168, %174 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : tensor<1x7x28672xbf16>
      %176 = stablehlo.reshape %175 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x7x28672xbf16>) -> tensor<7x28672xbf16>
      %177 = stablehlo.reshape %arg22 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<8192x28672xbf16>) -> tensor<1x8192x28672xbf16>
      %178 = stablehlo.reshape %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_1", ?}, {"_axis_0", ?}]>]>} : (tensor<1x8192x28672xbf16>) -> tensor<8192x28672xbf16>
      %179 = stablehlo.transpose %178, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {"_axis_1", ?}]>]>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<8192x28672xbf16>) -> tensor<28672x8192xbf16>
      %180 = stablehlo.dot_general %176, %179, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<7x28672xbf16>, tensor<28672x8192xbf16>) -> tensor<7x8192xbf16>
      %181 = sdy.all_reduce {"_axis_0"} %180 out_sharding=<@mesh, [{?}, {"_axis_1", ?}]> : tensor<7x8192xbf16>
      %182 = stablehlo.reshape %181 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
      %183 = stablehlo.add %141, %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
      %184 = stablehlo.convert %183 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
      %185 = stablehlo.power %184, %7 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
      %186 = stablehlo.reduce(%185 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x8192xf32>, tensor<f32>) -> tensor<1x7xf32>
      %187 = sdy.all_reduce {"_axis_1"} %186 out_sharding=<@mesh, [{}, {}]> : tensor<1x7xf32>
      %188 = stablehlo.multiply %187, %cst_2 : tensor<1x7xf32>
      %189 = stablehlo.reshape %188 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %190 = stablehlo.add %189, %cst_3 : tensor<1x7x1xf32>
      %191 = stablehlo.rsqrt %190 : tensor<1x7x1xf32>
      %192 = stablehlo.reshape %191 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %193 = stablehlo.broadcast_in_dim %192, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7xf32>) -> tensor<1x7x8192xf32>
      %194 = stablehlo.multiply %184, %193 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xf32>
      %195 = stablehlo.convert %194 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
      %196 = stablehlo.multiply %73, %195 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_1", ?}]>]>} : tensor<1x7x8192xbf16>
      %197 = stablehlo.reshape %196 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_1", ?}]>]>} : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
      %198 = stablehlo.reshape %arg21 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {?}, {"_axis_0", ?}]>]>} : (tensor<128256x8192xbf16>) -> tensor<1x128256x8192xbf16>
      %199 = stablehlo.reshape %198 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}]>]>} : (tensor<1x128256x8192xbf16>) -> tensor<128256x8192xbf16>
      %200 = stablehlo.transpose %199, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x8192xbf16>) -> tensor<8192x128256xbf16>
      %201 = sdy.all_slice [{}, {"_axis_0":(1)2}] %197 out_sharding=<@mesh, [{}, {"_axis_1", "_axis_0":(1)2}]> : tensor<7x8192xbf16>
      %202 = sdy.collective_permute %201 out_sharding=<@mesh, [{}, {"_axis_0"}]> : tensor<7x8192xbf16>
      %203 = stablehlo.dot_general %202, %200, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x128256xbf16>) -> tensor<7x128256xbf16>
      %204 = sdy.all_reduce {"_axis_0"} %203 out_sharding=<@mesh, [{}, {}]> : tensor<7x128256xbf16>
      %205 = stablehlo.reshape %204 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
      sdy.return %42, %69, %205 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
    } : (tensor<1024x8192xbf16>, tensor<1x7xi64>, tensor<128256x8192xbf16>, tensor<8192xbf16>, tensor<64xf32>, tensor<1024x8192xbf16>, tensor<128256x8192xbf16>, tensor<8192x28672xbf16>, tensor<28672x8192xbf16>, tensor<8192x8192xbf16>, tensor<1x7xi64>, tensor<8192x8192xbf16>, tensor<8192xbf16>, tensor<28672x8192xbf16>, tensor<8192xbf16>) -> (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>)
    return %0#0, %0#1, %0#2 : tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>
  }
}


loc("reshape.56"): error: ShardyToStableHLO lowering for CollectivePermuteOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3370.
loc("reshape.56"): error: ShardyToStableHLO lowering for CollectivePermuteOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3370.
loc("reshape.56"): error: ShardyToStableHLO lowering for CollectivePermuteOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3370.
loc("reshape.20"): error: ShardyToStableHLO lowering for CollectivePermuteOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3370.
loc("reshape.402"): error: ShardyToStableHLO lowering for CollectivePermuteOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3370.
loc("reshape.56"): error: ShardyToStableHLO lowering for CollectivePermuteOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3370.
loc("reshape.56"): error: ShardyToStableHLO lowering for CollectivePermuteOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3370.
loc("reshape.56"): error: ShardyToStableHLO lowering for CollectivePermuteOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3370.
loc("reshape.20"): error: ShardyToStableHLO lowering for CollectivePermuteOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3370.
loc("reshape.402"): error: ShardyToStableHLO lowering for CollectivePermuteOp is not implemented yet: https://github.com/tenstorrent/tt-mlir/issues/3370.
loc("reshape.56"): error: 'sdy.collective_permute' op requires the same type for all operands and results
// -----// IR Dump After UpdateGlobalToLocalShapesPass Failed (update-global-to-local-shapes) ('builtin.module' operation: @SyncTensorsGraph.406) //----- //
"builtin.module"() <{sym_name = "SyncTensorsGraph.406"}> ({
  "sdy.mesh"() <{mesh = #sdy.mesh<["_axis_0"=8, "_axis_1"=4]>, sym_name = "mesh"}> : () -> ()
  "func.func"() <{arg_attrs = [{ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"}, {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"}, {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"}, {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"}, {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"}, {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"}, {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"}, {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"}, {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"}, {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"}, {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"}, {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"}, {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"}], function_type = (tensor<1024x8192xbf16>, tensor<1x7xi64>, tensor<128256x8192xbf16>, tensor<8192xbf16>, tensor<64xf32>, tensor<1024x8192xbf16>, tensor<128256x8192xbf16>, tensor<8192x28672xbf16>, tensor<28672x8192xbf16>, tensor<8192x8192xbf16>, tensor<1x7xi64>, tensor<8192x8192xbf16>, tensor<8192xbf16>, tensor<28672x8192xbf16>, tensor<8192xbf16>) -> (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>), res_attrs = [{ttcore.shard_status = #ttcore.shard_status<unsharded>}, {ttcore.shard_status = #ttcore.shard_status<unsharded>}, {ttcore.shard_status = #ttcore.shard_status<unsharded>}], sym_name = "main"}> ({
  ^bb0(%arg0: tensor<1024x8192xbf16>, %arg1: tensor<1x7xi64>, %arg2: tensor<128256x8192xbf16>, %arg3: tensor<8192xbf16>, %arg4: tensor<64xf32>, %arg5: tensor<1024x8192xbf16>, %arg6: tensor<128256x8192xbf16>, %arg7: tensor<8192x28672xbf16>, %arg8: tensor<28672x8192xbf16>, %arg9: tensor<8192x8192xbf16>, %arg10: tensor<1x7xi64>, %arg11: tensor<8192x8192xbf16>, %arg12: tensor<8192xbf16>, %arg13: tensor<28672x8192xbf16>, %arg14: tensor<8192xbf16>):
    %0:3 = "sdy.manual_computation"(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14) <{in_shardings = #sdy.sharding_per_value<[<@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{"_axis_1"}, {"_axis_0"}]>, <@mesh, [{}, {}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {"_axis_1"}]>, <@mesh, [{}]>]>, manual_axes = #sdy<manual_axes{"_axis_0", "_axis_1"}>, out_shardings = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, <@mesh, [{?}, {"_axis_0", ?}, {?}, {?}]>, <@mesh, [{?}, {?}, {?}]>]>}> ({
    ^bb0(%arg15: tensor<128x2048xbf16>, %arg16: tensor<1x7xi64>, %arg17: tensor<128256x1024xbf16>, %arg18: tensor<8192xbf16>, %arg19: tensor<64xf32>, %arg20: tensor<128x2048xbf16>, %arg21: tensor<128256x1024xbf16>, %arg22: tensor<2048x3584xbf16>, %arg23: tensor<3584x2048xbf16>, %arg24: tensor<2048x1024xbf16>, %arg25: tensor<1x7xi64>, %arg26: tensor<1024x2048xbf16>, %arg27: tensor<8192xbf16>, %arg28: tensor<3584x2048xbf16>, %arg29: tensor<8192xbf16>):
      %1 = "stablehlo.constant"() <{value = dense<0.000000e+00> : tensor<f32>}> : () -> tensor<f32>
      %2 = "stablehlo.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6]> : tensor<7xi64>}> : () -> tensor<7xi64>
      %3 = "stablehlo.constant"() <{value = dense<0xFF800000> : tensor<f32>}> : () -> tensor<f32>
      %4 = "stablehlo.constant"() <{value = dense<2.000000e+00> : tensor<f32>}> : () -> tensor<f32>
      %5 = "stablehlo.constant"() <{value = dense<1.22070313E-4> : tensor<1x7xf32>}> : () -> tensor<1x7xf32>
      %6 = "stablehlo.constant"() <{value = dense<9.99999974E-6> : tensor<1x7x1xf32>}> : () -> tensor<1x7x1xf32>
      %7 = "stablehlo.constant"() <{value = dense<[[[0.000000e+00, 1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]]]> : tensor<1x1x7xf32>}> : () -> tensor<1x1x7xf32>
      %8 = "stablehlo.constant"() <{value = dense<8.837890e-02> : tensor<bf16>}> : () -> tensor<bf16>
      %9 = "stablehlo.constant"() <{value = dense<1> : tensor<i64>}> : () -> tensor<i64>
      %10 = "stablehlo.constant"() <{value = dense<0.000000e+00> : tensor<bf16>}> : () -> tensor<bf16>
      %11 = "stablehlo.constant"() <{value = dense<-3.389530e+38> : tensor<bf16>}> : () -> tensor<bf16>
      %12 = "stablehlo.broadcast_in_dim"(%11) <{broadcast_dimensions = array<i64>}> : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
      %13 = "stablehlo.broadcast_in_dim"(%10) <{broadcast_dimensions = array<i64>}> : (tensor<bf16>) -> tensor<1x1x7x7xbf16>
      %14 = "stablehlo.broadcast_in_dim"(%10) <{broadcast_dimensions = array<i64>}> : (tensor<bf16>) -> tensor<7x7xbf16>
      %15 = "stablehlo.broadcast_in_dim"(%11) <{broadcast_dimensions = array<i64>}> : (tensor<bf16>) -> tensor<7x7xbf16>
      %16 = "stablehlo.broadcast_in_dim"(%9) <{broadcast_dimensions = array<i64>}> : (tensor<i64>) -> tensor<7x7xi64>
      %17 = "stablehlo.broadcast_in_dim"(%8) <{broadcast_dimensions = array<i64>}> : (tensor<bf16>) -> tensor<1x8x7x7xbf16>
      %18 = "stablehlo.broadcast_in_dim"(%4) <{broadcast_dimensions = array<i64>}> : (tensor<f32>) -> tensor<1x7x2048xf32>
      %19 = "stablehlo.reshape"(%arg18) : (tensor<8192xbf16>) -> tensor<8x1024xbf16>
      %20 = "stablehlo.all_to_all"(%19) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>, split_count = 8 : i64, split_dimension = 0 : i64}> : (tensor<8x1024xbf16>) -> tensor<8x1024xbf16>
      %21 = "stablehlo.slice"(%20) <{limit_indices = array<i64: 1, 1024>, start_indices = array<i64: 0, 0>, strides = array<i64: 1, 1>}> : (tensor<8x1024xbf16>) -> tensor<1x1024xbf16>
      %22 = "stablehlo.reshape"(%21) : (tensor<1x1024xbf16>) -> tensor<1024xbf16>
      %23 = "stablehlo.reshape"(%22) : (tensor<1024xbf16>) -> tensor<1x1x1024xbf16>
      %24 = "stablehlo.reshape"(%23) : (tensor<1x1x1024xbf16>) -> tensor<1024xbf16>
      %25 = "stablehlo.broadcast_in_dim"(%24) <{broadcast_dimensions = array<i64: 2>}> : (tensor<1024xbf16>) -> tensor<1x7x1024xbf16>
      %26 = "stablehlo.reshape"(%arg17) : (tensor<128256x1024xbf16>) -> tensor<1x128256x1024xbf16>
      %27 = "stablehlo.reshape"(%26) : (tensor<1x128256x1024xbf16>) -> tensor<128256x1024xbf16>
      %28 = "stablehlo.reshape"(%arg16) : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
      %29 = "stablehlo.reshape"(%28) : (tensor<1x1x7xi64>) -> tensor<7xi64>
      %30 = "stablehlo.convert"(%29) : (tensor<7xi64>) -> tensor<7xui32>
      %31 = "stablehlo.gather"(%27, %30) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1024>}> : (tensor<128256x1024xbf16>, tensor<7xui32>) -> tensor<7x1024xbf16>
      %32 = "stablehlo.reshape"(%31) : (tensor<7x1024xbf16>) -> tensor<1x7x1024xbf16>
      %33 = "stablehlo.convert"(%32) : (tensor<1x7x1024xbf16>) -> tensor<1x7x1024xf32>
      %34 = "stablehlo.broadcast_in_dim"(%4) <{broadcast_dimensions = array<i64>}> : (tensor<f32>) -> tensor<1x7x1024xf32>
      %35 = "stablehlo.power"(%33, %34) : (tensor<1x7x1024xf32>, tensor<1x7x1024xf32>) -> tensor<1x7x1024xf32>
      %36 = "stablehlo.reduce"(%35, %1) <{dimensions = array<i64: 2>}> ({
      ^bb0(%arg60: tensor<f32>, %arg61: tensor<f32>):
        %244 = "stablehlo.add"(%arg60, %arg61) : (tensor<f32>, tensor<f32>) -> tensor<f32>
        "stablehlo.return"(%244) : (tensor<f32>) -> ()
      }) : (tensor<1x7x1024xf32>, tensor<f32>) -> tensor<1x7xf32>
      %37 = "stablehlo.all_reduce"(%36) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg58: tensor<f32>, %arg59: tensor<f32>):
        %243 = "stablehlo.add"(%arg58, %arg59) : (tensor<f32>, tensor<f32>) -> tensor<f32>
        "stablehlo.return"(%243) : (tensor<f32>) -> ()
      }) : (tensor<1x7xf32>) -> tensor<1x7xf32>
      %38 = "stablehlo.multiply"(%37, %5) : (tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
      %39 = "stablehlo.reshape"(%38) : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %40 = "stablehlo.add"(%39, %6) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
      %41 = "stablehlo.rsqrt"(%40) : (tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
      %42 = "stablehlo.reshape"(%41) : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %43 = "stablehlo.broadcast_in_dim"(%42) <{broadcast_dimensions = array<i64: 0, 1>}> : (tensor<1x7xf32>) -> tensor<1x7x1024xf32>
      %44 = "stablehlo.multiply"(%33, %43) : (tensor<1x7x1024xf32>, tensor<1x7x1024xf32>) -> tensor<1x7x1024xf32>
      %45 = "stablehlo.convert"(%44) : (tensor<1x7x1024xf32>) -> tensor<1x7x1024xbf16>
      %46 = "stablehlo.multiply"(%25, %45) : (tensor<1x7x1024xbf16>, tensor<1x7x1024xbf16>) -> tensor<1x7x1024xbf16>
      %47 = "stablehlo.reshape"(%46) : (tensor<1x7x1024xbf16>) -> tensor<7x1024xbf16>
      %48 = "stablehlo.reshape"(%arg15) : (tensor<128x2048xbf16>) -> tensor<1x128x2048xbf16>
      %49 = "stablehlo.reshape"(%48) : (tensor<1x128x2048xbf16>) -> tensor<128x2048xbf16>
      %50 = "stablehlo.transpose"(%49) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<128x2048xbf16>) -> tensor<2048x128xbf16>
      %51 = "sdy.collective_permute"(%47) <{out_sharding = #sdy.sharding<@mesh, [{}, {"_axis_1", "_axis_0":(4)2}]>}> : (tensor<7x1024xbf16>) -> tensor<7x8192xbf16>
      %52 = "stablehlo.all_gather"(%51) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> : (tensor<7x8192xbf16>) -> tensor<7x65536xbf16>
      %53 = "stablehlo.dot_general"(%52, %50) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>}> : (tensor<7x65536xbf16>, tensor<2048x128xbf16>) -> tensor<7x128xbf16>
      %54 = "stablehlo.all_reduce"(%53) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg56: tensor<bf16>, %arg57: tensor<bf16>):
        %242 = "stablehlo.add"(%arg56, %arg57) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
        "stablehlo.return"(%242) : (tensor<bf16>) -> ()
      }) : (tensor<7x128xbf16>) -> tensor<7x128xbf16>
      %55 = "stablehlo.reshape"(%54) : (tensor<7x128xbf16>) -> tensor<1x7x1x128xbf16>
      %56 = "stablehlo.transpose"(%55) <{permutation = array<i64: 0, 2, 1, 3>}> {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x1x128xbf16>) -> tensor<1x1x7x128xbf16>
      %57 = "stablehlo.reshape"(%arg20) : (tensor<128x2048xbf16>) -> tensor<1x128x2048xbf16>
      %58 = "stablehlo.reshape"(%57) : (tensor<1x128x2048xbf16>) -> tensor<128x2048xbf16>
      %59 = "stablehlo.transpose"(%58) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,1024]{0,1}"} : (tensor<128x2048xbf16>) -> tensor<2048x128xbf16>
      %60 = "sdy.collective_permute"(%47) <{out_sharding = #sdy.sharding<@mesh, [{}, {"_axis_1", "_axis_0":(4)2}]>}> : (tensor<7x1024xbf16>) -> tensor<7x8192xbf16>
      %61 = "stablehlo.all_gather"(%60) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> : (tensor<7x8192xbf16>) -> tensor<7x65536xbf16>
      %62 = "stablehlo.dot_general"(%61, %59) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>}> : (tensor<7x65536xbf16>, tensor<2048x128xbf16>) -> tensor<7x128xbf16>
      %63 = "stablehlo.all_reduce"(%62) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg54: tensor<bf16>, %arg55: tensor<bf16>):
        %241 = "stablehlo.add"(%arg54, %arg55) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
        "stablehlo.return"(%241) : (tensor<bf16>) -> ()
      }) : (tensor<7x128xbf16>) -> tensor<7x128xbf16>
      %64 = "stablehlo.reshape"(%63) : (tensor<7x128xbf16>) -> tensor<1x7x1x128xbf16>
      %65 = "stablehlo.transpose"(%64) <{permutation = array<i64: 0, 2, 1, 3>}> {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x1x128xbf16>) -> tensor<1x1x7x128xbf16>
      %66 = "stablehlo.reshape"(%arg19) : (tensor<64xf32>) -> tensor<1x1x64xf32>
      %67 = "stablehlo.reshape"(%66) : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32>
      %68 = "stablehlo.dot_general"(%67, %7) <{dot_dimension_numbers = #stablehlo.dot<lhs_batching_dimensions = [0], rhs_batching_dimensions = [0], lhs_contracting_dimensions = [2], rhs_contracting_dimensions = [1]>}> : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
      %69 = "stablehlo.transpose"(%68) <{permutation = array<i64: 0, 2, 1>}> {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
      %70 = "stablehlo.concatenate"(%69, %69) <{dimension = 2 : i64}> : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
      %71 = "stablehlo.cosine"(%70) : (tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
      %72 = "stablehlo.convert"(%71) : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
      %73 = "stablehlo.broadcast_in_dim"(%72) <{broadcast_dimensions = array<i64: 0, 2, 3>}> : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
      %74 = "stablehlo.multiply"(%65, %73) : (tensor<1x1x7x128xbf16>, tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
      %75 = "stablehlo.slice"(%65) <{limit_indices = array<i64: 1, 1, 7, 128>, start_indices = array<i64: 0, 0, 0, 64>, strides = array<i64: 1, 1, 1, 1>}> : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x64xbf16>
      %76 = "stablehlo.negate"(%75) : (tensor<1x1x7x64xbf16>) -> tensor<1x1x7x64xbf16>
      %77 = "stablehlo.slice"(%65) <{limit_indices = array<i64: 1, 1, 7, 64>, start_indices = array<i64: 0, 0, 0, 0>, strides = array<i64: 1, 1, 1, 1>}> : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x64xbf16>
      %78 = "stablehlo.concatenate"(%76, %77) <{dimension = 3 : i64}> : (tensor<1x1x7x64xbf16>, tensor<1x1x7x64xbf16>) -> tensor<1x1x7x128xbf16>
      %79 = "stablehlo.sine"(%70) : (tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
      %80 = "stablehlo.convert"(%79) : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
      %81 = "stablehlo.broadcast_in_dim"(%80) <{broadcast_dimensions = array<i64: 0, 2, 3>}> : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
      %82 = "stablehlo.multiply"(%78, %81) : (tensor<1x1x7x128xbf16>, tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
      %83 = "stablehlo.add"(%74, %82) : (tensor<1x1x7x128xbf16>, tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
      %84 = "stablehlo.reshape"(%arg29) : (tensor<8192xbf16>) -> tensor<4x2048xbf16>
      %85 = "stablehlo.all_to_all"(%84) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x2048xbf16>) -> tensor<4x2048xbf16>
      %86 = "stablehlo.slice"(%85) <{limit_indices = array<i64: 1, 2048>, start_indices = array<i64: 0, 0>, strides = array<i64: 1, 1>}> : (tensor<4x2048xbf16>) -> tensor<1x2048xbf16>
      %87 = "stablehlo.reshape"(%86) : (tensor<1x2048xbf16>) -> tensor<2048xbf16>
      %88 = "stablehlo.reshape"(%87) : (tensor<2048xbf16>) -> tensor<1x1x2048xbf16>
      %89 = "stablehlo.reshape"(%88) : (tensor<1x1x2048xbf16>) -> tensor<2048xbf16>
      %90 = "stablehlo.broadcast_in_dim"(%89) <{broadcast_dimensions = array<i64: 2>}> : (tensor<2048xbf16>) -> tensor<1x7x2048xbf16>
      %91 = "stablehlo.reshape"(%arg26) : (tensor<1024x2048xbf16>) -> tensor<1x1024x2048xbf16>
      %92 = "stablehlo.reshape"(%91) : (tensor<1x1024x2048xbf16>) -> tensor<1024x2048xbf16>
      %93 = "stablehlo.transpose"(%92) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<1024x2048xbf16>) -> tensor<2048x1024xbf16>
      %94 = "sdy.collective_permute"(%47) <{out_sharding = #sdy.sharding<@mesh, [{}, {"_axis_1", "_axis_0":(4)2}]>}> : (tensor<7x1024xbf16>) -> tensor<7x8192xbf16>
      %95 = "stablehlo.all_gather"(%94) <{all_gather_dim = 1 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> : (tensor<7x8192xbf16>) -> tensor<7x65536xbf16>
      %96 = "stablehlo.dot_general"(%95, %93) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>}> : (tensor<7x65536xbf16>, tensor<2048x1024xbf16>) -> tensor<7x1024xbf16>
      %97 = "stablehlo.all_reduce"(%96) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg52: tensor<bf16>, %arg53: tensor<bf16>):
        %240 = "stablehlo.add"(%arg52, %arg53) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
        "stablehlo.return"(%240) : (tensor<bf16>) -> ()
      }) : (tensor<7x1024xbf16>) -> tensor<7x1024xbf16>
      %98 = "stablehlo.reshape"(%97) : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
      %99 = "stablehlo.transpose"(%98) <{permutation = array<i64: 0, 2, 1, 3>}> {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
      %100 = "stablehlo.broadcast_in_dim"(%72) <{broadcast_dimensions = array<i64: 0, 2, 3>}> : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
      %101 = "stablehlo.multiply"(%99, %100) : (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
      %102 = "stablehlo.slice"(%99) <{limit_indices = array<i64: 1, 8, 7, 128>, start_indices = array<i64: 0, 0, 0, 64>, strides = array<i64: 1, 1, 1, 1>}> : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
      %103 = "stablehlo.negate"(%102) : (tensor<1x8x7x64xbf16>) -> tensor<1x8x7x64xbf16>
      %104 = "stablehlo.slice"(%99) <{limit_indices = array<i64: 1, 8, 7, 64>, start_indices = array<i64: 0, 0, 0, 0>, strides = array<i64: 1, 1, 1, 1>}> : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
      %105 = "stablehlo.concatenate"(%103, %104) <{dimension = 3 : i64}> : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
      %106 = "stablehlo.broadcast_in_dim"(%80) <{broadcast_dimensions = array<i64: 0, 2, 3>}> : (tensor<1x7x128xbf16>) -> tensor<1x8x7x128xbf16>
      %107 = "stablehlo.multiply"(%105, %106) : (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
      %108 = "stablehlo.add"(%101, %107) : (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
      %109 = "stablehlo.broadcast_in_dim"(%83) <{broadcast_dimensions = array<i64: 0, 1, 3, 4>}> : (tensor<1x1x7x128xbf16>) -> tensor<1x1x8x7x128xbf16>
      %110 = "stablehlo.reshape"(%109) : (tensor<1x1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
      %111 = "stablehlo.transpose"(%110) <{permutation = array<i64: 0, 1, 3, 2>}> {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,64,128,7]{2,3,1,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x128x7xbf16>
      %112 = "stablehlo.dot_general"(%108, %111) <{dot_dimension_numbers = #stablehlo.dot<lhs_batching_dimensions = [0, 1], rhs_batching_dimensions = [0, 1], lhs_contracting_dimensions = [3], rhs_contracting_dimensions = [2]>}> {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x8x7x128xbf16>, tensor<1x8x128x7xbf16>) -> tensor<1x8x7x7xbf16>
      %113 = "stablehlo.multiply"(%112, %17) : (tensor<1x8x7x7xbf16>, tensor<1x8x7x7xbf16>) -> tensor<1x8x7x7xbf16>
      %114 = "stablehlo.broadcast_in_dim"(%2) <{broadcast_dimensions = array<i64: 1>}> : (tensor<7xi64>) -> tensor<7x7xi64>
      %115 = "stablehlo.broadcast_in_dim"(%2) <{broadcast_dimensions = array<i64: 0>}> : (tensor<7xi64>) -> tensor<7x7xi64>
      %116 = "stablehlo.broadcast_in_dim"(%2) <{broadcast_dimensions = array<i64: 1>}> : (tensor<7xi64>) -> tensor<7x7xi64>
      %117 = "stablehlo.broadcast_in_dim"(%2) <{broadcast_dimensions = array<i64: 0>}> : (tensor<7xi64>) -> tensor<7x7xi64>
      %118 = "stablehlo.subtract"(%116, %117) : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi64>
      %119 = "stablehlo.compare"(%118, %16) <{comparison_direction = #stablehlo<comparison_direction GE>}> : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
      %120 = "stablehlo.select"(%119, %15, %14) : (tensor<7x7xi1>, tensor<7x7xbf16>, tensor<7x7xbf16>) -> tensor<7x7xbf16>
      %121 = "stablehlo.compare"(%114, %115) <{comparison_direction = #stablehlo<comparison_direction GT>}> : (tensor<7x7xi64>, tensor<7x7xi64>) -> tensor<7x7xi1>
      %122 = "stablehlo.convert"(%121) : (tensor<7x7xi1>) -> tensor<7x7xbf16>
      %123 = "stablehlo.multiply"(%120, %122) : (tensor<7x7xbf16>, tensor<7x7xbf16>) -> tensor<7x7xbf16>
      %124 = "stablehlo.reshape"(%123) : (tensor<7x7xbf16>) -> tensor<1x1x7x7xbf16>
      %125 = "stablehlo.reshape"(%arg25) : (tensor<1x7xi64>) -> tensor<1x1x7xi64>
      %126 = "stablehlo.reshape"(%125) : (tensor<1x1x7xi64>) -> tensor<1x1x1x7xi64>
      %127 = "stablehlo.convert"(%126) : (tensor<1x1x1x7xi64>) -> tensor<1x1x1x7xbf16>
      %128 = "stablehlo.reshape"(%127) : (tensor<1x1x1x7xbf16>) -> tensor<1x1x7xbf16>
      %129 = "stablehlo.broadcast_in_dim"(%128) <{broadcast_dimensions = array<i64: 0, 1, 3>}> : (tensor<1x1x7xbf16>) -> tensor<1x1x7x7xbf16>
      %130 = "stablehlo.add"(%124, %129) : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xbf16>
      %131 = "stablehlo.compare"(%130, %13) <{comparison_direction = #stablehlo<comparison_direction EQ>}> : (tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xi1>
      %132 = "stablehlo.select"(%131, %12, %124) : (tensor<1x1x7x7xi1>, tensor<1x1x7x7xbf16>, tensor<1x1x7x7xbf16>) -> tensor<1x1x7x7xbf16>
      %133 = "stablehlo.reshape"(%132) : (tensor<1x1x7x7xbf16>) -> tensor<1x7x7xbf16>
      %134 = "stablehlo.broadcast_in_dim"(%133) <{broadcast_dimensions = array<i64: 0, 2, 3>}> : (tensor<1x7x7xbf16>) -> tensor<1x8x7x7xbf16>
      %135 = "stablehlo.add"(%113, %134) : (tensor<1x8x7x7xbf16>, tensor<1x8x7x7xbf16>) -> tensor<1x8x7x7xbf16>
      %136 = "stablehlo.convert"(%135) : (tensor<1x8x7x7xbf16>) -> tensor<1x8x7x7xf32>
      %137 = "stablehlo.reduce"(%136, %3) <{dimensions = array<i64: 3>}> ({
      ^bb0(%arg50: tensor<f32>, %arg51: tensor<f32>):
        %239 = "stablehlo.maximum"(%arg50, %arg51) : (tensor<f32>, tensor<f32>) -> tensor<f32>
        "stablehlo.return"(%239) : (tensor<f32>) -> ()
      }) : (tensor<1x8x7x7xf32>, tensor<f32>) -> tensor<1x8x7xf32>
      %138 = "stablehlo.broadcast_in_dim"(%137) <{broadcast_dimensions = array<i64: 0, 1, 2>}> : (tensor<1x8x7xf32>) -> tensor<1x8x7x7xf32>
      %139 = "stablehlo.subtract"(%136, %138) : (tensor<1x8x7x7xf32>, tensor<1x8x7x7xf32>) -> tensor<1x8x7x7xf32>
      %140 = "stablehlo.exponential"(%139) : (tensor<1x8x7x7xf32>) -> tensor<1x8x7x7xf32>
      %141 = "stablehlo.reduce"(%140, %1) <{dimensions = array<i64: 3>}> ({
      ^bb0(%arg48: tensor<f32>, %arg49: tensor<f32>):
        %238 = "stablehlo.add"(%arg48, %arg49) : (tensor<f32>, tensor<f32>) -> tensor<f32>
        "stablehlo.return"(%238) : (tensor<f32>) -> ()
      }) : (tensor<1x8x7x7xf32>, tensor<f32>) -> tensor<1x8x7xf32>
      %142 = "stablehlo.broadcast_in_dim"(%141) <{broadcast_dimensions = array<i64: 0, 1, 2>}> : (tensor<1x8x7xf32>) -> tensor<1x8x7x7xf32>
      %143 = "stablehlo.divide"(%140, %142) : (tensor<1x8x7x7xf32>, tensor<1x8x7x7xf32>) -> tensor<1x8x7x7xf32>
      %144 = "stablehlo.convert"(%143) : (tensor<1x8x7x7xf32>) -> tensor<1x8x7x7xbf16>
      %145 = "stablehlo.broadcast_in_dim"(%56) <{broadcast_dimensions = array<i64: 0, 1, 3, 4>}> : (tensor<1x1x7x128xbf16>) -> tensor<1x1x8x7x128xbf16>
      %146 = "stablehlo.reshape"(%145) : (tensor<1x1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
      %147 = "stablehlo.dot_general"(%144, %146) <{dot_dimension_numbers = #stablehlo.dot<lhs_batching_dimensions = [0, 1], rhs_batching_dimensions = [0, 1], lhs_contracting_dimensions = [3], rhs_contracting_dimensions = [2]>}> {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x8x7x7xbf16>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
      %148 = "stablehlo.transpose"(%147) <{permutation = array<i64: 0, 2, 1, 3>}> {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,64,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x7x8x128xbf16>
      %149 = "stablehlo.reshape"(%148) : (tensor<1x7x8x128xbf16>) -> tensor<7x1024xbf16>
      %150 = "stablehlo.reshape"(%arg24) : (tensor<2048x1024xbf16>) -> tensor<1x2048x1024xbf16>
      %151 = "stablehlo.reshape"(%150) : (tensor<1x2048x1024xbf16>) -> tensor<2048x1024xbf16>
      %152 = "stablehlo.transpose"(%151) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,8192]{0,1}"} : (tensor<2048x1024xbf16>) -> tensor<1024x2048xbf16>
      %153 = "stablehlo.dot_general"(%149, %152) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>}> : (tensor<7x1024xbf16>, tensor<1024x2048xbf16>) -> tensor<7x2048xbf16>
      %154 = "stablehlo.all_reduce"(%153) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg46: tensor<bf16>, %arg47: tensor<bf16>):
        %237 = "stablehlo.add"(%arg46, %arg47) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
        "stablehlo.return"(%237) : (tensor<bf16>) -> ()
      }) : (tensor<7x2048xbf16>) -> tensor<7x2048xbf16>
      %155 = "stablehlo.reshape"(%154) : (tensor<7x2048xbf16>) -> tensor<1x7x2048xbf16>
      %156 = "sdy.collective_permute"(%32) <{out_sharding = #sdy.sharding<@mesh, [{}, {}, {"_axis_1", "_axis_0":(4)2}]>}> : (tensor<1x7x1024xbf16>) -> tensor<1x7x8192xbf16>
      %157 = "stablehlo.all_gather"(%156) <{all_gather_dim = 2 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> : (tensor<1x7x8192xbf16>) -> tensor<1x7x65536xbf16>
      %158 = "stablehlo.add"(%157, %155) : (tensor<1x7x65536xbf16>, tensor<1x7x2048xbf16>) -> tensor<1x7x2048xbf16>
      %159 = "stablehlo.reshape"(%arg27) : (tensor<8192xbf16>) -> tensor<4x2048xbf16>
      %160 = "stablehlo.all_to_all"(%159) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 0 : i64, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>, split_count = 4 : i64, split_dimension = 0 : i64}> : (tensor<4x2048xbf16>) -> tensor<4x2048xbf16>
      %161 = "stablehlo.slice"(%160) <{limit_indices = array<i64: 1, 2048>, start_indices = array<i64: 0, 0>, strides = array<i64: 1, 1>}> : (tensor<4x2048xbf16>) -> tensor<1x2048xbf16>
      %162 = "stablehlo.reshape"(%161) : (tensor<1x2048xbf16>) -> tensor<2048xbf16>
      %163 = "stablehlo.reshape"(%162) : (tensor<2048xbf16>) -> tensor<1x1x2048xbf16>
      %164 = "stablehlo.reshape"(%163) : (tensor<1x1x2048xbf16>) -> tensor<2048xbf16>
      %165 = "stablehlo.broadcast_in_dim"(%164) <{broadcast_dimensions = array<i64: 2>}> : (tensor<2048xbf16>) -> tensor<1x7x2048xbf16>
      %166 = "stablehlo.convert"(%158) : (tensor<1x7x2048xbf16>) -> tensor<1x7x2048xf32>
      %167 = "stablehlo.broadcast_in_dim"(%4) <{broadcast_dimensions = array<i64>}> : (tensor<f32>) -> tensor<1x7x2048xf32>
      %168 = "stablehlo.power"(%166, %167) : (tensor<1x7x2048xf32>, tensor<1x7x2048xf32>) -> tensor<1x7x2048xf32>
      %169 = "stablehlo.reduce"(%168, %1) <{dimensions = array<i64: 2>}> ({
      ^bb0(%arg44: tensor<f32>, %arg45: tensor<f32>):
        %236 = "stablehlo.add"(%arg44, %arg45) : (tensor<f32>, tensor<f32>) -> tensor<f32>
        "stablehlo.return"(%236) : (tensor<f32>) -> ()
      }) : (tensor<1x7x2048xf32>, tensor<f32>) -> tensor<1x7xf32>
      %170 = "stablehlo.all_reduce"(%169) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg42: tensor<f32>, %arg43: tensor<f32>):
        %235 = "stablehlo.add"(%arg42, %arg43) : (tensor<f32>, tensor<f32>) -> tensor<f32>
        "stablehlo.return"(%235) : (tensor<f32>) -> ()
      }) : (tensor<1x7xf32>) -> tensor<1x7xf32>
      %171 = "stablehlo.multiply"(%170, %5) : (tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
      %172 = "stablehlo.reshape"(%171) : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %173 = "stablehlo.add"(%172, %6) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
      %174 = "stablehlo.rsqrt"(%173) : (tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
      %175 = "stablehlo.reshape"(%174) : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %176 = "stablehlo.broadcast_in_dim"(%175) <{broadcast_dimensions = array<i64: 0, 1>}> : (tensor<1x7xf32>) -> tensor<1x7x2048xf32>
      %177 = "stablehlo.multiply"(%166, %176) : (tensor<1x7x2048xf32>, tensor<1x7x2048xf32>) -> tensor<1x7x2048xf32>
      %178 = "stablehlo.convert"(%177) : (tensor<1x7x2048xf32>) -> tensor<1x7x2048xbf16>
      %179 = "stablehlo.multiply"(%165, %178) : (tensor<1x7x2048xbf16>, tensor<1x7x2048xbf16>) -> tensor<1x7x2048xbf16>
      %180 = "stablehlo.reshape"(%179) : (tensor<1x7x2048xbf16>) -> tensor<7x2048xbf16>
      %181 = "stablehlo.reshape"(%arg28) : (tensor<3584x2048xbf16>) -> tensor<1x3584x2048xbf16>
      %182 = "stablehlo.reshape"(%181) : (tensor<1x3584x2048xbf16>) -> tensor<3584x2048xbf16>
      %183 = "stablehlo.transpose"(%182) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<3584x2048xbf16>) -> tensor<2048x3584xbf16>
      %184 = "stablehlo.dot_general"(%180, %183) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>}> : (tensor<7x2048xbf16>, tensor<2048x3584xbf16>) -> tensor<7x3584xbf16>
      %185 = "stablehlo.all_reduce"(%184) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg40: tensor<bf16>, %arg41: tensor<bf16>):
        %234 = "stablehlo.add"(%arg40, %arg41) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
        "stablehlo.return"(%234) : (tensor<bf16>) -> ()
      }) : (tensor<7x3584xbf16>) -> tensor<7x3584xbf16>
      %186 = "stablehlo.reshape"(%185) : (tensor<7x3584xbf16>) -> tensor<1x7x3584xbf16>
      %187 = "stablehlo.logistic"(%186) : (tensor<1x7x3584xbf16>) -> tensor<1x7x3584xbf16>
      %188 = "stablehlo.multiply"(%186, %187) : (tensor<1x7x3584xbf16>, tensor<1x7x3584xbf16>) -> tensor<1x7x3584xbf16>
      %189 = "stablehlo.reshape"(%arg23) : (tensor<3584x2048xbf16>) -> tensor<1x3584x2048xbf16>
      %190 = "stablehlo.reshape"(%189) : (tensor<1x3584x2048xbf16>) -> tensor<3584x2048xbf16>
      %191 = "stablehlo.transpose"(%190) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,28672]{0,1}"} : (tensor<3584x2048xbf16>) -> tensor<2048x3584xbf16>
      %192 = "stablehlo.dot_general"(%180, %191) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>}> : (tensor<7x2048xbf16>, tensor<2048x3584xbf16>) -> tensor<7x3584xbf16>
      %193 = "stablehlo.all_reduce"(%192) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg38: tensor<bf16>, %arg39: tensor<bf16>):
        %233 = "stablehlo.add"(%arg38, %arg39) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
        "stablehlo.return"(%233) : (tensor<bf16>) -> ()
      }) : (tensor<7x3584xbf16>) -> tensor<7x3584xbf16>
      %194 = "stablehlo.reshape"(%193) : (tensor<7x3584xbf16>) -> tensor<1x7x3584xbf16>
      %195 = "stablehlo.multiply"(%188, %194) : (tensor<1x7x3584xbf16>, tensor<1x7x3584xbf16>) -> tensor<1x7x3584xbf16>
      %196 = "stablehlo.reshape"(%195) : (tensor<1x7x3584xbf16>) -> tensor<7x3584xbf16>
      %197 = "stablehlo.reshape"(%arg22) : (tensor<2048x3584xbf16>) -> tensor<1x2048x3584xbf16>
      %198 = "stablehlo.reshape"(%197) : (tensor<1x2048x3584xbf16>) -> tensor<2048x3584xbf16>
      %199 = "stablehlo.transpose"(%198) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[28672,8192]{0,1}"} : (tensor<2048x3584xbf16>) -> tensor<3584x2048xbf16>
      %200 = "stablehlo.dot_general"(%196, %199) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>}> : (tensor<7x3584xbf16>, tensor<3584x2048xbf16>) -> tensor<7x2048xbf16>
      %201 = "stablehlo.all_reduce"(%200) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg36: tensor<bf16>, %arg37: tensor<bf16>):
        %232 = "stablehlo.add"(%arg36, %arg37) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
        "stablehlo.return"(%232) : (tensor<bf16>) -> ()
      }) : (tensor<7x2048xbf16>) -> tensor<7x2048xbf16>
      %202 = "stablehlo.reshape"(%201) : (tensor<7x2048xbf16>) -> tensor<1x7x2048xbf16>
      %203 = "stablehlo.add"(%158, %202) : (tensor<1x7x2048xbf16>, tensor<1x7x2048xbf16>) -> tensor<1x7x2048xbf16>
      %204 = "stablehlo.convert"(%203) : (tensor<1x7x2048xbf16>) -> tensor<1x7x2048xf32>
      %205 = "stablehlo.power"(%204, %18) : (tensor<1x7x2048xf32>, tensor<1x7x2048xf32>) -> tensor<1x7x2048xf32>
      %206 = "stablehlo.reduce"(%205, %1) <{dimensions = array<i64: 2>}> ({
      ^bb0(%arg34: tensor<f32>, %arg35: tensor<f32>):
        %231 = "stablehlo.add"(%arg34, %arg35) : (tensor<f32>, tensor<f32>) -> tensor<f32>
        "stablehlo.return"(%231) : (tensor<f32>) -> ()
      }) : (tensor<1x7x2048xf32>, tensor<f32>) -> tensor<1x7xf32>
      %207 = "stablehlo.all_reduce"(%206) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]> : tensor<8x4xi64>}> ({
      ^bb0(%arg32: tensor<f32>, %arg33: tensor<f32>):
        %230 = "stablehlo.add"(%arg32, %arg33) : (tensor<f32>, tensor<f32>) -> tensor<f32>
        "stablehlo.return"(%230) : (tensor<f32>) -> ()
      }) : (tensor<1x7xf32>) -> tensor<1x7xf32>
      %208 = "stablehlo.multiply"(%207, %5) : (tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
      %209 = "stablehlo.reshape"(%208) : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %210 = "stablehlo.add"(%209, %6) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
      %211 = "stablehlo.rsqrt"(%210) : (tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
      %212 = "stablehlo.reshape"(%211) : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %213 = "stablehlo.broadcast_in_dim"(%212) <{broadcast_dimensions = array<i64: 0, 1>}> : (tensor<1x7xf32>) -> tensor<1x7x2048xf32>
      %214 = "stablehlo.multiply"(%204, %213) : (tensor<1x7x2048xf32>, tensor<1x7x2048xf32>) -> tensor<1x7x2048xf32>
      %215 = "stablehlo.convert"(%214) : (tensor<1x7x2048xf32>) -> tensor<1x7x2048xbf16>
      %216 = "stablehlo.multiply"(%90, %215) : (tensor<1x7x2048xbf16>, tensor<1x7x2048xbf16>) -> tensor<1x7x2048xbf16>
      %217 = "stablehlo.reshape"(%216) : (tensor<1x7x2048xbf16>) -> tensor<7x2048xbf16>
      %218 = "stablehlo.reshape"(%arg21) : (tensor<128256x1024xbf16>) -> tensor<1x128256x1024xbf16>
      %219 = "stablehlo.reshape"(%218) : (tensor<1x128256x1024xbf16>) -> tensor<128256x1024xbf16>
      %220 = "stablehlo.transpose"(%219) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,128256]{0,1}"} : (tensor<128256x1024xbf16>) -> tensor<1024x128256xbf16>
      %221 = "stablehlo.reshape"(%217) : (tensor<7x2048xbf16>) -> tensor<7x8x256xbf16>
      %222 = "stablehlo.all_to_all"(%221) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, concat_dimension = 1 : i64, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>, split_count = 8 : i64, split_dimension = 1 : i64}> : (tensor<7x8x256xbf16>) -> tensor<7x8x256xbf16>
      %223 = "stablehlo.slice"(%222) <{limit_indices = array<i64: 7, 1, 256>, start_indices = array<i64: 0, 0, 0>, strides = array<i64: 1, 1, 1>}> : (tensor<7x8x256xbf16>) -> tensor<7x1x256xbf16>
      %224 = "stablehlo.reshape"(%223) : (tensor<7x1x256xbf16>) -> tensor<7x256xbf16>
      %225 = "sdy.collective_permute"(%224) <{out_sharding = #sdy.sharding<@mesh, [{}, {"_axis_0"}]>}> : (tensor<7x256xbf16>) -> tensor<7x8192xbf16>
      %226 = "stablehlo.dot_general"(%225, %220) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>}> : (tensor<7x8192xbf16>, tensor<1024x128256xbf16>) -> tensor<7x128256xbf16>
      %227 = "stablehlo.all_reduce"(%226) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 4, 8, 12, 16, 20, 24, 28], [1, 5, 9, 13, 17, 21, 25, 29], [2, 6, 10, 14, 18, 22, 26, 30], [3, 7, 11, 15, 19, 23, 27, 31]]> : tensor<4x8xi64>}> ({
      ^bb0(%arg30: tensor<bf16>, %arg31: tensor<bf16>):
        %229 = "stablehlo.add"(%arg30, %arg31) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
        "stablehlo.return"(%229) : (tensor<bf16>) -> ()
      }) : (tensor<7x128256xbf16>) -> tensor<7x128256xbf16>
      %228 = "stablehlo.reshape"(%227) : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
      "sdy.return"(%56, %83, %228) : (tensor<1x1x7x128xbf16>, tensor<1x1x7x128xbf16>, tensor<1x7x128256xbf16>) -> ()
    }) : (tensor<1024x8192xbf16>, tensor<1x7xi64>, tensor<128256x8192xbf16>, tensor<8192xbf16>, tensor<64xf32>, tensor<1024x8192xbf16>, tensor<128256x8192xbf16>, tensor<8192x28672xbf16>, tensor<28672x8192xbf16>, tensor<8192x8192xbf16>, tensor<1x7xi64>, tensor<8192x8192xbf16>, tensor<8192xbf16>, tensor<28672x8192xbf16>, tensor<8192xbf16>) -> (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>)
    "func.return"(%0#0, %0#1, %0#2) : (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x7x128256xbf16>) -> ()
  }) : () -> ()
}) {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} : () -> ()


2025-12-04 19:55:23.828 (  69.979s) [        CE2D1B80]      module_builder.cc:672    ERR| Failed to run stablehlo pipeline
2025-12-04 19:55:23.828 (  69.979s) [        CE2D1B80]      error_instance.cc:52       1| ErrorInstance::PJRT_Error_Message
2025-12-04 19:55:23.828 (  69.979s) [        CE2D1B80]      error_instance.cc:61       1| ErrorInstance::PJRT_Error_GetCode
2025-12-04 19:55:23.828 (  69.979s) [        CE2D1B80]      error_instance.cc:46       1| ErrorInstance::PJRT_Error_Destroy
Traceback (most recent call last):
  File "/home/hshah/tt-xla/examples/pytorch/multichip/galaxy/llama_70b.py", line 125, in <module>
    run_llama_70b()
  File "/home/hshah/tt-xla/examples/pytorch/multichip/galaxy/llama_70b.py", line 114, in run_llama_70b
    output = compiled_model(**input_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hshah/tt-xla/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hshah/tt-xla/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hshah/tt-xla/venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/hshah/tt-xla/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hshah/tt-xla/venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hshah/tt-xla/venv/lib/python3.11/site-packages/transformers/utils/generic.py", line 953, in wrapper
    @wraps(func)
  File "/home/hshah/tt-xla/venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/hshah/tt-xla/python_package/tt_torch/backend/backend.py", line 174, in __call__
    torch_xla._XLAC._xla_sync_multi(list(output), self.devices, wait=False)
ValueError: Error code: 13
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.522 (  70.673s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.741 (  70.892s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.741 (  70.892s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.741 (  70.892s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.742 (  70.892s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.742 (  70.892s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.742 (  70.893s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.742 (  70.893s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.742 (  70.893s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.742 (  70.893s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.742 (  70.893s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.742 (  70.893s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.742 (  70.893s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.742 (  70.893s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.742 (  70.893s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.742 (  70.893s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.743 (  70.893s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.743 (  70.893s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.743 (  70.893s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.743 (  70.893s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.743 (  70.893s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.743 (  70.894s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.743 (  70.894s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.743 (  70.894s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.743 (  70.894s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.743 (  70.894s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.743 (  70.894s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.743 (  70.894s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.743 (  70.894s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.743 (  70.894s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.743 (  70.894s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.743 (  70.894s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.743 (  70.894s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.743 (  70.894s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.743 (  70.894s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.743 (  70.894s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.744 (  70.894s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.744 (  70.894s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.744 (  70.894s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.744 (  70.894s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.744 (  70.894s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.744 (  70.895s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.744 (  70.895s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.744 (  70.895s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.744 (  70.895s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.744 (  70.895s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.744 (  70.895s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.744 (  70.895s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.744 (  70.895s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.744 (  70.895s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.744 (  70.895s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.744 (  70.895s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.744 (  70.895s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.744 (  70.895s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.744 (  70.895s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.744 (  70.895s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.744 (  70.895s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.745 (  70.895s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.745 (  70.895s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.745 (  70.895s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.745 (  70.895s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.745 (  70.895s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.745 (  70.896s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.745 (  70.896s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.745 (  70.896s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.745 (  70.896s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.745 (  70.896s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.745 (  70.896s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.745 (  70.896s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.745 (  70.896s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.745 (  70.896s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.745 (  70.896s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.745 (  70.896s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.745 (  70.896s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.745 (  70.896s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.745 (  70.896s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.745 (  70.896s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.745 (  70.896s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.746 (  70.896s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.746 (  70.896s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.746 (  70.896s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.746 (  70.896s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.746 (  70.896s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.746 (  70.896s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.746 (  70.897s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.746 (  70.897s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.746 (  70.897s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.746 (  70.897s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.746 (  70.897s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.746 (  70.897s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.746 (  70.897s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.746 (  70.897s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.746 (  70.897s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.746 (  70.897s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.746 (  70.897s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.746 (  70.897s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.746 (  70.897s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.746 (  70.897s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.747 (  70.897s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.747 (  70.897s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.747 (  70.897s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.747 (  70.897s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.747 (  70.897s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.747 (  70.897s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.747 (  70.898s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.747 (  70.898s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.747 (  70.898s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.747 (  70.898s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.747 (  70.898s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.747 (  70.898s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.747 (  70.898s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.747 (  70.898s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.747 (  70.898s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.747 (  70.898s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.747 (  70.898s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.747 (  70.898s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.747 (  70.898s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.747 (  70.898s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.747 (  70.898s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.747 (  70.898s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.748 (  70.898s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.748 (  70.898s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.748 (  70.898s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.748 (  70.898s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.748 (  70.898s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.748 (  70.898s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.748 (  70.899s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.748 (  70.899s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.748 (  70.899s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.748 (  70.899s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.765 (  70.916s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.782 (  70.933s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.799 (  70.950s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.816 (  70.966s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.837 (  70.988s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.854 (  71.005s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.872 (  71.022s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.889 (  71.040s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.905 (  71.056s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.922 (  71.073s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.939 (  71.090s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.957 (  71.107s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.973 (  71.124s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:24.991 (  71.142s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.008 (  71.159s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.025 (  71.176s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.042 (  71.193s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.058 (  71.209s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.075 (  71.226s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.092 (  71.242s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.108 (  71.259s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.127 (  71.277s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.143 (  71.294s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.159 (  71.310s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.176 (  71.326s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.191 (  71.342s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.208 (  71.359s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.225 (  71.375s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.241 (  71.392s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.257 (  71.408s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.274 (  71.424s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.291 (  71.441s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.291 (  71.441s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.291 (  71.441s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.291 (  71.442s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.291 (  71.442s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.291 (  71.442s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.291 (  71.442s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.291 (  71.442s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.291 (  71.442s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.291 (  71.442s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.291 (  71.442s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.291 (  71.442s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.291 (  71.442s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.291 (  71.442s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.291 (  71.442s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.291 (  71.442s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.291 (  71.442s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.291 (  71.442s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.292 (  71.442s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.292 (  71.442s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.292 (  71.442s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.292 (  71.442s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.292 (  71.442s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.292 (  71.443s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.292 (  71.443s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.292 (  71.443s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.292 (  71.443s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.292 (  71.443s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.292 (  71.443s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.292 (  71.443s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.292 (  71.443s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.292 (  71.443s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.292 (  71.443s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.292 (  71.443s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.292 (  71.443s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.292 (  71.443s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.293 (  71.443s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.293 (  71.443s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.293 (  71.443s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.293 (  71.443s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.293 (  71.443s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.293 (  71.444s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.293 (  71.444s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.293 (  71.444s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.293 (  71.444s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.293 (  71.444s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.293 (  71.444s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.293 (  71.444s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.293 (  71.444s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.293 (  71.444s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.293 (  71.444s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.293 (  71.444s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.293 (  71.444s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.293 (  71.444s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.293 (  71.444s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.293 (  71.444s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.293 (  71.444s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.294 (  71.444s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.294 (  71.444s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.294 (  71.444s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.294 (  71.444s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.294 (  71.445s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.294 (  71.445s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.294 (  71.445s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.294 (  71.445s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.294 (  71.445s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.294 (  71.445s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.294 (  71.445s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.294 (  71.445s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.294 (  71.445s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.294 (  71.445s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.294 (  71.445s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.294 (  71.445s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.294 (  71.445s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.295 (  71.445s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.295 (  71.445s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.295 (  71.445s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.295 (  71.445s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.295 (  71.445s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.295 (  71.445s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.295 (  71.446s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.295 (  71.446s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.295 (  71.446s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.295 (  71.446s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.295 (  71.446s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.295 (  71.446s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.295 (  71.446s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.295 (  71.446s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.295 (  71.446s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.295 (  71.446s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.295 (  71.446s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.295 (  71.446s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.295 (  71.446s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.295 (  71.446s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.295 (  71.446s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.295 (  71.446s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.295 (  71.446s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.296 (  71.446s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.296 (  71.446s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.296 (  71.446s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.296 (  71.446s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.296 (  71.446s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.296 (  71.446s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.296 (  71.447s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.296 (  71.447s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.296 (  71.447s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.296 (  71.447s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.296 (  71.447s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.296 (  71.447s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.296 (  71.447s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.296 (  71.447s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.296 (  71.447s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.296 (  71.447s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.296 (  71.447s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.296 (  71.447s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.296 (  71.447s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.296 (  71.447s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.296 (  71.447s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.296 (  71.447s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.296 (  71.447s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.297 (  71.447s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.297 (  71.447s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.297 (  71.447s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.297 (  71.447s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.297 (  71.447s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.297 (  71.447s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.297 (  71.448s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.297 (  71.448s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.297 (  71.448s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.297 (  71.448s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.297 (  71.448s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.297 (  71.448s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.297 (  71.448s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.297 (  71.448s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.297 (  71.448s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.297 (  71.448s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.297 (  71.448s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.297 (  71.448s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.297 (  71.448s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.297 (  71.448s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.297 (  71.448s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.297 (  71.448s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.298 (  71.448s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.298 (  71.448s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.298 (  71.448s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.298 (  71.448s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.298 (  71.448s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.298 (  71.448s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.298 (  71.449s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.298 (  71.449s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.298 (  71.449s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.298 (  71.449s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.298 (  71.449s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.298 (  71.449s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.298 (  71.449s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.298 (  71.449s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.298 (  71.449s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.298 (  71.449s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.298 (  71.449s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.298 (  71.449s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.298 (  71.449s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.298 (  71.449s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.298 (  71.449s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.298 (  71.449s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.298 (  71.449s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.299 (  71.449s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.299 (  71.449s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.299 (  71.449s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.299 (  71.449s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.299 (  71.449s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.299 (  71.450s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.299 (  71.450s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.299 (  71.450s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.299 (  71.450s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.299 (  71.450s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.299 (  71.450s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.299 (  71.450s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.299 (  71.450s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.299 (  71.450s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.299 (  71.450s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.299 (  71.450s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.299 (  71.450s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.299 (  71.450s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.299 (  71.450s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.299 (  71.450s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.299 (  71.450s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.299 (  71.450s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.300 (  71.450s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.300 (  71.450s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.300 (  71.450s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.300 (  71.450s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.300 (  71.450s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.300 (  71.450s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.300 (  71.451s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.300 (  71.451s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.300 (  71.451s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.300 (  71.451s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.300 (  71.451s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.300 (  71.451s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.300 (  71.451s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.300 (  71.451s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.300 (  71.451s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.300 (  71.451s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.300 (  71.451s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.300 (  71.451s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.300 (  71.451s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.300 (  71.451s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.300 (  71.451s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.300 (  71.451s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.300 (  71.451s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.301 (  71.451s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.301 (  71.451s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.301 (  71.451s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.301 (  71.451s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.301 (  71.451s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.301 (  71.451s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.301 (  71.452s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.301 (  71.452s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.301 (  71.452s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.301 (  71.452s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.301 (  71.452s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.301 (  71.452s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.301 (  71.452s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.301 (  71.452s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.308 (  71.458s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.325 (  71.476s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.342 (  71.492s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.358 (  71.509s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.375 (  71.525s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.392 (  71.543s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.409 (  71.560s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.426 (  71.577s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.443 (  71.594s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.460 (  71.610s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.476 (  71.627s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.492 (  71.643s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.509 (  71.659s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.525 (  71.676s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.541 (  71.692s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.557 (  71.708s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.573 (  71.724s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.589 (  71.740s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.605 (  71.756s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.622 (  71.772s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.636 (  71.787s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.653 (  71.803s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.669 (  71.820s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.685 (  71.836s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.702 (  71.852s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.718 (  71.869s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.734 (  71.884s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.749 (  71.900s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.765 (  71.916s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.780 (  71.931s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.796 (  71.946s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:25.811 (  71.962s) [        CE2D1B80]     buffer_instance.cc:509      1| BufferInstance::PJRT_Buffer_Destroy
2025-12-04 19:55:26.195 (  72.346s) [        CE2D1B80]     client_instance.cc:197      1| ClientInstance::~ClientInstance
2025-12-04 19:55:26.195 (  72.346s) [        CE2D1B80]     client_instance.cc:601      1| Closing parent mesh.
