#loc1 = loc("p0.3")
#loc2 = loc("p1.13")
#loc3 = loc("p2.30")
#loc4 = loc("p3.38")
#loc5 = loc("p4.43")
#loc6 = loc("p5.79")
#loc7 = loc("p6.121")
#loc8 = loc("p7.129")
#loc9 = loc("p8.151")
#loc10 = loc("p9.159")
#loc11 = loc("p10.168")
#loc12 = loc("p11.173")
#loc13 = loc("p12.182")
#loc14 = loc("p13.286")
#loc15 = loc("p14.321")
#loc16 = loc("p15.358")
#loc17 = loc("p16.448")
#loc18 = loc("p17.457")
#loc19 = loc("p18.503")
#loc74 = loc("scatter.127")
#loc81 = loc("scatter.157")
#loc162 = loc("dot.414")
#loc198 = loc("dot.469")
module @SyncTensorsGraph.516 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]> loc(#loc)
  func.func @main(%arg0: tensor<32xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"} loc("p0.3"), %arg1: tensor<64xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_rotary_emb_inv_freq"} loc("p1.13"), %arg2: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_k_proj_weight"} loc("p2.30"), %arg3: tensor<1x32xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"} loc("p3.38"), %arg4: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_embed_tokens_weight"} loc("p4.43"), %arg5: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___input_layernorm_weight"} loc("p5.79"), %arg6: tensor<1x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_2"} loc("p6.121"), %arg7: tensor<1024x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_v_proj_weight"} loc("p7.129"), %arg8: tensor<1x8x128x128xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_4"} loc("p8.151"), %arg9: tensor<128256x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___lm_head_weight"} loc("p9.159"), %arg10: tensor<3072x8192xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_down_proj_weight"} loc("p10.168"), %arg11: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_up_proj_weight"} loc("p11.173"), %arg12: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_o_proj_weight"} loc("p12.182"), %arg13: tensor<1x32xi64> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_3"} loc("p13.286"), %arg14: tensor<i1> {ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p14.321"), %arg15: tensor<3072x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___self_attn_q_proj_weight"} loc("p15.358"), %arg16: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___post_attention_layernorm_weight"} loc("p16.448"), %arg17: tensor<8192x3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_layers__modules__0___mlp_gate_proj_weight"} loc("p17.457"), %arg18: tensor<3072xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_norm_weight"} loc("p18.503")) -> (tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<32x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x32x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:4 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %arg18) in_shardings=[<@mesh, [{}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}, {"_axis_0"}]>, <@mesh, [{}, {}]>, <@mesh, []>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>] out_shardings=[<@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {"_axis_0"}, {}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg19: tensor<32xi64> loc("p0.3"), %arg20: tensor<64xf32> loc("p1.13"), %arg21: tensor<512x3072xbf16> loc("p2.30"), %arg22: tensor<1x32xi64> loc("p3.38"), %arg23: tensor<128256x3072xbf16> loc("p4.43"), %arg24: tensor<3072xbf16> loc("p5.79"), %arg25: tensor<1x4x128x128xbf16> loc("p6.121"), %arg26: tensor<512x3072xbf16> loc("p7.129"), %arg27: tensor<1x4x128x128xbf16> loc("p8.151"), %arg28: tensor<128256x3072xbf16> loc("p9.159"), %arg29: tensor<3072x4096xbf16> loc("p10.168"), %arg30: tensor<4096x3072xbf16> loc("p11.173"), %arg31: tensor<3072x1536xbf16> loc("p12.182"), %arg32: tensor<1x32xi64> loc("p13.286"), %arg33: tensor<i1> loc("p14.321"), %arg34: tensor<1536x3072xbf16> loc("p15.358"), %arg35: tensor<3072xbf16> loc("p16.448"), %arg36: tensor<4096x3072xbf16> loc("p17.457"), %arg37: tensor<3072xbf16> loc("p18.503")) {
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
      %c = stablehlo.constant dense<"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tensor<128xi64> loc(#loc)
      %c_0 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64> loc(#loc)
      %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)
      %c_2 = stablehlo.constant dense<128> : tensor<i64> loc(#loc)
      %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32> loc(#loc)
      %cst_4 = stablehlo.constant dense<3.25520843E-4> : tensor<f32> loc(#loc)
      %cst_5 = stablehlo.constant dense<9.99999974E-6> : tensor<f32> loc(#loc)
      %cst_6 = stablehlo.constant dense<8.837890e-02> : tensor<bf16> loc(#loc)
      %c_7 = stablehlo.constant dense<"0xFFFFFFFF000000000000000000000000"> : tensor<128xi1> loc(#loc)
      %c_8 = stablehlo.constant dense<1> : tensor<i64> loc(#loc)
      %cst_9 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
      %cst_10 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16> loc(#loc)
      %c_11 = stablehlo.constant dense<0> : tensor<i64> loc(#loc)
      %cst_12 = stablehlo.constant dense<"0x000000000000803F0000004000004040000080400000A0400000C0400000E0400000004100001041000020410000304100004041000050410000604100007041000080410000884100009041000098410000A0410000A8410000B0410000B8410000C0410000C8410000D0410000D8410000E0410000E8410000F0410000F84100000042000004420000084200000C4200001042000014420000184200001C4200002042000024420000284200002C4200003042000034420000384200003C4200004042000044420000484200004C4200005042000054420000584200005C4200006042000064420000684200006C4200007042000074420000784200007C42000080420000824200008442000086420000884200008A4200008C4200008E42000090420000924200009442000096420000984200009A4200009C4200009E420000A0420000A2420000A4420000A6420000A8420000AA420000AC420000AE420000B0420000B2420000B4420000B6420000B8420000BA420000BC420000BE420000C0420000C2420000C4420000C6420000C8420000CA420000CC420000CE420000D0420000D2420000D4420000D6420000D8420000DA420000DC420000DE420000E0420000E2420000E4420000E6420000E8420000EA420000EC420000EE420000F0420000F2420000F4420000F6420000F8420000FA420000FC420000FE42"> : tensor<128xf32> loc(#loc)
      %c_13 = stablehlo.constant dense<31> : tensor<i64> loc(#loc)
      %c_14 = stablehlo.constant dense<32> : tensor<i64> loc(#loc)
      %1 = stablehlo.broadcast_in_dim %c_14, dims = [] : (tensor<i64>) -> tensor<128xi64> loc(#loc)
      %2 = stablehlo.broadcast_in_dim %c_13, dims = [] : (tensor<i64>) -> tensor<128xi64> loc(#loc)
      %3 = stablehlo.broadcast_in_dim %c_11, dims = [] : (tensor<i64>) -> tensor<128xi64> loc(#loc)
      %4 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<1x1x32x32xbf16> loc(#loc)
      %5 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<1x1x32x32xbf16> loc(#loc)
      %6 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<32x128xbf16> loc(#loc)
      %7 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<32x128xbf16> loc(#loc)
      %8 = stablehlo.broadcast_in_dim %c_8, dims = [] : (tensor<i64>) -> tensor<32x128xi64> loc(#loc)
      %9 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<1x1x32x128xbf16> loc(#loc)
      %10 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<1x12x32x128xbf16> loc(#loc)
      %11 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<1x32x1xf32> loc(#loc)
      %12 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<1x32xf32> loc(#loc)
      %13 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x32x3072xf32> loc(#loc)
      %14 = stablehlo.broadcast_in_dim %c_2, dims = [] : (tensor<i64>) -> tensor<32xi64> loc(#loc)
      %15 = stablehlo.broadcast_in_dim %c_11, dims = [] : (tensor<i64>) -> tensor<32xi64> loc(#loc)
      %16 = stablehlo.reshape %arg19 : (tensor<32xi64>) -> tensor<1x1x32xi64> loc(#loc20)
      %17 = stablehlo.reshape %16 : (tensor<1x1x32xi64>) -> tensor<32xi64> loc(#loc21)
      %18 = stablehlo.compare  LT, %17, %15 : (tensor<32xi64>, tensor<32xi64>) -> tensor<32xi1> loc(#loc22)
      %19 = stablehlo.add %17, %14 : tensor<32xi64> loc(#loc23)
      %20 = stablehlo.select %18, %19, %17 : tensor<32xi1>, tensor<32xi64> loc(#loc24)
      %21 = stablehlo.reshape %20 : (tensor<32xi64>) -> tensor<32x1xi64> loc(#loc25)
      %22 = stablehlo.reshape %arg24 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc26)
      %23 = stablehlo.reshape %22 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc27)
      %24 = stablehlo.broadcast_in_dim %23, dims = [2] : (tensor<3072xbf16>) -> tensor<1x32x3072xbf16> loc(#loc28)
      %25 = stablehlo.reshape %arg23 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16> loc(#loc29)
      %26 = stablehlo.reshape %25 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16> loc(#loc30)
      %27 = stablehlo.reshape %arg22 : (tensor<1x32xi64>) -> tensor<1x1x32xi64> loc(#loc31)
      %28 = stablehlo.reshape %27 : (tensor<1x1x32xi64>) -> tensor<32xi64> loc(#loc32)
      %29 = stablehlo.convert %28 : (tensor<32xi64>) -> tensor<32xui32> loc(#loc33)
      %30 = "stablehlo.gather"(%26, %29) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<32xui32>) -> tensor<32x3072xbf16> loc(#loc34)
      %31 = stablehlo.reshape %30 : (tensor<32x3072xbf16>) -> tensor<1x32x3072xbf16> loc(#loc35)
      %32 = stablehlo.convert %31 : (tensor<1x32x3072xbf16>) -> tensor<1x32x3072xf32> loc(#loc36)
      %33 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x32x3072xf32> loc(#loc)
      %34 = stablehlo.power %32, %33 : tensor<1x32x3072xf32> loc(#loc37)
      %35 = stablehlo.reduce(%34 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x32x3072xf32>, tensor<f32>) -> tensor<1x32xf32> loc(#loc38)
      %36 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<1x32xf32> loc(#loc)
      %37 = stablehlo.multiply %35, %36 : tensor<1x32xf32> loc(#loc39)
      %38 = stablehlo.reshape %37 : (tensor<1x32xf32>) -> tensor<1x32x1xf32> loc(#loc40)
      %39 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<1x32x1xf32> loc(#loc)
      %40 = stablehlo.add %38, %39 : tensor<1x32x1xf32> loc(#loc41)
      %41 = stablehlo.rsqrt %40 : tensor<1x32x1xf32> loc(#loc42)
      %42 = stablehlo.reshape %41 : (tensor<1x32x1xf32>) -> tensor<1x32xf32> loc(#loc43)
      %43 = stablehlo.broadcast_in_dim %42, dims = [0, 1] : (tensor<1x32xf32>) -> tensor<1x32x3072xf32> loc(#loc44)
      %44 = stablehlo.multiply %32, %43 : tensor<1x32x3072xf32> loc(#loc45)
      %45 = stablehlo.convert %44 : (tensor<1x32x3072xf32>) -> tensor<1x32x3072xbf16> loc(#loc46)
      %46 = stablehlo.multiply %24, %45 : tensor<1x32x3072xbf16> loc(#loc47)
      %47 = stablehlo.reshape %46 : (tensor<1x32x3072xbf16>) -> tensor<32x3072xbf16> loc(#loc48)
      %48 = stablehlo.reshape %arg21 : (tensor<512x3072xbf16>) -> tensor<1x512x3072xbf16> loc(#loc49)
      %49 = stablehlo.reshape %48 : (tensor<1x512x3072xbf16>) -> tensor<512x3072xbf16> loc(#loc50)
      %50 = stablehlo.transpose %49, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<512x3072xbf16>) -> tensor<3072x512xbf16> loc(#loc51)
      %51 = stablehlo.dot_general %47, %50, contracting_dims = [1] x [0] : (tensor<32x3072xbf16>, tensor<3072x512xbf16>) -> tensor<32x512xbf16> loc(#loc52)
      %52 = stablehlo.reshape %51 : (tensor<32x512xbf16>) -> tensor<1x32x4x128xbf16> loc(#loc53)
      %53 = stablehlo.transpose %52, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,32,128]{3,1,2,0}"} : (tensor<1x32x4x128xbf16>) -> tensor<1x4x32x128xbf16> loc(#loc54)
      %54 = stablehlo.reshape %arg20 : (tensor<64xf32>) -> tensor<1x1x64xf32> loc(#loc55)
      %55 = stablehlo.reshape %54 : (tensor<1x1x64xf32>) -> tensor<1x64x1xf32> loc(#loc56)
      %56 = stablehlo.convert %16 : (tensor<1x1x32xi64>) -> tensor<1x1x32xf32> loc(#loc57)
      %57 = stablehlo.dot_general %55, %56, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x32xf32>) -> tensor<1x64x32xf32> loc(#loc58)
      %58 = stablehlo.transpose %57, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,32,64]{1,2,0}"} : (tensor<1x64x32xf32>) -> tensor<1x32x64xf32> loc(#loc59)
      %59 = stablehlo.concatenate %58, %58, dim = 2 : (tensor<1x32x64xf32>, tensor<1x32x64xf32>) -> tensor<1x32x128xf32> loc(#loc60)
      %60 = stablehlo.cosine %59 : tensor<1x32x128xf32> loc(#loc61)
      %61 = stablehlo.convert %60 : (tensor<1x32x128xf32>) -> tensor<1x32x128xbf16> loc(#loc62)
      %62 = stablehlo.broadcast_in_dim %61, dims = [0, 2, 3] : (tensor<1x32x128xbf16>) -> tensor<1x4x32x128xbf16> loc(#loc63)
      %63 = stablehlo.multiply %53, %62 : tensor<1x4x32x128xbf16> loc(#loc64)
      %64 = stablehlo.slice %53 [0:1, 0:4, 0:32, 64:128] : (tensor<1x4x32x128xbf16>) -> tensor<1x4x32x64xbf16> loc(#loc65)
      %65 = stablehlo.negate %64 : tensor<1x4x32x64xbf16> loc(#loc66)
      %66 = stablehlo.slice %53 [0:1, 0:4, 0:32, 0:64] : (tensor<1x4x32x128xbf16>) -> tensor<1x4x32x64xbf16> loc(#loc67)
      %67 = stablehlo.concatenate %65, %66, dim = 3 : (tensor<1x4x32x64xbf16>, tensor<1x4x32x64xbf16>) -> tensor<1x4x32x128xbf16> loc(#loc68)
      %68 = stablehlo.sine %59 : tensor<1x32x128xf32> loc(#loc69)
      %69 = stablehlo.convert %68 : (tensor<1x32x128xf32>) -> tensor<1x32x128xbf16> loc(#loc70)
      %70 = stablehlo.broadcast_in_dim %69, dims = [0, 2, 3] : (tensor<1x32x128xbf16>) -> tensor<1x4x32x128xbf16> loc(#loc71)
      %71 = stablehlo.multiply %67, %70 : tensor<1x4x32x128xbf16> loc(#loc72)
      %72 = stablehlo.add %63, %71 : tensor<1x4x32x128xbf16> loc(#loc73)
      %73 = "stablehlo.scatter"(%arg25, %21, %72) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg38: tensor<bf16> loc("scatter.127"), %arg39: tensor<bf16> loc("scatter.127")):
        stablehlo.return %arg39 : tensor<bf16> loc(#loc)
      }) : (tensor<1x4x128x128xbf16>, tensor<32x1xi64>, tensor<1x4x32x128xbf16>) -> tensor<1x4x128x128xbf16> loc(#loc74)
      %74 = stablehlo.reshape %arg26 : (tensor<512x3072xbf16>) -> tensor<1x512x3072xbf16> loc(#loc75)
      %75 = stablehlo.reshape %74 : (tensor<1x512x3072xbf16>) -> tensor<512x3072xbf16> loc(#loc76)
      %76 = stablehlo.transpose %75, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,1024]{0,1}"} : (tensor<512x3072xbf16>) -> tensor<3072x512xbf16> loc(#loc77)
      %77 = stablehlo.dot_general %47, %76, contracting_dims = [1] x [0] : (tensor<32x3072xbf16>, tensor<3072x512xbf16>) -> tensor<32x512xbf16> loc(#loc78)
      %78 = stablehlo.reshape %77 : (tensor<32x512xbf16>) -> tensor<1x32x4x128xbf16> loc(#loc79)
      %79 = stablehlo.transpose %78, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,32,128]{3,1,2,0}"} : (tensor<1x32x4x128xbf16>) -> tensor<1x4x32x128xbf16> loc(#loc80)
      %80 = "stablehlo.scatter"(%arg27, %21, %79) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg38: tensor<bf16> loc("scatter.157"), %arg39: tensor<bf16> loc("scatter.157")):
        stablehlo.return %arg39 : tensor<bf16> loc(#loc)
      }) : (tensor<1x4x128x128xbf16>, tensor<32x1xi64>, tensor<1x4x32x128xbf16>) -> tensor<1x4x128x128xbf16> loc(#loc81)
      %81 = stablehlo.reshape %arg37 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc82)
      %82 = stablehlo.reshape %81 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc83)
      %83 = stablehlo.broadcast_in_dim %82, dims = [2] : (tensor<3072xbf16>) -> tensor<1x32x3072xbf16> loc(#loc84)
      %84 = stablehlo.reshape %arg34 : (tensor<1536x3072xbf16>) -> tensor<1x1536x3072xbf16> loc(#loc85)
      %85 = stablehlo.reshape %84 : (tensor<1x1536x3072xbf16>) -> tensor<1536x3072xbf16> loc(#loc86)
      %86 = stablehlo.transpose %85, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<1536x3072xbf16>) -> tensor<3072x1536xbf16> loc(#loc87)
      %87 = stablehlo.dot_general %47, %86, contracting_dims = [1] x [0] : (tensor<32x3072xbf16>, tensor<3072x1536xbf16>) -> tensor<32x1536xbf16> loc(#loc88)
      %88 = stablehlo.reshape %87 : (tensor<32x1536xbf16>) -> tensor<1x32x12x128xbf16> loc(#loc89)
      %89 = stablehlo.transpose %88, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,32,128]{3,1,2,0}"} : (tensor<1x32x12x128xbf16>) -> tensor<1x12x32x128xbf16> loc(#loc90)
      %90 = stablehlo.broadcast_in_dim %61, dims = [0, 2, 3] : (tensor<1x32x128xbf16>) -> tensor<1x12x32x128xbf16> loc(#loc91)
      %91 = stablehlo.multiply %89, %90 : tensor<1x12x32x128xbf16> loc(#loc92)
      %92 = stablehlo.slice %89 [0:1, 0:12, 0:32, 64:128] : (tensor<1x12x32x128xbf16>) -> tensor<1x12x32x64xbf16> loc(#loc93)
      %93 = stablehlo.negate %92 : tensor<1x12x32x64xbf16> loc(#loc94)
      %94 = stablehlo.slice %89 [0:1, 0:12, 0:32, 0:64] : (tensor<1x12x32x128xbf16>) -> tensor<1x12x32x64xbf16> loc(#loc95)
      %95 = stablehlo.concatenate %93, %94, dim = 3 : (tensor<1x12x32x64xbf16>, tensor<1x12x32x64xbf16>) -> tensor<1x12x32x128xbf16> loc(#loc96)
      %96 = stablehlo.broadcast_in_dim %69, dims = [0, 2, 3] : (tensor<1x32x128xbf16>) -> tensor<1x12x32x128xbf16> loc(#loc97)
      %97 = stablehlo.multiply %95, %96 : tensor<1x12x32x128xbf16> loc(#loc98)
      %98 = stablehlo.add %91, %97 : tensor<1x12x32x128xbf16> loc(#loc99)
      %99 = stablehlo.broadcast_in_dim %73, dims = [0, 1, 3, 4] : (tensor<1x4x128x128xbf16>) -> tensor<1x4x3x128x128xbf16> loc(#loc100)
      %100 = stablehlo.reshape %99 : (tensor<1x4x3x128x128xbf16>) -> tensor<1x12x128x128xbf16> loc(#loc101)
      %101 = stablehlo.transpose %100, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x12x128x128xbf16>) -> tensor<1x12x128x128xbf16> loc(#loc102)
      %102 = stablehlo.dot_general %98, %101, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x12x32x128xbf16>, tensor<1x12x128x128xbf16>) -> tensor<1x12x32x128xbf16> loc(#loc103)
      %103 = stablehlo.multiply %102, %10 : tensor<1x12x32x128xbf16> loc(#loc104)
      %104 = stablehlo.broadcast_in_dim %arg33, dims = [] : (tensor<i1>) -> tensor<128xi1> loc(#loc105)
      %105 = stablehlo.and %104, %c_7 : tensor<128xi1> loc(#loc106)
      %106 = stablehlo.reshape %105 : (tensor<128xi1>) -> tensor<1x1x1x128xi1> loc(#loc107)
      %107 = stablehlo.reshape %105 : (tensor<128xi1>) -> tensor<1x1x128xi1> loc(#loc108)
      %108 = stablehlo.broadcast_in_dim %107, dims = [0, 1, 3] : (tensor<1x1x128xi1>) -> tensor<1x1x32x128xi1> loc(#loc109)
      %109 = stablehlo.not %106 : tensor<1x1x1x128xi1> loc(#loc110)
      %110 = stablehlo.reshape %109 : (tensor<1x1x1x128xi1>) -> tensor<1x1x128xi1> loc(#loc111)
      %111 = stablehlo.broadcast_in_dim %110, dims = [0, 1, 3] : (tensor<1x1x128xi1>) -> tensor<1x1x32x128xi1> loc(#loc112)
      %112 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<128xi64>) -> tensor<32x128xi64> loc(#loc113)
      %113 = stablehlo.broadcast_in_dim %c_0, dims = [0] : (tensor<32xi64>) -> tensor<32x128xi64> loc(#loc114)
      %114 = stablehlo.broadcast_in_dim %c, dims = [1] : (tensor<128xi64>) -> tensor<32x128xi64> loc(#loc113)
      %115 = stablehlo.subtract %114, %113 : tensor<32x128xi64> loc(#loc115)
      %116 = stablehlo.compare  GE, %115, %8 : (tensor<32x128xi64>, tensor<32x128xi64>) -> tensor<32x128xi1> loc(#loc116)
      %117 = stablehlo.select %116, %7, %6 : tensor<32x128xi1>, tensor<32x128xbf16> loc(#loc117)
      %118 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<32xi64>) -> tensor<32x128xi64> loc(#loc118)
      %119 = stablehlo.compare  GT, %112, %118 : (tensor<32x128xi64>, tensor<32x128xi64>) -> tensor<32x128xi1> loc(#loc119)
      %120 = stablehlo.convert %119 : (tensor<32x128xi1>) -> tensor<32x128xbf16> loc(#loc120)
      %121 = stablehlo.multiply %117, %120 : tensor<32x128xbf16> loc(#loc121)
      %122 = stablehlo.reshape %121 : (tensor<32x128xbf16>) -> tensor<1x1x32x128xbf16> loc(#loc122)
      %123 = stablehlo.slice %122 [0:1, 0:1, 0:32, 0:32] : (tensor<1x1x32x128xbf16>) -> tensor<1x1x32x32xbf16> loc(#loc123)
      %124 = stablehlo.reshape %arg32 : (tensor<1x32xi64>) -> tensor<1x1x32xi64> loc(#loc124)
      %125 = stablehlo.reshape %124 : (tensor<1x1x32xi64>) -> tensor<1x1x1x32xi64> loc(#loc125)
      %126 = stablehlo.convert %125 : (tensor<1x1x1x32xi64>) -> tensor<1x1x1x32xbf16> loc(#loc126)
      %127 = stablehlo.reshape %126 : (tensor<1x1x1x32xbf16>) -> tensor<1x1x32xbf16> loc(#loc127)
      %128 = stablehlo.broadcast_in_dim %127, dims = [0, 1, 3] : (tensor<1x1x32xbf16>) -> tensor<1x1x32x32xbf16> loc(#loc128)
      %129 = stablehlo.add %123, %128 : tensor<1x1x32x32xbf16> loc(#loc129)
      %130 = stablehlo.compare  EQ, %129, %5 : (tensor<1x1x32x32xbf16>, tensor<1x1x32x32xbf16>) -> tensor<1x1x32x32xi1> loc(#loc130)
      %131 = stablehlo.select %130, %4, %123 : tensor<1x1x32x32xi1>, tensor<1x1x32x32xbf16> loc(#loc131)
      %132 = stablehlo.floor %cst_12 : tensor<128xf32> loc(#loc132)
      %133 = stablehlo.convert %132 : (tensor<128xf32>) -> tensor<128xi64> loc(#loc133)
      %134 = stablehlo.broadcast_in_dim %c_11, dims = [] : (tensor<i64>) -> tensor<128xi64> loc(#loc)
      %135 = stablehlo.clamp %134, %133, %2 : tensor<128xi64> loc(#loc134)
      %136 = stablehlo.compare  LT, %135, %3 : (tensor<128xi64>, tensor<128xi64>) -> tensor<128xi1> loc(#loc135)
      %137 = stablehlo.add %135, %1 : tensor<128xi64> loc(#loc136)
      %138 = stablehlo.select %136, %137, %135 : tensor<128xi1>, tensor<128xi64> loc(#loc137)
      %139 = stablehlo.reshape %138 : (tensor<128xi64>) -> tensor<128x1xi64> loc(#loc138)
      %140 = "stablehlo.gather"(%131, %139) <{dimension_numbers = #stablehlo.gather<offset_dims = [0, 1, 2], collapsed_slice_dims = [3], start_index_map = [3], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1, 32, 1>}> : (tensor<1x1x32x32xbf16>, tensor<128x1xi64>) -> tensor<1x1x32x128xbf16> loc(#loc139)
      %141 = stablehlo.select %111, %9, %140 : tensor<1x1x32x128xi1>, tensor<1x1x32x128xbf16> loc(#loc140)
      %142 = stablehlo.select %108, %141, %122 : tensor<1x1x32x128xi1>, tensor<1x1x32x128xbf16> loc(#loc141)
      %143 = stablehlo.reshape %142 : (tensor<1x1x32x128xbf16>) -> tensor<1x32x128xbf16> loc(#loc142)
      %144 = stablehlo.broadcast_in_dim %143, dims = [0, 2, 3] : (tensor<1x32x128xbf16>) -> tensor<1x12x32x128xbf16> loc(#loc143)
      %145 = stablehlo.add %103, %144 : tensor<1x12x32x128xbf16> loc(#loc144)
      %146 = stablehlo.convert %145 : (tensor<1x12x32x128xbf16>) -> tensor<1x12x32x128xf32> loc(#loc145)
      %147 = stablehlo.reduce(%146 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x12x32x128xf32>, tensor<f32>) -> tensor<1x12x32xf32> loc(#loc146)
      %148 = stablehlo.broadcast_in_dim %147, dims = [0, 1, 2] : (tensor<1x12x32xf32>) -> tensor<1x12x32x128xf32> loc(#loc147)
      %149 = stablehlo.subtract %146, %148 : tensor<1x12x32x128xf32> loc(#loc148)
      %150 = stablehlo.exponential %149 : tensor<1x12x32x128xf32> loc(#loc149)
      %151 = stablehlo.reduce(%150 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x12x32x128xf32>, tensor<f32>) -> tensor<1x12x32xf32> loc(#loc150)
      %152 = stablehlo.broadcast_in_dim %151, dims = [0, 1, 2] : (tensor<1x12x32xf32>) -> tensor<1x12x32x128xf32> loc(#loc151)
      %153 = stablehlo.divide %150, %152 : tensor<1x12x32x128xf32> loc(#loc152)
      %154 = stablehlo.convert %153 : (tensor<1x12x32x128xf32>) -> tensor<1x12x32x128xbf16> loc(#loc153)
      %155 = stablehlo.broadcast_in_dim %80, dims = [0, 1, 3, 4] : (tensor<1x4x128x128xbf16>) -> tensor<1x4x3x128x128xbf16> loc(#loc154)
      %156 = stablehlo.reshape %155 : (tensor<1x4x3x128x128xbf16>) -> tensor<1x12x128x128xbf16> loc(#loc155)
      %157 = stablehlo.dot_general %154, %156, batching_dims = [0, 1] x [0, 1], contracting_dims = [3] x [2] {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<1x12x32x128xbf16>, tensor<1x12x128x128xbf16>) -> tensor<1x12x32x128xbf16> loc(#loc156)
      %158 = stablehlo.transpose %157, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,32,24,128]{3,1,2,0}"} : (tensor<1x12x32x128xbf16>) -> tensor<1x32x12x128xbf16> loc(#loc157)
      %159 = stablehlo.reshape %158 : (tensor<1x32x12x128xbf16>) -> tensor<32x1536xbf16> loc(#loc158)
      %160 = stablehlo.reshape %arg31 : (tensor<3072x1536xbf16>) -> tensor<1x3072x1536xbf16> loc(#loc159)
      %161 = stablehlo.reshape %160 : (tensor<1x3072x1536xbf16>) -> tensor<3072x1536xbf16> loc(#loc160)
      %162 = stablehlo.transpose %161, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,3072]{0,1}"} : (tensor<3072x1536xbf16>) -> tensor<1536x3072xbf16> loc(#loc161)
      %163 = stablehlo.dot_general %159, %162, contracting_dims = [1] x [0] : (tensor<32x1536xbf16>, tensor<1536x3072xbf16>) -> tensor<32x3072xbf16> loc(#loc162)
      %164 = "stablehlo.all_reduce"(%163) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg38: tensor<bf16> loc("dot.414"), %arg39: tensor<bf16> loc("dot.414")):
        %225 = stablehlo.add %arg38, %arg39 : tensor<bf16> loc(#loc162)
        stablehlo.return %225 : tensor<bf16> loc(#loc162)
      }) : (tensor<32x3072xbf16>) -> tensor<32x3072xbf16> loc(#loc162)
      %165 = stablehlo.reshape %164 : (tensor<32x3072xbf16>) -> tensor<1x32x3072xbf16> loc(#loc163)
      %166 = stablehlo.add %31, %165 : tensor<1x32x3072xbf16> loc(#loc164)
      %167 = stablehlo.reshape %arg35 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc165)
      %168 = stablehlo.reshape %167 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc166)
      %169 = stablehlo.broadcast_in_dim %168, dims = [2] : (tensor<3072xbf16>) -> tensor<1x32x3072xbf16> loc(#loc167)
      %170 = stablehlo.convert %166 : (tensor<1x32x3072xbf16>) -> tensor<1x32x3072xf32> loc(#loc168)
      %171 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1x32x3072xf32> loc(#loc)
      %172 = stablehlo.power %170, %171 : tensor<1x32x3072xf32> loc(#loc169)
      %173 = stablehlo.reduce(%172 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x32x3072xf32>, tensor<f32>) -> tensor<1x32xf32> loc(#loc170)
      %174 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<1x32xf32> loc(#loc)
      %175 = stablehlo.multiply %173, %174 : tensor<1x32xf32> loc(#loc171)
      %176 = stablehlo.reshape %175 : (tensor<1x32xf32>) -> tensor<1x32x1xf32> loc(#loc172)
      %177 = stablehlo.broadcast_in_dim %cst_5, dims = [] : (tensor<f32>) -> tensor<1x32x1xf32> loc(#loc)
      %178 = stablehlo.add %176, %177 : tensor<1x32x1xf32> loc(#loc173)
      %179 = stablehlo.rsqrt %178 : tensor<1x32x1xf32> loc(#loc174)
      %180 = stablehlo.reshape %179 : (tensor<1x32x1xf32>) -> tensor<1x32xf32> loc(#loc175)
      %181 = stablehlo.broadcast_in_dim %180, dims = [0, 1] : (tensor<1x32xf32>) -> tensor<1x32x3072xf32> loc(#loc176)
      %182 = stablehlo.multiply %170, %181 : tensor<1x32x3072xf32> loc(#loc177)
      %183 = stablehlo.convert %182 : (tensor<1x32x3072xf32>) -> tensor<1x32x3072xbf16> loc(#loc178)
      %184 = stablehlo.multiply %169, %183 : tensor<1x32x3072xbf16> loc(#loc179)
      %185 = stablehlo.reshape %184 : (tensor<1x32x3072xbf16>) -> tensor<32x3072xbf16> loc(#loc180)
      %186 = stablehlo.reshape %arg36 : (tensor<4096x3072xbf16>) -> tensor<1x4096x3072xbf16> loc(#loc181)
      %187 = stablehlo.reshape %186 : (tensor<1x4096x3072xbf16>) -> tensor<4096x3072xbf16> loc(#loc182)
      %188 = stablehlo.transpose %187, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<4096x3072xbf16>) -> tensor<3072x4096xbf16> loc(#loc183)
      %189 = stablehlo.dot_general %185, %188, contracting_dims = [1] x [0] : (tensor<32x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<32x4096xbf16> loc(#loc184)
      %190 = stablehlo.reshape %189 : (tensor<32x4096xbf16>) -> tensor<1x32x4096xbf16> loc(#loc185)
      %191 = stablehlo.logistic %190 : tensor<1x32x4096xbf16> loc(#loc186)
      %192 = stablehlo.multiply %190, %191 : tensor<1x32x4096xbf16> loc(#loc187)
      %193 = stablehlo.reshape %arg30 : (tensor<4096x3072xbf16>) -> tensor<1x4096x3072xbf16> loc(#loc188)
      %194 = stablehlo.reshape %193 : (tensor<1x4096x3072xbf16>) -> tensor<4096x3072xbf16> loc(#loc189)
      %195 = stablehlo.transpose %194, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,8192]{0,1}"} : (tensor<4096x3072xbf16>) -> tensor<3072x4096xbf16> loc(#loc190)
      %196 = stablehlo.dot_general %185, %195, contracting_dims = [1] x [0] : (tensor<32x3072xbf16>, tensor<3072x4096xbf16>) -> tensor<32x4096xbf16> loc(#loc191)
      %197 = stablehlo.reshape %196 : (tensor<32x4096xbf16>) -> tensor<1x32x4096xbf16> loc(#loc192)
      %198 = stablehlo.multiply %192, %197 : tensor<1x32x4096xbf16> loc(#loc193)
      %199 = stablehlo.reshape %198 : (tensor<1x32x4096xbf16>) -> tensor<32x4096xbf16> loc(#loc194)
      %200 = stablehlo.reshape %arg29 : (tensor<3072x4096xbf16>) -> tensor<1x3072x4096xbf16> loc(#loc195)
      %201 = stablehlo.reshape %200 : (tensor<1x3072x4096xbf16>) -> tensor<3072x4096xbf16> loc(#loc196)
      %202 = stablehlo.transpose %201, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[8192,3072]{0,1}"} : (tensor<3072x4096xbf16>) -> tensor<4096x3072xbf16> loc(#loc197)
      %203 = stablehlo.dot_general %199, %202, contracting_dims = [1] x [0] : (tensor<32x4096xbf16>, tensor<4096x3072xbf16>) -> tensor<32x3072xbf16> loc(#loc198)
      %204 = "stablehlo.all_reduce"(%203) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 1]]> : tensor<1x2xi64>}> ({
      ^bb0(%arg38: tensor<bf16> loc("dot.469"), %arg39: tensor<bf16> loc("dot.469")):
        %225 = stablehlo.add %arg38, %arg39 : tensor<bf16> loc(#loc198)
        stablehlo.return %225 : tensor<bf16> loc(#loc198)
      }) : (tensor<32x3072xbf16>) -> tensor<32x3072xbf16> loc(#loc198)
      %205 = stablehlo.reshape %204 : (tensor<32x3072xbf16>) -> tensor<1x32x3072xbf16> loc(#loc199)
      %206 = stablehlo.add %166, %205 : tensor<1x32x3072xbf16> loc(#loc200)
      %207 = stablehlo.convert %206 : (tensor<1x32x3072xbf16>) -> tensor<1x32x3072xf32> loc(#loc201)
      %208 = stablehlo.power %207, %13 : tensor<1x32x3072xf32> loc(#loc202)
      %209 = stablehlo.reduce(%208 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x32x3072xf32>, tensor<f32>) -> tensor<1x32xf32> loc(#loc203)
      %210 = stablehlo.multiply %209, %12 : tensor<1x32xf32> loc(#loc204)
      %211 = stablehlo.reshape %210 : (tensor<1x32xf32>) -> tensor<1x32x1xf32> loc(#loc205)
      %212 = stablehlo.add %211, %11 : tensor<1x32x1xf32> loc(#loc206)
      %213 = stablehlo.rsqrt %212 : tensor<1x32x1xf32> loc(#loc207)
      %214 = stablehlo.reshape %213 : (tensor<1x32x1xf32>) -> tensor<1x32xf32> loc(#loc208)
      %215 = stablehlo.broadcast_in_dim %214, dims = [0, 1] : (tensor<1x32xf32>) -> tensor<1x32x3072xf32> loc(#loc209)
      %216 = stablehlo.multiply %207, %215 : tensor<1x32x3072xf32> loc(#loc210)
      %217 = stablehlo.convert %216 : (tensor<1x32x3072xf32>) -> tensor<1x32x3072xbf16> loc(#loc211)
      %218 = stablehlo.multiply %83, %217 : tensor<1x32x3072xbf16> loc(#loc212)
      %219 = stablehlo.reshape %218 : (tensor<1x32x3072xbf16>) -> tensor<32x3072xbf16> loc(#loc213)
      %220 = stablehlo.reshape %arg28 : (tensor<128256x3072xbf16>) -> tensor<1x128256x3072xbf16> loc(#loc214)
      %221 = stablehlo.reshape %220 : (tensor<1x128256x3072xbf16>) -> tensor<128256x3072xbf16> loc(#loc215)
      %222 = stablehlo.transpose %221, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,128256]{0,1}"} : (tensor<128256x3072xbf16>) -> tensor<3072x128256xbf16> loc(#loc216)
      %223 = stablehlo.dot_general %219, %222, contracting_dims = [1] x [0] : (tensor<32x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<32x128256xbf16> loc(#loc217)
      %224 = stablehlo.reshape %223 : (tensor<32x128256xbf16>) -> tensor<1x32x128256xbf16> loc(#loc218)
      sdy.return %73, %80, %223, %224 : tensor<1x4x128x128xbf16>, tensor<1x4x128x128xbf16>, tensor<32x128256xbf16>, tensor<1x32x128256xbf16> loc(#loc)
    } : (tensor<32xi64>, tensor<64xf32>, tensor<1024x3072xbf16>, tensor<1x32xi64>, tensor<128256x3072xbf16>, tensor<3072xbf16>, tensor<1x8x128x128xbf16>, tensor<1024x3072xbf16>, tensor<1x8x128x128xbf16>, tensor<128256x3072xbf16>, tensor<3072x8192xbf16>, tensor<8192x3072xbf16>, tensor<3072x3072xbf16>, tensor<1x32xi64>, tensor<i1>, tensor<3072x3072xbf16>, tensor<3072xbf16>, tensor<8192x3072xbf16>, tensor<3072xbf16>) -> (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<32x128256xbf16>, tensor<1x32x128256xbf16>) loc(#loc)
    return %0#0, %0#1, %0#2, %0#3 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<32x128256xbf16>, tensor<1x32x128256xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc20 = loc("reshape.4")
#loc21 = loc("reshape.6")
#loc22 = loc("compare.117")
#loc23 = loc("add.114")
#loc24 = loc("select.118")
#loc25 = loc("reshape.119")
#loc26 = loc("reshape.80")
#loc27 = loc("reshape.82")
#loc28 = loc("broadcast.83")
#loc29 = loc("reshape.44")
#loc30 = loc("reshape.46")
#loc31 = loc("reshape.39")
#loc32 = loc("reshape.42")
#loc33 = loc("convert.47")
#loc34 = loc("gather.48")
#loc35 = loc("reshape.49")
#loc36 = loc("convert.50")
#loc37 = loc("power.52")
#loc38 = loc("reduce.59")
#loc39 = loc("multiply.68")
#loc40 = loc("reshape.69")
#loc41 = loc("add.73")
#loc42 = loc("rsqrt.74")
#loc43 = loc("reshape.75")
#loc44 = loc("broadcast.76")
#loc45 = loc("multiply.77")
#loc46 = loc("convert.78")
#loc47 = loc("multiply.84")
#loc48 = loc("reshape.85")
#loc49 = loc("reshape.31")
#loc50 = loc("reshape.33")
#loc51 = loc("transpose.34")
#loc52 = loc("dot.86")
#loc53 = loc("reshape.88")
#loc54 = loc("transpose.89")
#loc55 = loc("reshape.14")
#loc56 = loc("reshape.19")
#loc57 = loc("convert.11")
#loc58 = loc("dot.22")
#loc59 = loc("transpose.23")
#loc60 = loc("concatenate.24")
#loc61 = loc("cosine.98")
#loc62 = loc("convert.101")
#loc63 = loc("broadcast.104")
#loc64 = loc("multiply.105")
#loc65 = loc("slice.91")
#loc66 = loc("negate.92")
#loc67 = loc("slice.90")
#loc68 = loc("concatenate.93")
#loc69 = loc("sine.25")
#loc70 = loc("convert.28")
#loc71 = loc("broadcast.95")
#loc72 = loc("multiply.96")
#loc73 = loc("add.108")
#loc75 = loc("reshape.130")
#loc76 = loc("reshape.132")
#loc77 = loc("transpose.133")
#loc78 = loc("dot.135")
#loc79 = loc("reshape.137")
#loc80 = loc("transpose.138")
#loc82 = loc("reshape.504")
#loc83 = loc("reshape.506")
#loc84 = loc("broadcast.507")
#loc85 = loc("reshape.359")
#loc86 = loc("reshape.361")
#loc87 = loc("transpose.362")
#loc88 = loc("dot.364")
#loc89 = loc("reshape.366")
#loc90 = loc("transpose.367")
#loc91 = loc("broadcast.376")
#loc92 = loc("multiply.377")
#loc93 = loc("slice.369")
#loc94 = loc("negate.370")
#loc95 = loc("slice.368")
#loc96 = loc("concatenate.371")
#loc97 = loc("broadcast.373")
#loc98 = loc("multiply.374")
#loc99 = loc("add.380")
#loc100 = loc("broadcast.354")
#loc101 = loc("reshape.355")
#loc102 = loc("transpose.356")
#loc103 = loc("dot.381")
#loc104 = loc("multiply.384")
#loc105 = loc("broadcast.325")
#loc106 = loc("and.328")
#loc107 = loc("reshape.329")
#loc108 = loc("reshape.337")
#loc109 = loc("broadcast.338")
#loc110 = loc("not.330")
#loc111 = loc("reshape.332")
#loc112 = loc("broadcast.333")
#loc113 = loc("broadcast.225")
#loc114 = loc("broadcast.227")
#loc115 = loc("subtract.228")
#loc116 = loc("compare.230")
#loc117 = loc("select.232")
#loc118 = loc("broadcast.201")
#loc119 = loc("compare.202")
#loc120 = loc("convert.203")
#loc121 = loc("multiply.233")
#loc122 = loc("reshape.235")
#loc123 = loc("slice.297")
#loc124 = loc("reshape.287")
#loc125 = loc("reshape.292")
#loc126 = loc("convert.298")
#loc127 = loc("reshape.301")
#loc128 = loc("broadcast.302")
#loc129 = loc("add.303")
#loc130 = loc("compare.306")
#loc131 = loc("select.308")
#loc132 = loc("floor.256")
#loc133 = loc("convert.257")
#loc134 = loc("clamp.260")
#loc135 = loc("compare.269")
#loc136 = loc("add.266")
#loc137 = loc("select.270")
#loc138 = loc("reshape.271")
#loc139 = loc("gather.310")
#loc140 = loc("select.334")
#loc141 = loc("select.339")
#loc142 = loc("reshape.387")
#loc143 = loc("broadcast.388")
#loc144 = loc("add.389")
#loc145 = loc("convert.390")
#loc146 = loc("reduce.396")
#loc147 = loc("broadcast.397")
#loc148 = loc("subtract.398")
#loc149 = loc("exponential.399")
#loc150 = loc("reduce.405")
#loc151 = loc("broadcast.406")
#loc152 = loc("divide.407")
#loc153 = loc("convert.408")
#loc154 = loc("broadcast.194")
#loc155 = loc("reshape.195")
#loc156 = loc("dot.409")
#loc157 = loc("transpose.411")
#loc158 = loc("reshape.413")
#loc159 = loc("reshape.183")
#loc160 = loc("reshape.185")
#loc161 = loc("transpose.186")
#loc163 = loc("reshape.415")
#loc164 = loc("add.418")
#loc165 = loc("reshape.449")
#loc166 = loc("reshape.451")
#loc167 = loc("broadcast.452")
#loc168 = loc("convert.419")
#loc169 = loc("power.421")
#loc170 = loc("reduce.428")
#loc171 = loc("multiply.437")
#loc172 = loc("reshape.438")
#loc173 = loc("add.442")
#loc174 = loc("rsqrt.443")
#loc175 = loc("reshape.444")
#loc176 = loc("broadcast.445")
#loc177 = loc("multiply.446")
#loc178 = loc("convert.447")
#loc179 = loc("multiply.453")
#loc180 = loc("reshape.462")
#loc181 = loc("reshape.458")
#loc182 = loc("reshape.460")
#loc183 = loc("transpose.461")
#loc184 = loc("dot.463")
#loc185 = loc("reshape.464")
#loc186 = loc("logistic.465")
#loc187 = loc("multiply.466")
#loc188 = loc("reshape.174")
#loc189 = loc("reshape.176")
#loc190 = loc("transpose.177")
#loc191 = loc("dot.455")
#loc192 = loc("reshape.456")
#loc193 = loc("multiply.467")
#loc194 = loc("reshape.468")
#loc195 = loc("reshape.169")
#loc196 = loc("reshape.171")
#loc197 = loc("transpose.172")
#loc199 = loc("reshape.470")
#loc200 = loc("add.473")
#loc201 = loc("convert.474")
#loc202 = loc("power.476")
#loc203 = loc("reduce.483")
#loc204 = loc("multiply.492")
#loc205 = loc("reshape.493")
#loc206 = loc("add.497")
#loc207 = loc("rsqrt.498")
#loc208 = loc("reshape.499")
#loc209 = loc("broadcast.500")
#loc210 = loc("multiply.501")
#loc211 = loc("convert.502")
#loc212 = loc("multiply.508")
#loc213 = loc("reshape.512")
#loc214 = loc("reshape.160")
#loc215 = loc("reshape.162")
#loc216 = loc("transpose.163")
#loc217 = loc("dot.513")
#loc218 = loc("reshape.514")