WARNING:root:Defaulting to PJRT_DEVICE=CPU
Using TT-Metal from the source tree: /localdev/hshah/tt-xla/third_party/tt-mlir/src/tt-mlir/third_party/tt-metal/src/tt-metal
WARNING: TT plugin is setting XLA_STABLEHLO_COMPILE to 1. This is required for TT PJRT plugin to work correctly.
============================= test session starts ==============================
platform linux -- Python 3.11.14, pytest-8.4.2, pluggy-1.6.0 -- /localdev/hshah/tt-xla/venv/bin/python
cachedir: .pytest_cache
rootdir: /localdev/hshah/tt-xla
configfile: pytest.ini
plugins: anyio-4.11.0, jaxtyping-0.3.3, forked-1.6.0, split-0.10.0
collecting ... Workaround to exclude model: suryaocr from discovery. Issue #1166
collected 1 item

tests/runner/test_models.py::test_all_models[opt/sequence_classification/pytorch-facebook/opt-125m-data_parallel-full-inference] Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-125m and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-10-28 19:04:40.376 (   0.000s) [        2AFB8480]   plugin_attributes.cc:58       1| PluginAttributes::PJRT_Plugin_Initialize
2025-10-28 19:04:40.376 (   0.000s) [        2AFB8480]     client_instance.cc:481      1| ClientInstance::PJRT_Client_Create
2025-10-28 19:04:40.384 (   0.008s) [        2AFB8480]     client_instance.cc:44       1| ClientInstance::ClientInstance
2025-10-28 19:04:40.384 (   0.008s) [        2AFB8480]     client_instance.cc:73       1| ClientInstance::Initialize
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]              stubs.inc:103   WARN| STUB: PJRT_Client_TopologyDescription
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]      error_instance.cc:52       1| ErrorInstance::PJRT_Error_Message
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]      error_instance.cc:61       1| ErrorInstance::PJRT_Error_GetCode
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]      error_instance.cc:46       1| ErrorInstance::PJRT_Error_Destroy
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     client_instance.cc:529      1| ClientInstance::PJRT_Client_PlatformVersion
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     client_instance.cc:510      1| ClientInstance::PJRT_Client_PlatformName
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     client_instance.cc:540      1| ClientInstance::PJRT_Client_Devices
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     client_instance.cc:553      1| ClientInstance::PJRT_Client_AddressableDevices
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     client_instance.cc:603      1| ClientInstance::PJRT_Client_AddressableMemories
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]   plugin_attributes.cc:64       1| PluginAttributes::PJRT_Plugin_Attributes
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:53.191 (  12.815s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.734 (  16.358s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.734 (  16.358s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.734 (  16.358s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.734 (  16.358s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.734 (  16.358s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.734 (  16.358s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.734 (  16.358s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.734 (  16.358s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.750 (  16.374s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.750 (  16.374s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.750 (  16.374s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.750 (  16.374s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.750 (  16.374s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.750 (  16.374s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.750 (  16.374s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.750 (  16.374s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.751 (  16.375s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:56.751 (  16.375s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:56.751 (  16.375s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-10-28 19:04:56.751 (  16.375s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:56.751 (  16.375s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:56.751 (  16.375s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:56.751 (  16.375s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:56.751 (  16.375s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-10-28 19:04:56.751 (  16.375s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:56.751 (  16.375s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:56.751 (  16.375s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:56.751 (  16.375s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:56.751 (  16.375s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-10-28 19:04:56.751 (  16.375s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:56.751 (  16.375s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:56.751 (  16.375s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:56.751 (  16.375s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:56.751 (  16.375s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-10-28 19:04:56.751 (  16.376s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:56.751 (  16.376s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:56.752 (  16.376s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-10-28 19:04:56.752 (  16.377s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:56.752 (  16.377s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:56.752 (  16.377s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:56.752 (  16.377s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:56.752 (  16.377s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:56.753 (  16.377s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.649 (  17.273s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.649 (  17.273s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.649 (  17.273s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.649 (  17.273s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.649 (  17.273s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.649 (  17.273s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.649 (  17.273s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.649 (  17.273s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.651 (  17.275s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.651 (  17.275s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.651 (  17.275s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.652 (  17.276s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.652 (  17.277s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.652 (  17.277s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.652 (  17.277s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.652 (  17.277s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.652 (  17.277s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.652 (  17.277s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.652 (  17.277s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.686 (  17.310s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.686 (  17.310s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.686 (  17.310s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.686 (  17.311s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.687 (  17.311s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.693 (  17.317s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.693 (  17.317s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.693 (  17.317s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.693 (  17.317s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.693 (  17.317s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.693 (  17.317s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.693 (  17.317s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.693 (  17.317s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.693 (  17.317s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.693 (  17.317s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.693 (  17.317s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.693 (  17.317s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.693 (  17.317s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.693 (  17.317s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.693 (  17.317s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.693 (  17.317s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.693 (  17.318s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.693 (  17.318s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.693 (  17.318s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.693 (  17.318s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.694 (  17.318s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.694 (  17.319s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.694 (  17.319s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.694 (  17.319s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.694 (  17.319s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.695 (  17.319s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.697 (  17.321s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.698 (  17.322s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.699 (  17.324s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.700 (  17.324s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.700 (  17.325s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.700 (  17.325s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.700 (  17.325s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.700 (  17.325s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.700 (  17.325s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.701 (  17.325s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.702 (  17.326s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.702 (  17.326s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.702 (  17.326s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.702 (  17.326s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.702 (  17.326s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.702 (  17.326s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.702 (  17.326s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.702 (  17.326s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.702 (  17.326s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.702 (  17.326s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.702 (  17.326s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.702 (  17.326s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.702 (  17.326s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.702 (  17.326s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.702 (  17.326s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.702 (  17.326s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.702 (  17.326s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.702 (  17.326s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.702 (  17.326s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.702 (  17.326s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.702 (  17.326s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.702 (  17.326s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.702 (  17.326s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.702 (  17.327s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.702 (  17.327s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.702 (  17.327s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.702 (  17.327s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.702 (  17.327s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.703 (  17.327s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.704 (  17.328s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.704 (  17.328s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.704 (  17.328s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.704 (  17.328s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.704 (  17.328s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.704 (  17.328s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.704 (  17.328s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.704 (  17.328s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.705 (  17.329s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.705 (  17.330s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.705 (  17.330s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.706 (  17.330s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.706 (  17.331s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.706 (  17.331s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.706 (  17.331s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.707 (  17.331s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.708 (  17.332s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.708 (  17.332s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.708 (  17.332s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.708 (  17.332s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.708 (  17.332s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.708 (  17.332s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.708 (  17.332s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.708 (  17.332s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.710 (  17.334s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.710 (  17.334s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.710 (  17.334s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.710 (  17.334s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.710 (  17.334s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.710 (  17.334s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.710 (  17.334s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.710 (  17.334s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.710 (  17.334s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.710 (  17.334s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.710 (  17.334s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.710 (  17.334s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.710 (  17.334s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.710 (  17.334s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.710 (  17.334s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.710 (  17.334s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.710 (  17.334s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.710 (  17.334s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.710 (  17.334s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.710 (  17.334s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.710 (  17.334s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.710 (  17.334s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.710 (  17.334s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.710 (  17.335s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.710 (  17.335s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.710 (  17.335s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.711 (  17.335s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.711 (  17.336s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.711 (  17.336s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.711 (  17.336s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.711 (  17.336s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.711 (  17.336s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.712 (  17.336s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.712 (  17.336s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.712 (  17.336s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.712 (  17.336s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.712 (  17.336s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.712 (  17.336s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.712 (  17.336s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.712 (  17.336s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.712 (  17.336s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.712 (  17.336s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.712 (  17.336s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.712 (  17.336s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.712 (  17.336s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.712 (  17.336s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.712 (  17.336s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.715 (  17.339s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.716 (  17.340s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.717 (  17.341s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.717 (  17.342s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.717 (  17.342s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.717 (  17.342s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.717 (  17.342s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.717 (  17.342s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.718 (  17.342s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.718 (  17.343s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.718 (  17.343s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.718 (  17.343s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.718 (  17.343s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.718 (  17.343s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.719 (  17.343s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.719 (  17.343s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.719 (  17.343s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.719 (  17.343s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.719 (  17.343s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.719 (  17.343s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.719 (  17.343s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.719 (  17.343s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.719 (  17.343s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.719 (  17.343s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.719 (  17.343s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.719 (  17.343s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.719 (  17.343s) [        2AFB8480]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-10-28 19:04:57.719 (  17.343s) [        2AFB8480]     client_instance.cc:659      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-10-28 19:04:57.719 (  17.343s) [        2AFB8480]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-10-28 19:04:57.719 (  17.343s) [        2AFB8480]      event_instance.cc:223      1| EventInstance::PJRT_Event_OnReady
2025-10-28 19:04:57.719 (  17.343s) [        2AFB8480]      event_instance.cc:172      1| EventInstance::PJRT_Event_Destroy
2025-10-28 19:04:57.725 (  17.349s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.725 (  17.349s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.725 (  17.349s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.725 (  17.349s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.725 (  17.349s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.725 (  17.349s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.725 (  17.349s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.725 (  17.349s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.725 (  17.349s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.725 (  17.349s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.725 (  17.349s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.725 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.725 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.725 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.725 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.725 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.725 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.725 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.725 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.725 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.725 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.726 (  17.350s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.727 (  17.351s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.727 (  17.351s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.727 (  17.351s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.727 (  17.351s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.727 (  17.351s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.727 (  17.351s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.727 (  17.351s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.727 (  17.351s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.727 (  17.351s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.727 (  17.351s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.727 (  17.351s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.727 (  17.351s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.727 (  17.351s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.727 (  17.351s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.727 (  17.351s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.727 (  17.351s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.727 (  17.351s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.727 (  17.351s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.728 (  17.352s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.728 (  17.352s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.728 (  17.352s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.728 (  17.352s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:57.756 (  17.380s) [        2AFB8480]     client_instance.cc:616      1| ClientInstance::PJRT_Client_Compile
2025-10-28 19:04:57.756 (  17.380s) [        2AFB8480]      module_builder.cc:220      1| ModuleBuilder::buildModule
2025-10-28 19:04:57.758 (  17.382s) [        2AFB8480]      module_builder.cc:963      1| MLIR Module vhlo:
#loc1 = loc("p0.2")
#loc2 = loc("p1.66")
#loc3 = loc("p2.72")
#loc4 = loc("p3.76")
#loc5 = loc("p4.85")
#loc6 = loc("p5.92")
#loc7 = loc("p6.99")
#loc8 = loc("p7.106")
#loc9 = loc("p8.112")
#loc10 = loc("p9.116")
#loc11 = loc("p10.124")
#loc12 = loc("p11.128")
#loc13 = loc("p12.134")
#loc14 = loc("p13.138")
#loc15 = loc("p14.144")
#loc16 = loc("p15.148")
#loc17 = loc("p16.159")
#loc18 = loc("p17.179")
#loc19 = loc("p18.187")
#loc20 = loc("p19.364")
#loc21 = loc("p20.368")
#loc22 = loc("p21.387")
#loc23 = loc("p22.391")
#loc40 = loc("reduce-window.168")
#loc50 = loc("reduce.242")
#loc56 = loc("reduce.225")
#loc128 = loc("reduce.424")
#loc133 = loc("reduce.433")
#loc170 = loc("reduce.499")
#loc176 = loc("reduce.482")
#loc218 = loc("reduce.595")
#loc224 = loc("reduce.578")
#loc255 = loc("reduce.39")
module @SyncTensorsGraph.630 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<8x32x!vhlo.i64_v1> loc("p0.2"), %arg1: !vhlo.tensor_v1<2x768x!vhlo.bf16_v1> loc("p1.66"), %arg2: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("p2.72"), %arg3: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("p3.76"), %arg4: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("p4.85"), %arg5: !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc("p5.92"), %arg6: !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc("p6.99"), %arg7: !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc("p7.106"), %arg8: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("p8.112"), %arg9: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("p9.116"), %arg10: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("p10.124"), %arg11: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("p11.128"), %arg12: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("p12.134"), %arg13: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("p13.138"), %arg14: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("p14.144"), %arg15: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("p15.148"), %arg16: !vhlo.tensor_v1<8x32x!vhlo.i64_v1> loc("p16.159"), %arg17: !vhlo.tensor_v1<2050x768x!vhlo.bf16_v1> loc("p17.179"), %arg18: !vhlo.tensor_v1<50272x768x!vhlo.bf16_v1> loc("p18.187"), %arg19: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("p19.364"), %arg20: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("p20.368"), %arg21: !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc("p21.387"), %arg22: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc("p22.391")) -> (!vhlo.tensor_v1<8x2x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<i64>>}> : () -> !vhlo.tensor_v1<!vhlo.i64_v1> loc(#loc)
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<i32>>}> : () -> !vhlo.tensor_v1<!vhlo.i32_v1> loc(#loc)
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64>>}> : () -> !vhlo.tensor_v1<32x!vhlo.i64_v1> loc(#loc)
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc)
    %5 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi32>>}> : () -> !vhlo.tensor_v1<32x!vhlo.i32_v1> loc(#loc)
    %6 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<-2147483648> : tensor<i32>>}> : () -> !vhlo.tensor_v1<!vhlo.i32_v1> loc(#loc)
    %7 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2> : tensor<i64>>}> : () -> !vhlo.tensor_v1<!vhlo.i64_v1> loc(#loc)
    %8 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.250000e-01> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc)
    %9 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1> : tensor<i64>>}> : () -> !vhlo.tensor_v1<!vhlo.i64_v1> loc(#loc)
    %10 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<-3.389530e+38> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc)
    %11 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.304630e-03> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc)
    %12 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.001360e-05> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc)
    %13 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<32> : tensor<8xi64>>}> : () -> !vhlo.tensor_v1<8x!vhlo.i64_v1> loc(#loc)
    %14 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<8xi64>>}> : () -> !vhlo.tensor_v1<8x!vhlo.i64_v1> loc(#loc)
    %15 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[[0], [1], [2], [3], [4], [5], [6], [7]]> : tensor<8x1xi64>>}> : () -> !vhlo.tensor_v1<8x1x!vhlo.i64_v1> loc(#loc)
    %16 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc)
    %17 = "vhlo.broadcast_in_dim_v1"(%16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x3072x!vhlo.bf16_v1> loc(#loc)
    %18 = "vhlo.broadcast_in_dim_v1"(%12) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x1x!vhlo.bf16_v1> loc(#loc)
    %19 = "vhlo.broadcast_in_dim_v1"(%11) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x!vhlo.bf16_v1> loc(#loc)
    %20 = "vhlo.broadcast_in_dim_v1"(%10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1> loc(#loc)
    %21 = "vhlo.broadcast_in_dim_v1"(%16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1> loc(#loc)
    %22 = "vhlo.broadcast_in_dim_v1"(%16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.bf16_v1> loc(#loc)
    %23 = "vhlo.broadcast_in_dim_v1"(%10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.bf16_v1> loc(#loc)
    %24 = "vhlo.broadcast_in_dim_v1"(%9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.i64_v1> loc(#loc)
    %25 = "vhlo.broadcast_in_dim_v1"(%8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc)
    %26 = "vhlo.broadcast_in_dim_v1"(%12) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1> loc(#loc)
    %27 = "vhlo.broadcast_in_dim_v1"(%11) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1> loc(#loc)
    %28 = "vhlo.broadcast_in_dim_v1"(%7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1> loc(#loc)
    %29 = "vhlo.broadcast_in_dim_v1"(%9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1> loc(#loc)
    %30 = "vhlo.reshape_v1"(%arg18) : (!vhlo.tensor_v1<50272x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x50272x768x!vhlo.bf16_v1> loc(#loc24)
    %31 = "vhlo.custom_call_v1"(%30) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_embed_tokens_weight">}>} : (!vhlo.tensor_v1<1x50272x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x50272x768x!vhlo.bf16_v1> loc(#loc25)
    %32 = "vhlo.reshape_v1"(%31) : (!vhlo.tensor_v1<1x50272x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<50272x768x!vhlo.bf16_v1> loc(#loc26)
    %33 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x8x32x!vhlo.i64_v1> loc(#loc27)
    %34 = "vhlo.custom_call_v1"(%33) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x8x32x!vhlo.i64_v1> loc(#loc28)
    %35 = "vhlo.reshape_v1"(%34) : (!vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1> loc(#loc29)
    %36 = "vhlo.reshape_v1"(%34) : (!vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<256x!vhlo.i64_v1> loc(#loc30)
    %37 = "vhlo.convert_v1"(%36) : (!vhlo.tensor_v1<256x!vhlo.i64_v1>) -> !vhlo.tensor_v1<256x!vhlo.ui32_v1> loc(#loc31)
    %38 = "vhlo.gather_v2"(%32, %37) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 768]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<50272x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1> loc(#loc32)
    %39 = "vhlo.reshape_v1"(%38) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc33)
    %40 = "vhlo.reshape_v1"(%arg17) : (!vhlo.tensor_v1<2050x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x2050x768x!vhlo.bf16_v1> loc(#loc34)
    %41 = "vhlo.custom_call_v1"(%40) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_embed_positions_weight">}>} : (!vhlo.tensor_v1<1x2050x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x2050x768x!vhlo.bf16_v1> loc(#loc35)
    %42 = "vhlo.reshape_v1"(%41) : (!vhlo.tensor_v1<1x2050x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2050x768x!vhlo.bf16_v1> loc(#loc36)
    %43 = "vhlo.reshape_v1"(%arg16) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x8x32x!vhlo.i64_v1> loc(#loc37)
    %44 = "vhlo.custom_call_v1"(%43) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_1">}>} : (!vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x8x32x!vhlo.i64_v1> loc(#loc38)
    %45 = "vhlo.reshape_v1"(%44) : (!vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1> loc(#loc39)
    %46 = "vhlo.reduce_window_v1"(%45, %0) <{base_dilations = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>, padding = #vhlo.tensor_v1<dense<[[0, 0], [31, 0]]> : tensor<2x2xi64>>, window_dilations = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>, window_dimensions = #vhlo.tensor_v1<dense<[1, 32]> : tensor<2xi64>>, window_strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.i64_v1> loc("reduce-window.168"), %arg24: !vhlo.tensor_v1<!vhlo.i64_v1> loc("reduce-window.168")):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.i64_v1>, !vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<!vhlo.i64_v1> loc(#loc41)
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>, !vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1> loc(#loc40)
    %47 = "vhlo.multiply_v1"(%46, %45) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>, !vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1> loc(#loc42)
    %48 = "vhlo.subtract_v1"(%47, %29) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>, !vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1> loc(#loc43)
    %49 = "vhlo.add_v1"(%48, %28) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>, !vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1> loc(#loc44)
    %50 = "vhlo.reshape_v1"(%49) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<256x!vhlo.i64_v1> loc(#loc45)
    %51 = "vhlo.convert_v1"(%50) : (!vhlo.tensor_v1<256x!vhlo.i64_v1>) -> !vhlo.tensor_v1<256x!vhlo.ui32_v1> loc(#loc46)
    %52 = "vhlo.gather_v2"(%42, %51) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 768]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<2050x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1> loc(#loc47)
    %53 = "vhlo.reshape_v1"(%52) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc48)
    %54 = "vhlo.add_v1"(%39, %53) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc49)
    %55 = "vhlo.reduce_v1"(%54, %16) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("reduce.242"), %arg24: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("reduce.242")):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc51)
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1> loc(#loc50)
    %56 = "vhlo.multiply_v1"(%55, %27) : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1> loc(#loc52)
    %57 = "vhlo.broadcast_in_dim_v1"(%56) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc53)
    %58 = "vhlo.subtract_v1"(%54, %57) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc54)
    %59 = "vhlo.multiply_v1"(%58, %58) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc55)
    %60 = "vhlo.reduce_v1"(%59, %16) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("reduce.225"), %arg24: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("reduce.225")):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc57)
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1> loc(#loc56)
    %61 = "vhlo.multiply_v1"(%60, %27) : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1> loc(#loc58)
    %62 = "vhlo.reshape_v1"(%61) : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1> loc(#loc59)
    %63 = "vhlo.add_v1"(%62, %26) : (!vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1> loc(#loc60)
    %64 = "vhlo.rsqrt_v2"(%63) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1> loc(#loc61)
    %65 = "vhlo.reshape_v1"(%64) : (!vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1> loc(#loc62)
    %66 = "vhlo.broadcast_in_dim_v1"(%65) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc63)
    %67 = "vhlo.multiply_v1"(%58, %66) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc64)
    %68 = "vhlo.reshape_v1"(%arg15) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc65)
    %69 = "vhlo.custom_call_v1"(%68) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_layer_norm_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc66)
    %70 = "vhlo.reshape_v1"(%69) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc67)
    %71 = "vhlo.broadcast_in_dim_v1"(%70) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc68)
    %72 = "vhlo.multiply_v1"(%67, %71) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc69)
    %73 = "vhlo.reshape_v1"(%arg14) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc70)
    %74 = "vhlo.custom_call_v1"(%73) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_layer_norm_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc71)
    %75 = "vhlo.reshape_v1"(%74) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc72)
    %76 = "vhlo.broadcast_in_dim_v1"(%75) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc73)
    %77 = "vhlo.add_v1"(%72, %76) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc74)
    %78 = "vhlo.reshape_v1"(%77) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1> loc(#loc75)
    %79 = "vhlo.reshape_v1"(%arg22) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc76)
    %80 = "vhlo.custom_call_v1"(%79) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc77)
    %81 = "vhlo.reshape_v1"(%80) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc78)
    %82 = "vhlo.transpose_v1"(%81) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc79)
    %83 = "vhlo.dot_general_v2"(%78, %82) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1> loc(#loc80)
    %84 = "vhlo.reshape_v1"(%83) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc81)
    %85 = "vhlo.reshape_v1"(%arg21) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc82)
    %86 = "vhlo.custom_call_v1"(%85) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc83)
    %87 = "vhlo.reshape_v1"(%86) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc84)
    %88 = "vhlo.broadcast_in_dim_v1"(%87) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc85)
    %89 = "vhlo.add_v1"(%84, %88) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc86)
    %90 = "vhlo.multiply_v1"(%89, %25) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc87)
    %91 = "vhlo.reshape_v1"(%90) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1> loc(#loc88)
    %92 = "vhlo.transpose_v1"(%91) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[8,12,32,64]{3,1,2,0}">} : (!vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x32x64x!vhlo.bf16_v1> loc(#loc89)
    %93 = "vhlo.reshape_v1"(%92) : (!vhlo.tensor_v1<8x12x32x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<96x32x64x!vhlo.bf16_v1> loc(#loc90)
    %94 = "vhlo.reshape_v1"(%arg20) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc91)
    %95 = "vhlo.custom_call_v1"(%94) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc92)
    %96 = "vhlo.reshape_v1"(%95) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc93)
    %97 = "vhlo.transpose_v1"(%96) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc94)
    %98 = "vhlo.dot_general_v2"(%78, %97) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1> loc(#loc95)
    %99 = "vhlo.reshape_v1"(%98) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc96)
    %100 = "vhlo.reshape_v1"(%arg19) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc97)
    %101 = "vhlo.custom_call_v1"(%100) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc98)
    %102 = "vhlo.reshape_v1"(%101) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc99)
    %103 = "vhlo.broadcast_in_dim_v1"(%102) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc100)
    %104 = "vhlo.add_v1"(%99, %103) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc101)
    %105 = "vhlo.reshape_v1"(%104) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1> loc(#loc102)
    %106 = "vhlo.transpose_v1"(%105) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 3, 1]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x64x32x!vhlo.bf16_v1> loc(#loc103)
    %107 = "vhlo.reshape_v1"(%106) : (!vhlo.tensor_v1<8x12x64x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<96x64x32x!vhlo.bf16_v1> loc(#loc104)
    %108 = "vhlo.dot_general_v2"(%93, %107) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<96x32x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<96x64x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<96x32x32x!vhlo.bf16_v1> loc(#loc105)
    %109 = "vhlo.reshape_v1"(%108) : (!vhlo.tensor_v1<96x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1> loc(#loc106)
    %110 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.i64_v1> loc(#loc107)
    %111 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.i64_v1> loc(#loc108)
    %112 = "vhlo.subtract_v1"(%110, %111) : (!vhlo.tensor_v1<32x32x!vhlo.i64_v1>, !vhlo.tensor_v1<32x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.i64_v1> loc(#loc109)
    %113 = "vhlo.compare_v1"(%112, %24) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GE>}> : (!vhlo.tensor_v1<32x32x!vhlo.i64_v1>, !vhlo.tensor_v1<32x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.bool_v1> loc(#loc110)
    %114 = "vhlo.select_v1"(%113, %23, %22) : (!vhlo.tensor_v1<32x32x!vhlo.bool_v1>, !vhlo.tensor_v1<32x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.bf16_v1> loc(#loc111)
    %115 = "vhlo.compare_v1"(%110, %111) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<32x32x!vhlo.i64_v1>, !vhlo.tensor_v1<32x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.bool_v1> loc(#loc112)
    %116 = "vhlo.convert_v1"(%115) : (!vhlo.tensor_v1<32x32x!vhlo.bool_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.bf16_v1> loc(#loc113)
    %117 = "vhlo.multiply_v1"(%114, %116) : (!vhlo.tensor_v1<32x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.bf16_v1> loc(#loc114)
    %118 = "vhlo.reshape_v1"(%117) : (!vhlo.tensor_v1<32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x32x32x!vhlo.bf16_v1> loc(#loc115)
    %119 = "vhlo.broadcast_in_dim_v1"(%118) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[1, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1> loc(#loc116)
    %120 = "vhlo.reshape_v1"(%44) : (!vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x1x1x32x!vhlo.i64_v1> loc(#loc117)
    %121 = "vhlo.convert_v1"(%120) : (!vhlo.tensor_v1<8x1x1x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x1x1x32x!vhlo.bf16_v1> loc(#loc118)
    %122 = "vhlo.reshape_v1"(%121) : (!vhlo.tensor_v1<8x1x1x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x!vhlo.bf16_v1> loc(#loc119)
    %123 = "vhlo.broadcast_in_dim_v1"(%122) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<8x1x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1> loc(#loc120)
    %124 = "vhlo.add_v1"(%119, %123) : (!vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1> loc(#loc121)
    %125 = "vhlo.compare_v1"(%124, %21) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x32x!vhlo.bool_v1> loc(#loc122)
    %126 = "vhlo.select_v1"(%125, %20, %119) : (!vhlo.tensor_v1<8x1x32x32x!vhlo.bool_v1>, !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1> loc(#loc123)
    %127 = "vhlo.reshape_v1"(%126) : (!vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x32x!vhlo.bf16_v1> loc(#loc124)
    %128 = "vhlo.broadcast_in_dim_v1"(%127) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<8x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1> loc(#loc125)
    %129 = "vhlo.add_v1"(%109, %128) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1> loc(#loc126)
    %130 = "vhlo.convert_v1"(%129) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1> loc(#loc127)
    %131 = "vhlo.reduce_v1"(%130, %3) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.424"), %arg24: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.424")):
      %261 = "vhlo.maximum_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc129)
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x!vhlo.f32_v1> loc(#loc128)
    %132 = "vhlo.broadcast_in_dim_v1"(%131) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<8x12x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1> loc(#loc130)
    %133 = "vhlo.subtract_v1"(%130, %132) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>, !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1> loc(#loc131)
    %134 = "vhlo.exponential_v2"(%133) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1> loc(#loc132)
    %135 = "vhlo.reduce_v1"(%134, %4) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.433"), %arg24: !vhlo.tensor_v1<!vhlo.f32_v1> loc("reduce.433")):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1> loc(#loc134)
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x!vhlo.f32_v1> loc(#loc133)
    %136 = "vhlo.broadcast_in_dim_v1"(%135) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<8x12x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1> loc(#loc135)
    %137 = "vhlo.divide_v1"(%134, %136) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>, !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1> loc(#loc136)
    %138 = "vhlo.convert_v1"(%137) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1> loc(#loc137)
    %139 = "vhlo.reshape_v1"(%138) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<96x32x32x!vhlo.bf16_v1> loc(#loc138)
    %140 = "vhlo.reshape_v1"(%arg13) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc139)
    %141 = "vhlo.custom_call_v1"(%140) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc140)
    %142 = "vhlo.reshape_v1"(%141) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc141)
    %143 = "vhlo.transpose_v1"(%142) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc142)
    %144 = "vhlo.dot_general_v2"(%78, %143) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1> loc(#loc143)
    %145 = "vhlo.reshape_v1"(%144) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc144)
    %146 = "vhlo.reshape_v1"(%arg12) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc145)
    %147 = "vhlo.custom_call_v1"(%146) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc146)
    %148 = "vhlo.reshape_v1"(%147) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc147)
    %149 = "vhlo.broadcast_in_dim_v1"(%148) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc148)
    %150 = "vhlo.add_v1"(%145, %149) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc149)
    %151 = "vhlo.reshape_v1"(%150) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1> loc(#loc150)
    %152 = "vhlo.transpose_v1"(%151) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[8,12,32,64]{3,1,2,0}">} : (!vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x32x64x!vhlo.bf16_v1> loc(#loc151)
    %153 = "vhlo.reshape_v1"(%152) : (!vhlo.tensor_v1<8x12x32x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<96x32x64x!vhlo.bf16_v1> loc(#loc152)
    %154 = "vhlo.dot_general_v2"(%139, %153) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<96x32x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<96x32x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<96x32x64x!vhlo.bf16_v1> loc(#loc153)
    %155 = "vhlo.reshape_v1"(%154) : (!vhlo.tensor_v1<96x32x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x32x64x!vhlo.bf16_v1> loc(#loc154)
    %156 = "vhlo.transpose_v1"(%155) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[8,32,12,64]{3,1,2,0}">} : (!vhlo.tensor_v1<8x12x32x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1> loc(#loc155)
    %157 = "vhlo.reshape_v1"(%156) : (!vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1> loc(#loc156)
    %158 = "vhlo.reshape_v1"(%arg11) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc157)
    %159 = "vhlo.custom_call_v1"(%158) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1> loc(#loc158)
    %160 = "vhlo.reshape_v1"(%159) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc159)
    %161 = "vhlo.transpose_v1"(%160) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1> loc(#loc160)
    %162 = "vhlo.dot_general_v2"(%157, %161) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1> loc(#loc161)
    %163 = "vhlo.reshape_v1"(%162) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc162)
    %164 = "vhlo.reshape_v1"(%arg10) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc163)
    %165 = "vhlo.custom_call_v1"(%164) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc164)
    %166 = "vhlo.reshape_v1"(%165) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc165)
    %167 = "vhlo.broadcast_in_dim_v1"(%166) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc166)
    %168 = "vhlo.add_v1"(%163, %167) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc167)
    %169 = "vhlo.add_v1"(%54, %168) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc168)
    %170 = "vhlo.reshape_v1"(%169) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1> loc(#loc169)
    %171 = "vhlo.reduce_v1"(%170, %16) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("reduce.499"), %arg24: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("reduce.499")):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc171)
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x!vhlo.bf16_v1> loc(#loc170)
    %172 = "vhlo.multiply_v1"(%171, %19) : (!vhlo.tensor_v1<256x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x!vhlo.bf16_v1> loc(#loc172)
    %173 = "vhlo.broadcast_in_dim_v1"(%172) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1> loc(#loc173)
    %174 = "vhlo.subtract_v1"(%170, %173) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1> loc(#loc174)
    %175 = "vhlo.multiply_v1"(%174, %174) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1> loc(#loc175)
    %176 = "vhlo.reduce_v1"(%175, %16) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("reduce.482"), %arg24: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("reduce.482")):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc177)
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x!vhlo.bf16_v1> loc(#loc176)
    %177 = "vhlo.multiply_v1"(%176, %19) : (!vhlo.tensor_v1<256x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x!vhlo.bf16_v1> loc(#loc178)
    %178 = "vhlo.reshape_v1"(%177) : (!vhlo.tensor_v1<256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x1x!vhlo.bf16_v1> loc(#loc179)
    %179 = "vhlo.add_v1"(%178, %18) : (!vhlo.tensor_v1<256x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x1x!vhlo.bf16_v1> loc(#loc180)
    %180 = "vhlo.rsqrt_v2"(%179) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<256x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x1x!vhlo.bf16_v1> loc(#loc181)
    %181 = "vhlo.reshape_v1"(%180) : (!vhlo.tensor_v1<256x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x!vhlo.bf16_v1> loc(#loc182)
    %182 = "vhlo.broadcast_in_dim_v1"(%181) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1> loc(#loc183)
    %183 = "vhlo.multiply_v1"(%174, %182) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1> loc(#loc184)
    %184 = "vhlo.reshape_v1"(%arg9) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc185)
    %185 = "vhlo.custom_call_v1"(%184) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_final_layer_norm_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc186)
    %186 = "vhlo.reshape_v1"(%185) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc187)
    %187 = "vhlo.broadcast_in_dim_v1"(%186) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1> loc(#loc188)
    %188 = "vhlo.multiply_v1"(%183, %187) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1> loc(#loc189)
    %189 = "vhlo.reshape_v1"(%arg8) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc190)
    %190 = "vhlo.custom_call_v1"(%189) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_final_layer_norm_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc191)
    %191 = "vhlo.reshape_v1"(%190) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc192)
    %192 = "vhlo.broadcast_in_dim_v1"(%191) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1> loc(#loc193)
    %193 = "vhlo.add_v1"(%188, %192) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1> loc(#loc194)
    %194 = "vhlo.reshape_v1"(%arg7) : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc195)
    %195 = "vhlo.custom_call_v1"(%194) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_fc1_weight">}>} : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1> loc(#loc196)
    %196 = "vhlo.reshape_v1"(%195) : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc197)
    %197 = "vhlo.transpose_v1"(%196) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,3072]{0,1}">} : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc198)
    %198 = "vhlo.dot_general_v2"(%193, %197) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x3072x!vhlo.bf16_v1> loc(#loc199)
    %199 = "vhlo.reshape_v1"(%arg6) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc200)
    %200 = "vhlo.custom_call_v1"(%199) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1> loc(#loc201)
    %201 = "vhlo.reshape_v1"(%200) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1> loc(#loc202)
    %202 = "vhlo.broadcast_in_dim_v1"(%201) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x3072x!vhlo.bf16_v1> loc(#loc203)
    %203 = "vhlo.add_v1"(%198, %202) : (!vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x3072x!vhlo.bf16_v1> loc(#loc204)
    %204 = "vhlo.maximum_v1"(%203, %17) : (!vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x3072x!vhlo.bf16_v1> loc(#loc205)
    %205 = "vhlo.reshape_v1"(%arg5) : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc206)
    %206 = "vhlo.custom_call_v1"(%205) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_fc2_weight">}>} : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1> loc(#loc207)
    %207 = "vhlo.reshape_v1"(%206) : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1> loc(#loc208)
    %208 = "vhlo.transpose_v1"(%207) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,768]{0,1}">} : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1> loc(#loc209)
    %209 = "vhlo.dot_general_v2"(%204, %208) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1> loc(#loc210)
    %210 = "vhlo.reshape_v1"(%arg4) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc211)
    %211 = "vhlo.custom_call_v1"(%210) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc212)
    %212 = "vhlo.reshape_v1"(%211) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc213)
    %213 = "vhlo.broadcast_in_dim_v1"(%212) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1> loc(#loc214)
    %214 = "vhlo.add_v1"(%209, %213) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1> loc(#loc215)
    %215 = "vhlo.add_v1"(%170, %214) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1> loc(#loc216)
    %216 = "vhlo.reshape_v1"(%215) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc217)
    %217 = "vhlo.reduce_v1"(%216, %16) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("reduce.595"), %arg24: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("reduce.595")):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc219)
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1> loc(#loc218)
    %218 = "vhlo.multiply_v1"(%217, %27) : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1> loc(#loc220)
    %219 = "vhlo.broadcast_in_dim_v1"(%218) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc221)
    %220 = "vhlo.subtract_v1"(%216, %219) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc222)
    %221 = "vhlo.multiply_v1"(%220, %220) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc223)
    %222 = "vhlo.reduce_v1"(%221, %16) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("reduce.578"), %arg24: !vhlo.tensor_v1<!vhlo.bf16_v1> loc("reduce.578")):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1> loc(#loc225)
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1> loc(#loc224)
    %223 = "vhlo.multiply_v1"(%222, %27) : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1> loc(#loc226)
    %224 = "vhlo.reshape_v1"(%223) : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1> loc(#loc227)
    %225 = "vhlo.add_v1"(%224, %26) : (!vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1> loc(#loc228)
    %226 = "vhlo.rsqrt_v2"(%225) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1> loc(#loc229)
    %227 = "vhlo.reshape_v1"(%226) : (!vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1> loc(#loc230)
    %228 = "vhlo.broadcast_in_dim_v1"(%227) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc231)
    %229 = "vhlo.multiply_v1"(%220, %228) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc232)
    %230 = "vhlo.reshape_v1"(%arg3) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc233)
    %231 = "vhlo.custom_call_v1"(%230) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_final_layer_norm_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc234)
    %232 = "vhlo.reshape_v1"(%231) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc235)
    %233 = "vhlo.broadcast_in_dim_v1"(%232) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc236)
    %234 = "vhlo.multiply_v1"(%229, %233) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc237)
    %235 = "vhlo.reshape_v1"(%arg2) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc238)
    %236 = "vhlo.custom_call_v1"(%235) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_final_layer_norm_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1> loc(#loc239)
    %237 = "vhlo.reshape_v1"(%236) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1> loc(#loc240)
    %238 = "vhlo.broadcast_in_dim_v1"(%237) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc241)
    %239 = "vhlo.add_v1"(%234, %238) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1> loc(#loc242)
    %240 = "vhlo.reshape_v1"(%239) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1> loc(#loc243)
    %241 = "vhlo.reshape_v1"(%arg1) : (!vhlo.tensor_v1<2x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x2x768x!vhlo.bf16_v1> loc(#loc244)
    %242 = "vhlo.custom_call_v1"(%241) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___score_weight">}>} : (!vhlo.tensor_v1<1x2x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x2x768x!vhlo.bf16_v1> loc(#loc245)
    %243 = "vhlo.reshape_v1"(%242) : (!vhlo.tensor_v1<1x2x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x768x!vhlo.bf16_v1> loc(#loc246)
    %244 = "vhlo.transpose_v1"(%243) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,2]{0,1}">} : (!vhlo.tensor_v1<2x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x2x!vhlo.bf16_v1> loc(#loc247)
    %245 = "vhlo.dot_general_v2"(%240, %244) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x2x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x2x!vhlo.bf16_v1> loc(#loc248)
    %246 = "vhlo.reshape_v1"(%245) : (!vhlo.tensor_v1<256x2x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x2x!vhlo.bf16_v1> loc(#loc249)
    %247 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i32_v1> loc(#loc250)
    %248 = "vhlo.compare_v1"(%35, %29) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 NE>}> : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>, !vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bool_v1> loc(#loc251)
    %249 = "vhlo.convert_v1"(%248) : (!vhlo.tensor_v1<8x32x!vhlo.bool_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i32_v1> loc(#loc252)
    %250 = "vhlo.multiply_v1"(%247, %249) : (!vhlo.tensor_v1<8x32x!vhlo.i32_v1>, !vhlo.tensor_v1<8x32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i32_v1> loc(#loc253)
    %251 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<32x!vhlo.i32_v1> loc(#loc254)
    %252 = "vhlo.broadcast_in_dim_v1"(%251) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i32_v1> loc(#loc254)
    %253:2 = "vhlo.reduce_v1"(%250, %252, %6, %1) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.i32_v1> loc("reduce.39"), %arg24: !vhlo.tensor_v1<!vhlo.i32_v1> loc("reduce.39"), %arg25: !vhlo.tensor_v1<!vhlo.i32_v1> loc("reduce.39"), %arg26: !vhlo.tensor_v1<!vhlo.i32_v1> loc("reduce.39")):
      %261 = "vhlo.compare_v1"(%arg23, %arg25) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GE>}> : (!vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc256)
      %262 = "vhlo.select_v1"(%261, %arg23, %arg25) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1> loc(#loc257)
      %263 = "vhlo.compare_v1"(%arg23, %arg25) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1> loc(#loc258)
      %264 = "vhlo.minimum_v1"(%arg24, %arg26) : (!vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1> loc(#loc259)
      %265 = "vhlo.select_v1"(%261, %arg24, %arg26) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1> loc(#loc260)
      %266 = "vhlo.select_v1"(%263, %264, %265) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1> loc(#loc261)
      "vhlo.return_v1"(%262, %266) : (!vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> () loc(#loc)
    }) : (!vhlo.tensor_v1<8x32x!vhlo.i32_v1>, !vhlo.tensor_v1<8x32x!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> (!vhlo.tensor_v1<8x!vhlo.i32_v1>, !vhlo.tensor_v1<8x!vhlo.i32_v1>) loc(#loc255)
    %254 = "vhlo.convert_v1"(%253#1) : (!vhlo.tensor_v1<8x!vhlo.i32_v1>) -> !vhlo.tensor_v1<8x!vhlo.i64_v1> loc(#loc262)
    %255 = "vhlo.compare_v1"(%254, %14) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 LT>}> : (!vhlo.tensor_v1<8x!vhlo.i64_v1>, !vhlo.tensor_v1<8x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x!vhlo.bool_v1> loc(#loc263)
    %256 = "vhlo.add_v1"(%254, %13) : (!vhlo.tensor_v1<8x!vhlo.i64_v1>, !vhlo.tensor_v1<8x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x!vhlo.i64_v1> loc(#loc264)
    %257 = "vhlo.select_v1"(%255, %256, %254) : (!vhlo.tensor_v1<8x!vhlo.bool_v1>, !vhlo.tensor_v1<8x!vhlo.i64_v1>, !vhlo.tensor_v1<8x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x!vhlo.i64_v1> loc(#loc265)
    %258 = "vhlo.reshape_v1"(%257) : (!vhlo.tensor_v1<8x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x1x!vhlo.i64_v1> loc(#loc266)
    %259 = "vhlo.concatenate_v1"(%15, %258) <{dimension = #vhlo.integer_v1<1 : i64>}> : (!vhlo.tensor_v1<8x1x!vhlo.i64_v1>, !vhlo.tensor_v1<8x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x2x!vhlo.i64_v1> loc(#loc267)
    %260 = "vhlo.gather_v2"(%246, %259) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>, start_index_map = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<8x32x2x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x2x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x2x!vhlo.bf16_v1> loc(#loc268)
    "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<8x2x!vhlo.bf16_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">} loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc24 = loc("reshape.188")
#loc25 = loc("custom-call.189")
#loc26 = loc("reshape.190")
#loc27 = loc("reshape.3")
#loc28 = loc("custom-call.4")
#loc29 = loc("reshape.5")
#loc30 = loc("reshape.186")
#loc31 = loc("convert.191")
#loc32 = loc("gather.192")
#loc33 = loc("reshape.193")
#loc34 = loc("reshape.180")
#loc35 = loc("custom-call.181")
#loc36 = loc("reshape.182")
#loc37 = loc("reshape.160")
#loc38 = loc("custom-call.161")
#loc39 = loc("reshape.162")
#loc41 = loc("add.167")
#loc42 = loc("multiply.169")
#loc43 = loc("subtract.172")
#loc44 = loc("add.177")
#loc45 = loc("reshape.178")
#loc46 = loc("convert.183")
#loc47 = loc("gather.184")
#loc48 = loc("reshape.185")
#loc49 = loc("add.196")
#loc51 = loc("add.241")
#loc52 = loc("multiply.251")
#loc53 = loc("broadcast.261")
#loc54 = loc("subtract.262")
#loc55 = loc("multiply.218")
#loc57 = loc("add.224")
#loc58 = loc("multiply.234")
#loc59 = loc("reshape.235")
#loc60 = loc("add.255")
#loc61 = loc("rsqrt.256")
#loc62 = loc("reshape.263")
#loc63 = loc("broadcast.264")
#loc64 = loc("multiply.265")
#loc65 = loc("reshape.149")
#loc66 = loc("custom-call.150")
#loc67 = loc("reshape.151")
#loc68 = loc("broadcast.266")
#loc69 = loc("multiply.267")
#loc70 = loc("reshape.145")
#loc71 = loc("custom-call.146")
#loc72 = loc("reshape.147")
#loc73 = loc("broadcast.270")
#loc74 = loc("add.271")
#loc75 = loc("reshape.396")
#loc76 = loc("reshape.392")
#loc77 = loc("custom-call.393")
#loc78 = loc("reshape.394")
#loc79 = loc("transpose.395")
#loc80 = loc("dot.397")
#loc81 = loc("reshape.398")
#loc82 = loc("reshape.388")
#loc83 = loc("custom-call.389")
#loc84 = loc("reshape.390")
#loc85 = loc("broadcast.401")
#loc86 = loc("add.402")
#loc87 = loc("multiply.404")
#loc88 = loc("reshape.405")
#loc89 = loc("transpose.406")
#loc90 = loc("reshape.408")
#loc91 = loc("reshape.369")
#loc92 = loc("custom-call.370")
#loc93 = loc("reshape.371")
#loc94 = loc("transpose.372")
#loc95 = loc("dot.374")
#loc96 = loc("reshape.375")
#loc97 = loc("reshape.365")
#loc98 = loc("custom-call.366")
#loc99 = loc("reshape.367")
#loc100 = loc("broadcast.378")
#loc101 = loc("add.379")
#loc102 = loc("reshape.380")
#loc103 = loc("transpose.382")
#loc104 = loc("reshape.384")
#loc105 = loc("dot.409")
#loc106 = loc("reshape.410")
#loc107 = loc("broadcast.313")
#loc108 = loc("broadcast.315")
#loc109 = loc("subtract.316")
#loc110 = loc("compare.318")
#loc111 = loc("select.320")
#loc112 = loc("compare.290")
#loc113 = loc("convert.291")
#loc114 = loc("multiply.321")
#loc115 = loc("reshape.322")
#loc116 = loc("broadcast.328")
#loc117 = loc("reshape.343")
#loc118 = loc("convert.348")
#loc119 = loc("reshape.351")
#loc120 = loc("broadcast.352")
#loc121 = loc("add.353")
#loc122 = loc("compare.356")
#loc123 = loc("select.358")
#loc124 = loc("reshape.415")
#loc125 = loc("broadcast.416")
#loc126 = loc("add.417")
#loc127 = loc("convert.418")
#loc129 = loc("maximum.423")
#loc130 = loc("broadcast.425")
#loc131 = loc("subtract.426")
#loc132 = loc("exponential.427")
#loc134 = loc("add.432")
#loc135 = loc("broadcast.434")
#loc136 = loc("divide.435")
#loc137 = loc("convert.436")
#loc138 = loc("reshape.438")
#loc139 = loc("reshape.139")
#loc140 = loc("custom-call.140")
#loc141 = loc("reshape.141")
#loc142 = loc("transpose.142")
#loc143 = loc("dot.273")
#loc144 = loc("reshape.274")
#loc145 = loc("reshape.135")
#loc146 = loc("custom-call.136")
#loc147 = loc("reshape.137")
#loc148 = loc("broadcast.277")
#loc149 = loc("add.278")
#loc150 = loc("reshape.279")
#loc151 = loc("transpose.280")
#loc152 = loc("reshape.282")
#loc153 = loc("dot.439")
#loc154 = loc("reshape.440")
#loc155 = loc("transpose.441")
#loc156 = loc("reshape.443")
#loc157 = loc("reshape.129")
#loc158 = loc("custom-call.130")
#loc159 = loc("reshape.131")
#loc160 = loc("transpose.132")
#loc161 = loc("dot.444")
#loc162 = loc("reshape.445")
#loc163 = loc("reshape.125")
#loc164 = loc("custom-call.126")
#loc165 = loc("reshape.127")
#loc166 = loc("broadcast.448")
#loc167 = loc("add.449")
#loc168 = loc("add.452")
#loc169 = loc("reshape.453")
#loc171 = loc("add.498")
#loc172 = loc("multiply.508")
#loc173 = loc("broadcast.518")
#loc174 = loc("subtract.519")
#loc175 = loc("multiply.475")
#loc177 = loc("add.481")
#loc178 = loc("multiply.491")
#loc179 = loc("reshape.492")
#loc180 = loc("add.512")
#loc181 = loc("rsqrt.513")
#loc182 = loc("reshape.520")
#loc183 = loc("broadcast.521")
#loc184 = loc("multiply.522")
#loc185 = loc("reshape.117")
#loc186 = loc("custom-call.118")
#loc187 = loc("reshape.119")
#loc188 = loc("broadcast.523")
#loc189 = loc("multiply.524")
#loc190 = loc("reshape.113")
#loc191 = loc("custom-call.114")
#loc192 = loc("reshape.115")
#loc193 = loc("broadcast.527")
#loc194 = loc("add.528")
#loc195 = loc("reshape.107")
#loc196 = loc("custom-call.108")
#loc197 = loc("reshape.109")
#loc198 = loc("transpose.110")
#loc199 = loc("dot.529")
#loc200 = loc("reshape.100")
#loc201 = loc("custom-call.101")
#loc202 = loc("reshape.102")
#loc203 = loc("broadcast.534")
#loc204 = loc("add.535")
#loc205 = loc("maximum.538")
#loc206 = loc("reshape.93")
#loc207 = loc("custom-call.94")
#loc208 = loc("reshape.95")
#loc209 = loc("transpose.96")
#loc210 = loc("dot.539")
#loc211 = loc("reshape.86")
#loc212 = loc("custom-call.87")
#loc213 = loc("reshape.88")
#loc214 = loc("broadcast.544")
#loc215 = loc("add.545")
#loc216 = loc("add.548")
#loc217 = loc("reshape.549")
#loc219 = loc("add.594")
#loc220 = loc("multiply.604")
#loc221 = loc("broadcast.614")
#loc222 = loc("subtract.615")
#loc223 = loc("multiply.571")
#loc225 = loc("add.577")
#loc226 = loc("multiply.587")
#loc227 = loc("reshape.588")
#loc228 = loc("add.608")
#loc229 = loc("rsqrt.609")
#loc230 = loc("reshape.616")
#loc231 = loc("broadcast.617")
#loc232 = loc("multiply.618")
#loc233 = loc("reshape.77")
#loc234 = loc("custom-call.78")
#loc235 = loc("reshape.79")
#loc236 = loc("broadcast.619")
#loc237 = loc("multiply.620")
#loc238 = loc("reshape.73")
#loc239 = loc("custom-call.74")
#loc240 = loc("reshape.75")
#loc241 = loc("broadcast.623")
#loc242 = loc("add.624")
#loc243 = loc("reshape.625")
#loc244 = loc("reshape.67")
#loc245 = loc("custom-call.68")
#loc246 = loc("reshape.69")
#loc247 = loc("transpose.70")
#loc248 = loc("dot.626")
#loc249 = loc("reshape.627")
#loc250 = loc("broadcast.10")
#loc251 = loc("compare.7")
#loc252 = loc("convert.8")
#loc253 = loc("multiply.11")
#loc254 = loc("iota.14")
#loc256 = loc("compare.32")
#loc257 = loc("select.33")
#loc258 = loc("compare.35")
#loc259 = loc("minimum.36")
#loc260 = loc("select.34")
#loc261 = loc("select.37")
#loc262 = loc("convert.41")
#loc263 = loc("compare.50")
#loc264 = loc("add.47")
#loc265 = loc("select.51")
#loc266 = loc("reshape.64")
#loc267 = loc("concatenate.65")
#loc268 = loc("gather.628")
------------------ END OF MLIR MODULE ------------------
// -----// IR Dump Before VhloToVersionPass (vhlo-to-version) ('builtin.module' operation: @SyncTensorsGraph.630) //----- //
module @SyncTensorsGraph.630 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<8x32x!vhlo.i64_v1>, %arg1: !vhlo.tensor_v1<2x768x!vhlo.bf16_v1>, %arg2: !vhlo.tensor_v1<768x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<768x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<768x!vhlo.bf16_v1>, %arg5: !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg7: !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>, %arg8: !vhlo.tensor_v1<768x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<768x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<768x!vhlo.bf16_v1>, %arg11: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<768x!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>, %arg14: !vhlo.tensor_v1<768x!vhlo.bf16_v1>, %arg15: !vhlo.tensor_v1<768x!vhlo.bf16_v1>, %arg16: !vhlo.tensor_v1<8x32x!vhlo.i64_v1>, %arg17: !vhlo.tensor_v1<2050x768x!vhlo.bf16_v1>, %arg18: !vhlo.tensor_v1<50272x768x!vhlo.bf16_v1>, %arg19: !vhlo.tensor_v1<768x!vhlo.bf16_v1>, %arg20: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>, %arg21: !vhlo.tensor_v1<768x!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<8x2x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<i64>>}> : () -> !vhlo.tensor_v1<!vhlo.i64_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<i32>>}> : () -> !vhlo.tensor_v1<!vhlo.i32_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64>>}> : () -> !vhlo.tensor_v1<32x!vhlo.i64_v1>
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %5 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi32>>}> : () -> !vhlo.tensor_v1<32x!vhlo.i32_v1>
    %6 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<-2147483648> : tensor<i32>>}> : () -> !vhlo.tensor_v1<!vhlo.i32_v1>
    %7 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2> : tensor<i64>>}> : () -> !vhlo.tensor_v1<!vhlo.i64_v1>
    %8 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.250000e-01> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %9 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1> : tensor<i64>>}> : () -> !vhlo.tensor_v1<!vhlo.i64_v1>
    %10 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<-3.389530e+38> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %11 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.304630e-03> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %12 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.001360e-05> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %13 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<32> : tensor<8xi64>>}> : () -> !vhlo.tensor_v1<8x!vhlo.i64_v1>
    %14 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<8xi64>>}> : () -> !vhlo.tensor_v1<8x!vhlo.i64_v1>
    %15 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[[0], [1], [2], [3], [4], [5], [6], [7]]> : tensor<8x1xi64>>}> : () -> !vhlo.tensor_v1<8x1x!vhlo.i64_v1>
    %16 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %17 = "vhlo.broadcast_in_dim_v1"(%16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>
    %18 = "vhlo.broadcast_in_dim_v1"(%12) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x1x!vhlo.bf16_v1>
    %19 = "vhlo.broadcast_in_dim_v1"(%11) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x!vhlo.bf16_v1>
    %20 = "vhlo.broadcast_in_dim_v1"(%10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>
    %21 = "vhlo.broadcast_in_dim_v1"(%16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>
    %22 = "vhlo.broadcast_in_dim_v1"(%16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.bf16_v1>
    %23 = "vhlo.broadcast_in_dim_v1"(%10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.bf16_v1>
    %24 = "vhlo.broadcast_in_dim_v1"(%9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.i64_v1>
    %25 = "vhlo.broadcast_in_dim_v1"(%8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %26 = "vhlo.broadcast_in_dim_v1"(%12) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>
    %27 = "vhlo.broadcast_in_dim_v1"(%11) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>
    %28 = "vhlo.broadcast_in_dim_v1"(%7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1>
    %29 = "vhlo.broadcast_in_dim_v1"(%9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1>
    %30 = "vhlo.reshape_v1"(%arg18) : (!vhlo.tensor_v1<50272x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x50272x768x!vhlo.bf16_v1>
    %31 = "vhlo.custom_call_v1"(%30) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_embed_tokens_weight">}>} : (!vhlo.tensor_v1<1x50272x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x50272x768x!vhlo.bf16_v1>
    %32 = "vhlo.reshape_v1"(%31) : (!vhlo.tensor_v1<1x50272x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<50272x768x!vhlo.bf16_v1>
    %33 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>
    %34 = "vhlo.custom_call_v1"(%33) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>
    %35 = "vhlo.reshape_v1"(%34) : (!vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1>
    %36 = "vhlo.reshape_v1"(%34) : (!vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<256x!vhlo.i64_v1>
    %37 = "vhlo.convert_v1"(%36) : (!vhlo.tensor_v1<256x!vhlo.i64_v1>) -> !vhlo.tensor_v1<256x!vhlo.ui32_v1>
    %38 = "vhlo.gather_v2"(%32, %37) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 768]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<50272x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %39 = "vhlo.reshape_v1"(%38) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %40 = "vhlo.reshape_v1"(%arg17) : (!vhlo.tensor_v1<2050x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x2050x768x!vhlo.bf16_v1>
    %41 = "vhlo.custom_call_v1"(%40) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_embed_positions_weight">}>} : (!vhlo.tensor_v1<1x2050x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x2050x768x!vhlo.bf16_v1>
    %42 = "vhlo.reshape_v1"(%41) : (!vhlo.tensor_v1<1x2050x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2050x768x!vhlo.bf16_v1>
    %43 = "vhlo.reshape_v1"(%arg16) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>
    %44 = "vhlo.custom_call_v1"(%43) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_1">}>} : (!vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>
    %45 = "vhlo.reshape_v1"(%44) : (!vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1>
    %46 = "vhlo.reduce_window_v1"(%45, %0) <{base_dilations = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>, padding = #vhlo.tensor_v1<dense<[[0, 0], [31, 0]]> : tensor<2x2xi64>>, window_dilations = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>, window_dimensions = #vhlo.tensor_v1<dense<[1, 32]> : tensor<2xi64>>, window_strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.i64_v1>, %arg24: !vhlo.tensor_v1<!vhlo.i64_v1>):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.i64_v1>, !vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<!vhlo.i64_v1>
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> ()
    }) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>, !vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1>
    %47 = "vhlo.multiply_v1"(%46, %45) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>, !vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1>
    %48 = "vhlo.subtract_v1"(%47, %29) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>, !vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1>
    %49 = "vhlo.add_v1"(%48, %28) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>, !vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1>
    %50 = "vhlo.reshape_v1"(%49) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<256x!vhlo.i64_v1>
    %51 = "vhlo.convert_v1"(%50) : (!vhlo.tensor_v1<256x!vhlo.i64_v1>) -> !vhlo.tensor_v1<256x!vhlo.ui32_v1>
    %52 = "vhlo.gather_v2"(%42, %51) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 768]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<2050x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %53 = "vhlo.reshape_v1"(%52) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %54 = "vhlo.add_v1"(%39, %53) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %55 = "vhlo.reduce_v1"(%54, %16) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg24: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>
    %56 = "vhlo.multiply_v1"(%55, %27) : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>
    %57 = "vhlo.broadcast_in_dim_v1"(%56) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %58 = "vhlo.subtract_v1"(%54, %57) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %59 = "vhlo.multiply_v1"(%58, %58) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %60 = "vhlo.reduce_v1"(%59, %16) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg24: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>
    %61 = "vhlo.multiply_v1"(%60, %27) : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>
    %62 = "vhlo.reshape_v1"(%61) : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>
    %63 = "vhlo.add_v1"(%62, %26) : (!vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>
    %64 = "vhlo.rsqrt_v2"(%63) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>
    %65 = "vhlo.reshape_v1"(%64) : (!vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>
    %66 = "vhlo.broadcast_in_dim_v1"(%65) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %67 = "vhlo.multiply_v1"(%58, %66) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %68 = "vhlo.reshape_v1"(%arg15) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %69 = "vhlo.custom_call_v1"(%68) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_layer_norm_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %70 = "vhlo.reshape_v1"(%69) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1>
    %71 = "vhlo.broadcast_in_dim_v1"(%70) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %72 = "vhlo.multiply_v1"(%67, %71) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %73 = "vhlo.reshape_v1"(%arg14) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %74 = "vhlo.custom_call_v1"(%73) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_layer_norm_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %75 = "vhlo.reshape_v1"(%74) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1>
    %76 = "vhlo.broadcast_in_dim_v1"(%75) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %77 = "vhlo.add_v1"(%72, %76) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %78 = "vhlo.reshape_v1"(%77) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %79 = "vhlo.reshape_v1"(%arg22) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>
    %80 = "vhlo.custom_call_v1"(%79) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>
    %81 = "vhlo.reshape_v1"(%80) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>
    %82 = "vhlo.transpose_v1"(%81) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>
    %83 = "vhlo.dot_general_v2"(%78, %82) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %84 = "vhlo.reshape_v1"(%83) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %85 = "vhlo.reshape_v1"(%arg21) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %86 = "vhlo.custom_call_v1"(%85) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %87 = "vhlo.reshape_v1"(%86) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1>
    %88 = "vhlo.broadcast_in_dim_v1"(%87) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %89 = "vhlo.add_v1"(%84, %88) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %90 = "vhlo.multiply_v1"(%89, %25) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %91 = "vhlo.reshape_v1"(%90) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1>
    %92 = "vhlo.transpose_v1"(%91) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[8,12,32,64]{3,1,2,0}">} : (!vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x32x64x!vhlo.bf16_v1>
    %93 = "vhlo.reshape_v1"(%92) : (!vhlo.tensor_v1<8x12x32x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<96x32x64x!vhlo.bf16_v1>
    %94 = "vhlo.reshape_v1"(%arg20) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>
    %95 = "vhlo.custom_call_v1"(%94) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>
    %96 = "vhlo.reshape_v1"(%95) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>
    %97 = "vhlo.transpose_v1"(%96) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>
    %98 = "vhlo.dot_general_v2"(%78, %97) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %99 = "vhlo.reshape_v1"(%98) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %100 = "vhlo.reshape_v1"(%arg19) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %101 = "vhlo.custom_call_v1"(%100) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %102 = "vhlo.reshape_v1"(%101) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1>
    %103 = "vhlo.broadcast_in_dim_v1"(%102) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %104 = "vhlo.add_v1"(%99, %103) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %105 = "vhlo.reshape_v1"(%104) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1>
    %106 = "vhlo.transpose_v1"(%105) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 3, 1]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x64x32x!vhlo.bf16_v1>
    %107 = "vhlo.reshape_v1"(%106) : (!vhlo.tensor_v1<8x12x64x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<96x64x32x!vhlo.bf16_v1>
    %108 = "vhlo.dot_general_v2"(%93, %107) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<96x32x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<96x64x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<96x32x32x!vhlo.bf16_v1>
    %109 = "vhlo.reshape_v1"(%108) : (!vhlo.tensor_v1<96x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1>
    %110 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.i64_v1>
    %111 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.i64_v1>
    %112 = "vhlo.subtract_v1"(%110, %111) : (!vhlo.tensor_v1<32x32x!vhlo.i64_v1>, !vhlo.tensor_v1<32x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.i64_v1>
    %113 = "vhlo.compare_v1"(%112, %24) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GE>}> : (!vhlo.tensor_v1<32x32x!vhlo.i64_v1>, !vhlo.tensor_v1<32x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.bool_v1>
    %114 = "vhlo.select_v1"(%113, %23, %22) : (!vhlo.tensor_v1<32x32x!vhlo.bool_v1>, !vhlo.tensor_v1<32x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.bf16_v1>
    %115 = "vhlo.compare_v1"(%110, %111) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<32x32x!vhlo.i64_v1>, !vhlo.tensor_v1<32x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.bool_v1>
    %116 = "vhlo.convert_v1"(%115) : (!vhlo.tensor_v1<32x32x!vhlo.bool_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.bf16_v1>
    %117 = "vhlo.multiply_v1"(%114, %116) : (!vhlo.tensor_v1<32x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.bf16_v1>
    %118 = "vhlo.reshape_v1"(%117) : (!vhlo.tensor_v1<32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x32x32x!vhlo.bf16_v1>
    %119 = "vhlo.broadcast_in_dim_v1"(%118) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[1, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>
    %120 = "vhlo.reshape_v1"(%44) : (!vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x1x1x32x!vhlo.i64_v1>
    %121 = "vhlo.convert_v1"(%120) : (!vhlo.tensor_v1<8x1x1x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x1x1x32x!vhlo.bf16_v1>
    %122 = "vhlo.reshape_v1"(%121) : (!vhlo.tensor_v1<8x1x1x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x!vhlo.bf16_v1>
    %123 = "vhlo.broadcast_in_dim_v1"(%122) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<8x1x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>
    %124 = "vhlo.add_v1"(%119, %123) : (!vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>
    %125 = "vhlo.compare_v1"(%124, %21) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x32x!vhlo.bool_v1>
    %126 = "vhlo.select_v1"(%125, %20, %119) : (!vhlo.tensor_v1<8x1x32x32x!vhlo.bool_v1>, !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>
    %127 = "vhlo.reshape_v1"(%126) : (!vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x32x!vhlo.bf16_v1>
    %128 = "vhlo.broadcast_in_dim_v1"(%127) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<8x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1>
    %129 = "vhlo.add_v1"(%109, %128) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1>
    %130 = "vhlo.convert_v1"(%129) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>
    %131 = "vhlo.reduce_v1"(%130, %3) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg24: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %261 = "vhlo.maximum_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x!vhlo.f32_v1>
    %132 = "vhlo.broadcast_in_dim_v1"(%131) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<8x12x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>
    %133 = "vhlo.subtract_v1"(%130, %132) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>, !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>
    %134 = "vhlo.exponential_v2"(%133) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>
    %135 = "vhlo.reduce_v1"(%134, %4) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg24: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x!vhlo.f32_v1>
    %136 = "vhlo.broadcast_in_dim_v1"(%135) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<8x12x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>
    %137 = "vhlo.divide_v1"(%134, %136) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>, !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>
    %138 = "vhlo.convert_v1"(%137) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1>
    %139 = "vhlo.reshape_v1"(%138) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<96x32x32x!vhlo.bf16_v1>
    %140 = "vhlo.reshape_v1"(%arg13) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>
    %141 = "vhlo.custom_call_v1"(%140) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>
    %142 = "vhlo.reshape_v1"(%141) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>
    %143 = "vhlo.transpose_v1"(%142) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>
    %144 = "vhlo.dot_general_v2"(%78, %143) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %145 = "vhlo.reshape_v1"(%144) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %146 = "vhlo.reshape_v1"(%arg12) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %147 = "vhlo.custom_call_v1"(%146) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %148 = "vhlo.reshape_v1"(%147) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1>
    %149 = "vhlo.broadcast_in_dim_v1"(%148) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %150 = "vhlo.add_v1"(%145, %149) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %151 = "vhlo.reshape_v1"(%150) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1>
    %152 = "vhlo.transpose_v1"(%151) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[8,12,32,64]{3,1,2,0}">} : (!vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x32x64x!vhlo.bf16_v1>
    %153 = "vhlo.reshape_v1"(%152) : (!vhlo.tensor_v1<8x12x32x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<96x32x64x!vhlo.bf16_v1>
    %154 = "vhlo.dot_general_v2"(%139, %153) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<96x32x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<96x32x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<96x32x64x!vhlo.bf16_v1>
    %155 = "vhlo.reshape_v1"(%154) : (!vhlo.tensor_v1<96x32x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x32x64x!vhlo.bf16_v1>
    %156 = "vhlo.transpose_v1"(%155) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[8,32,12,64]{3,1,2,0}">} : (!vhlo.tensor_v1<8x12x32x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1>
    %157 = "vhlo.reshape_v1"(%156) : (!vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %158 = "vhlo.reshape_v1"(%arg11) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>
    %159 = "vhlo.custom_call_v1"(%158) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>
    %160 = "vhlo.reshape_v1"(%159) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>
    %161 = "vhlo.transpose_v1"(%160) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>
    %162 = "vhlo.dot_general_v2"(%157, %161) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %163 = "vhlo.reshape_v1"(%162) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %164 = "vhlo.reshape_v1"(%arg10) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %165 = "vhlo.custom_call_v1"(%164) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %166 = "vhlo.reshape_v1"(%165) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1>
    %167 = "vhlo.broadcast_in_dim_v1"(%166) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %168 = "vhlo.add_v1"(%163, %167) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %169 = "vhlo.add_v1"(%54, %168) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %170 = "vhlo.reshape_v1"(%169) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %171 = "vhlo.reduce_v1"(%170, %16) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg24: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x!vhlo.bf16_v1>
    %172 = "vhlo.multiply_v1"(%171, %19) : (!vhlo.tensor_v1<256x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x!vhlo.bf16_v1>
    %173 = "vhlo.broadcast_in_dim_v1"(%172) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %174 = "vhlo.subtract_v1"(%170, %173) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %175 = "vhlo.multiply_v1"(%174, %174) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %176 = "vhlo.reduce_v1"(%175, %16) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg24: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x!vhlo.bf16_v1>
    %177 = "vhlo.multiply_v1"(%176, %19) : (!vhlo.tensor_v1<256x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x!vhlo.bf16_v1>
    %178 = "vhlo.reshape_v1"(%177) : (!vhlo.tensor_v1<256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x1x!vhlo.bf16_v1>
    %179 = "vhlo.add_v1"(%178, %18) : (!vhlo.tensor_v1<256x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x1x!vhlo.bf16_v1>
    %180 = "vhlo.rsqrt_v2"(%179) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<256x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x1x!vhlo.bf16_v1>
    %181 = "vhlo.reshape_v1"(%180) : (!vhlo.tensor_v1<256x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x!vhlo.bf16_v1>
    %182 = "vhlo.broadcast_in_dim_v1"(%181) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %183 = "vhlo.multiply_v1"(%174, %182) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %184 = "vhlo.reshape_v1"(%arg9) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %185 = "vhlo.custom_call_v1"(%184) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_final_layer_norm_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %186 = "vhlo.reshape_v1"(%185) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1>
    %187 = "vhlo.broadcast_in_dim_v1"(%186) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %188 = "vhlo.multiply_v1"(%183, %187) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %189 = "vhlo.reshape_v1"(%arg8) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %190 = "vhlo.custom_call_v1"(%189) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_final_layer_norm_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %191 = "vhlo.reshape_v1"(%190) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1>
    %192 = "vhlo.broadcast_in_dim_v1"(%191) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %193 = "vhlo.add_v1"(%188, %192) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %194 = "vhlo.reshape_v1"(%arg7) : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>
    %195 = "vhlo.custom_call_v1"(%194) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_fc1_weight">}>} : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>
    %196 = "vhlo.reshape_v1"(%195) : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>
    %197 = "vhlo.transpose_v1"(%196) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,3072]{0,1}">} : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>
    %198 = "vhlo.dot_general_v2"(%193, %197) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>
    %199 = "vhlo.reshape_v1"(%arg6) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %200 = "vhlo.custom_call_v1"(%199) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %201 = "vhlo.reshape_v1"(%200) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1>
    %202 = "vhlo.broadcast_in_dim_v1"(%201) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>
    %203 = "vhlo.add_v1"(%198, %202) : (!vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>
    %204 = "vhlo.maximum_v1"(%203, %17) : (!vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>
    %205 = "vhlo.reshape_v1"(%arg5) : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>
    %206 = "vhlo.custom_call_v1"(%205) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_fc2_weight">}>} : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>
    %207 = "vhlo.reshape_v1"(%206) : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>
    %208 = "vhlo.transpose_v1"(%207) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,768]{0,1}">} : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>
    %209 = "vhlo.dot_general_v2"(%204, %208) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %210 = "vhlo.reshape_v1"(%arg4) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %211 = "vhlo.custom_call_v1"(%210) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %212 = "vhlo.reshape_v1"(%211) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1>
    %213 = "vhlo.broadcast_in_dim_v1"(%212) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %214 = "vhlo.add_v1"(%209, %213) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %215 = "vhlo.add_v1"(%170, %214) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %216 = "vhlo.reshape_v1"(%215) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %217 = "vhlo.reduce_v1"(%216, %16) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg24: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>
    %218 = "vhlo.multiply_v1"(%217, %27) : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>
    %219 = "vhlo.broadcast_in_dim_v1"(%218) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %220 = "vhlo.subtract_v1"(%216, %219) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %221 = "vhlo.multiply_v1"(%220, %220) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %222 = "vhlo.reduce_v1"(%221, %16) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg24: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>
    %223 = "vhlo.multiply_v1"(%222, %27) : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>
    %224 = "vhlo.reshape_v1"(%223) : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>
    %225 = "vhlo.add_v1"(%224, %26) : (!vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>
    %226 = "vhlo.rsqrt_v2"(%225) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>
    %227 = "vhlo.reshape_v1"(%226) : (!vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>
    %228 = "vhlo.broadcast_in_dim_v1"(%227) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %229 = "vhlo.multiply_v1"(%220, %228) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %230 = "vhlo.reshape_v1"(%arg3) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %231 = "vhlo.custom_call_v1"(%230) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_final_layer_norm_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %232 = "vhlo.reshape_v1"(%231) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1>
    %233 = "vhlo.broadcast_in_dim_v1"(%232) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %234 = "vhlo.multiply_v1"(%229, %233) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %235 = "vhlo.reshape_v1"(%arg2) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %236 = "vhlo.custom_call_v1"(%235) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_final_layer_norm_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %237 = "vhlo.reshape_v1"(%236) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1>
    %238 = "vhlo.broadcast_in_dim_v1"(%237) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %239 = "vhlo.add_v1"(%234, %238) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %240 = "vhlo.reshape_v1"(%239) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %241 = "vhlo.reshape_v1"(%arg1) : (!vhlo.tensor_v1<2x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x2x768x!vhlo.bf16_v1>
    %242 = "vhlo.custom_call_v1"(%241) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___score_weight">}>} : (!vhlo.tensor_v1<1x2x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x2x768x!vhlo.bf16_v1>
    %243 = "vhlo.reshape_v1"(%242) : (!vhlo.tensor_v1<1x2x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x768x!vhlo.bf16_v1>
    %244 = "vhlo.transpose_v1"(%243) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,2]{0,1}">} : (!vhlo.tensor_v1<2x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x2x!vhlo.bf16_v1>
    %245 = "vhlo.dot_general_v2"(%240, %244) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x2x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x2x!vhlo.bf16_v1>
    %246 = "vhlo.reshape_v1"(%245) : (!vhlo.tensor_v1<256x2x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x2x!vhlo.bf16_v1>
    %247 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i32_v1>
    %248 = "vhlo.compare_v1"(%35, %29) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 NE>}> : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>, !vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bool_v1>
    %249 = "vhlo.convert_v1"(%248) : (!vhlo.tensor_v1<8x32x!vhlo.bool_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i32_v1>
    %250 = "vhlo.multiply_v1"(%247, %249) : (!vhlo.tensor_v1<8x32x!vhlo.i32_v1>, !vhlo.tensor_v1<8x32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i32_v1>
    %251 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<32x!vhlo.i32_v1>
    %252 = "vhlo.broadcast_in_dim_v1"(%251) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i32_v1>
    %253:2 = "vhlo.reduce_v1"(%250, %252, %6, %1) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.i32_v1>, %arg24: !vhlo.tensor_v1<!vhlo.i32_v1>, %arg25: !vhlo.tensor_v1<!vhlo.i32_v1>, %arg26: !vhlo.tensor_v1<!vhlo.i32_v1>):
      %261 = "vhlo.compare_v1"(%arg23, %arg25) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GE>}> : (!vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
      %262 = "vhlo.select_v1"(%261, %arg23, %arg25) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1>
      %263 = "vhlo.compare_v1"(%arg23, %arg25) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
      %264 = "vhlo.minimum_v1"(%arg24, %arg26) : (!vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1>
      %265 = "vhlo.select_v1"(%261, %arg24, %arg26) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1>
      %266 = "vhlo.select_v1"(%263, %264, %265) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1>
      "vhlo.return_v1"(%262, %266) : (!vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> ()
    }) : (!vhlo.tensor_v1<8x32x!vhlo.i32_v1>, !vhlo.tensor_v1<8x32x!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> (!vhlo.tensor_v1<8x!vhlo.i32_v1>, !vhlo.tensor_v1<8x!vhlo.i32_v1>)
    %254 = "vhlo.convert_v1"(%253#1) : (!vhlo.tensor_v1<8x!vhlo.i32_v1>) -> !vhlo.tensor_v1<8x!vhlo.i64_v1>
    %255 = "vhlo.compare_v1"(%254, %14) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 LT>}> : (!vhlo.tensor_v1<8x!vhlo.i64_v1>, !vhlo.tensor_v1<8x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x!vhlo.bool_v1>
    %256 = "vhlo.add_v1"(%254, %13) : (!vhlo.tensor_v1<8x!vhlo.i64_v1>, !vhlo.tensor_v1<8x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x!vhlo.i64_v1>
    %257 = "vhlo.select_v1"(%255, %256, %254) : (!vhlo.tensor_v1<8x!vhlo.bool_v1>, !vhlo.tensor_v1<8x!vhlo.i64_v1>, !vhlo.tensor_v1<8x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x!vhlo.i64_v1>
    %258 = "vhlo.reshape_v1"(%257) : (!vhlo.tensor_v1<8x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x1x!vhlo.i64_v1>
    %259 = "vhlo.concatenate_v1"(%15, %258) <{dimension = #vhlo.integer_v1<1 : i64>}> : (!vhlo.tensor_v1<8x1x!vhlo.i64_v1>, !vhlo.tensor_v1<8x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x2x!vhlo.i64_v1>
    %260 = "vhlo.gather_v2"(%246, %259) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>, start_index_map = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<8x32x2x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x2x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x2x!vhlo.bf16_v1>
    "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<8x2x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump Before VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.630) //----- //
module @SyncTensorsGraph.630 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<8x32x!vhlo.i64_v1>, %arg1: !vhlo.tensor_v1<2x768x!vhlo.bf16_v1>, %arg2: !vhlo.tensor_v1<768x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<768x!vhlo.bf16_v1>, %arg4: !vhlo.tensor_v1<768x!vhlo.bf16_v1>, %arg5: !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg7: !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>, %arg8: !vhlo.tensor_v1<768x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<768x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<768x!vhlo.bf16_v1>, %arg11: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<768x!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>, %arg14: !vhlo.tensor_v1<768x!vhlo.bf16_v1>, %arg15: !vhlo.tensor_v1<768x!vhlo.bf16_v1>, %arg16: !vhlo.tensor_v1<8x32x!vhlo.i64_v1>, %arg17: !vhlo.tensor_v1<2050x768x!vhlo.bf16_v1>, %arg18: !vhlo.tensor_v1<50272x768x!vhlo.bf16_v1>, %arg19: !vhlo.tensor_v1<768x!vhlo.bf16_v1>, %arg20: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>, %arg21: !vhlo.tensor_v1<768x!vhlo.bf16_v1>, %arg22: !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<8x2x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<i64>>}> : () -> !vhlo.tensor_v1<!vhlo.i64_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<i32>>}> : () -> !vhlo.tensor_v1<!vhlo.i32_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64>>}> : () -> !vhlo.tensor_v1<32x!vhlo.i64_v1>
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %5 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi32>>}> : () -> !vhlo.tensor_v1<32x!vhlo.i32_v1>
    %6 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<-2147483648> : tensor<i32>>}> : () -> !vhlo.tensor_v1<!vhlo.i32_v1>
    %7 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2> : tensor<i64>>}> : () -> !vhlo.tensor_v1<!vhlo.i64_v1>
    %8 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.250000e-01> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %9 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1> : tensor<i64>>}> : () -> !vhlo.tensor_v1<!vhlo.i64_v1>
    %10 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<-3.389530e+38> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %11 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.304630e-03> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %12 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<1.001360e-05> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %13 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<32> : tensor<8xi64>>}> : () -> !vhlo.tensor_v1<8x!vhlo.i64_v1>
    %14 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<8xi64>>}> : () -> !vhlo.tensor_v1<8x!vhlo.i64_v1>
    %15 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<[[0], [1], [2], [3], [4], [5], [6], [7]]> : tensor<8x1xi64>>}> : () -> !vhlo.tensor_v1<8x1x!vhlo.i64_v1>
    %16 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %17 = "vhlo.broadcast_in_dim_v1"(%16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>
    %18 = "vhlo.broadcast_in_dim_v1"(%12) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x1x!vhlo.bf16_v1>
    %19 = "vhlo.broadcast_in_dim_v1"(%11) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x!vhlo.bf16_v1>
    %20 = "vhlo.broadcast_in_dim_v1"(%10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>
    %21 = "vhlo.broadcast_in_dim_v1"(%16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>
    %22 = "vhlo.broadcast_in_dim_v1"(%16) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.bf16_v1>
    %23 = "vhlo.broadcast_in_dim_v1"(%10) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.bf16_v1>
    %24 = "vhlo.broadcast_in_dim_v1"(%9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.i64_v1>
    %25 = "vhlo.broadcast_in_dim_v1"(%8) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %26 = "vhlo.broadcast_in_dim_v1"(%12) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>
    %27 = "vhlo.broadcast_in_dim_v1"(%11) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>
    %28 = "vhlo.broadcast_in_dim_v1"(%7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1>
    %29 = "vhlo.broadcast_in_dim_v1"(%9) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1>
    %30 = "vhlo.reshape_v1"(%arg18) : (!vhlo.tensor_v1<50272x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x50272x768x!vhlo.bf16_v1>
    %31 = "vhlo.custom_call_v1"(%30) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_embed_tokens_weight">}>} : (!vhlo.tensor_v1<1x50272x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x50272x768x!vhlo.bf16_v1>
    %32 = "vhlo.reshape_v1"(%31) : (!vhlo.tensor_v1<1x50272x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<50272x768x!vhlo.bf16_v1>
    %33 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>
    %34 = "vhlo.custom_call_v1"(%33) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_0">}>} : (!vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>
    %35 = "vhlo.reshape_v1"(%34) : (!vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1>
    %36 = "vhlo.reshape_v1"(%34) : (!vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<256x!vhlo.i64_v1>
    %37 = "vhlo.convert_v1"(%36) : (!vhlo.tensor_v1<256x!vhlo.i64_v1>) -> !vhlo.tensor_v1<256x!vhlo.ui32_v1>
    %38 = "vhlo.gather_v2"(%32, %37) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 768]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<50272x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %39 = "vhlo.reshape_v1"(%38) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %40 = "vhlo.reshape_v1"(%arg17) : (!vhlo.tensor_v1<2050x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x2050x768x!vhlo.bf16_v1>
    %41 = "vhlo.custom_call_v1"(%40) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_embed_positions_weight">}>} : (!vhlo.tensor_v1<1x2050x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x2050x768x!vhlo.bf16_v1>
    %42 = "vhlo.reshape_v1"(%41) : (!vhlo.tensor_v1<1x2050x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2050x768x!vhlo.bf16_v1>
    %43 = "vhlo.reshape_v1"(%arg16) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>
    %44 = "vhlo.custom_call_v1"(%43) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"input">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"args_1">}>} : (!vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>
    %45 = "vhlo.reshape_v1"(%44) : (!vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1>
    %46 = "vhlo.reduce_window_v1"(%45, %0) <{base_dilations = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>, padding = #vhlo.tensor_v1<dense<[[0, 0], [31, 0]]> : tensor<2x2xi64>>, window_dilations = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>, window_dimensions = #vhlo.tensor_v1<dense<[1, 32]> : tensor<2xi64>>, window_strides = #vhlo.tensor_v1<dense<1> : tensor<2xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.i64_v1>, %arg24: !vhlo.tensor_v1<!vhlo.i64_v1>):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.i64_v1>, !vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<!vhlo.i64_v1>
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> ()
    }) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>, !vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1>
    %47 = "vhlo.multiply_v1"(%46, %45) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>, !vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1>
    %48 = "vhlo.subtract_v1"(%47, %29) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>, !vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1>
    %49 = "vhlo.add_v1"(%48, %28) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>, !vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1>
    %50 = "vhlo.reshape_v1"(%49) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<256x!vhlo.i64_v1>
    %51 = "vhlo.convert_v1"(%50) : (!vhlo.tensor_v1<256x!vhlo.i64_v1>) -> !vhlo.tensor_v1<256x!vhlo.ui32_v1>
    %52 = "vhlo.gather_v2"(%42, %51) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 768]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<2050x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %53 = "vhlo.reshape_v1"(%52) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %54 = "vhlo.add_v1"(%39, %53) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %55 = "vhlo.reduce_v1"(%54, %16) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg24: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>
    %56 = "vhlo.multiply_v1"(%55, %27) : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>
    %57 = "vhlo.broadcast_in_dim_v1"(%56) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %58 = "vhlo.subtract_v1"(%54, %57) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %59 = "vhlo.multiply_v1"(%58, %58) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %60 = "vhlo.reduce_v1"(%59, %16) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg24: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>
    %61 = "vhlo.multiply_v1"(%60, %27) : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>
    %62 = "vhlo.reshape_v1"(%61) : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>
    %63 = "vhlo.add_v1"(%62, %26) : (!vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>
    %64 = "vhlo.rsqrt_v2"(%63) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>
    %65 = "vhlo.reshape_v1"(%64) : (!vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>
    %66 = "vhlo.broadcast_in_dim_v1"(%65) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %67 = "vhlo.multiply_v1"(%58, %66) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %68 = "vhlo.reshape_v1"(%arg15) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %69 = "vhlo.custom_call_v1"(%68) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_layer_norm_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %70 = "vhlo.reshape_v1"(%69) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1>
    %71 = "vhlo.broadcast_in_dim_v1"(%70) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %72 = "vhlo.multiply_v1"(%67, %71) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %73 = "vhlo.reshape_v1"(%arg14) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %74 = "vhlo.custom_call_v1"(%73) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_layer_norm_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %75 = "vhlo.reshape_v1"(%74) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1>
    %76 = "vhlo.broadcast_in_dim_v1"(%75) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %77 = "vhlo.add_v1"(%72, %76) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %78 = "vhlo.reshape_v1"(%77) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %79 = "vhlo.reshape_v1"(%arg22) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>
    %80 = "vhlo.custom_call_v1"(%79) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_q_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>
    %81 = "vhlo.reshape_v1"(%80) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>
    %82 = "vhlo.transpose_v1"(%81) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>
    %83 = "vhlo.dot_general_v2"(%78, %82) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %84 = "vhlo.reshape_v1"(%83) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %85 = "vhlo.reshape_v1"(%arg21) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %86 = "vhlo.custom_call_v1"(%85) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_q_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %87 = "vhlo.reshape_v1"(%86) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1>
    %88 = "vhlo.broadcast_in_dim_v1"(%87) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %89 = "vhlo.add_v1"(%84, %88) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %90 = "vhlo.multiply_v1"(%89, %25) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %91 = "vhlo.reshape_v1"(%90) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1>
    %92 = "vhlo.transpose_v1"(%91) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[8,12,32,64]{3,1,2,0}">} : (!vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x32x64x!vhlo.bf16_v1>
    %93 = "vhlo.reshape_v1"(%92) : (!vhlo.tensor_v1<8x12x32x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<96x32x64x!vhlo.bf16_v1>
    %94 = "vhlo.reshape_v1"(%arg20) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>
    %95 = "vhlo.custom_call_v1"(%94) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_k_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>
    %96 = "vhlo.reshape_v1"(%95) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>
    %97 = "vhlo.transpose_v1"(%96) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>
    %98 = "vhlo.dot_general_v2"(%78, %97) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %99 = "vhlo.reshape_v1"(%98) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %100 = "vhlo.reshape_v1"(%arg19) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %101 = "vhlo.custom_call_v1"(%100) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_k_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %102 = "vhlo.reshape_v1"(%101) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1>
    %103 = "vhlo.broadcast_in_dim_v1"(%102) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %104 = "vhlo.add_v1"(%99, %103) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %105 = "vhlo.reshape_v1"(%104) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1>
    %106 = "vhlo.transpose_v1"(%105) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 3, 1]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x64x32x!vhlo.bf16_v1>
    %107 = "vhlo.reshape_v1"(%106) : (!vhlo.tensor_v1<8x12x64x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<96x64x32x!vhlo.bf16_v1>
    %108 = "vhlo.dot_general_v2"(%93, %107) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<96x32x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<96x64x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<96x32x32x!vhlo.bf16_v1>
    %109 = "vhlo.reshape_v1"(%108) : (!vhlo.tensor_v1<96x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1>
    %110 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.i64_v1>
    %111 = "vhlo.broadcast_in_dim_v1"(%2) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.i64_v1>
    %112 = "vhlo.subtract_v1"(%110, %111) : (!vhlo.tensor_v1<32x32x!vhlo.i64_v1>, !vhlo.tensor_v1<32x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.i64_v1>
    %113 = "vhlo.compare_v1"(%112, %24) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GE>}> : (!vhlo.tensor_v1<32x32x!vhlo.i64_v1>, !vhlo.tensor_v1<32x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.bool_v1>
    %114 = "vhlo.select_v1"(%113, %23, %22) : (!vhlo.tensor_v1<32x32x!vhlo.bool_v1>, !vhlo.tensor_v1<32x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.bf16_v1>
    %115 = "vhlo.compare_v1"(%110, %111) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<32x32x!vhlo.i64_v1>, !vhlo.tensor_v1<32x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.bool_v1>
    %116 = "vhlo.convert_v1"(%115) : (!vhlo.tensor_v1<32x32x!vhlo.bool_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.bf16_v1>
    %117 = "vhlo.multiply_v1"(%114, %116) : (!vhlo.tensor_v1<32x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<32x32x!vhlo.bf16_v1>
    %118 = "vhlo.reshape_v1"(%117) : (!vhlo.tensor_v1<32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x32x32x!vhlo.bf16_v1>
    %119 = "vhlo.broadcast_in_dim_v1"(%118) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[1, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>
    %120 = "vhlo.reshape_v1"(%44) : (!vhlo.tensor_v1<1x8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x1x1x32x!vhlo.i64_v1>
    %121 = "vhlo.convert_v1"(%120) : (!vhlo.tensor_v1<8x1x1x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x1x1x32x!vhlo.bf16_v1>
    %122 = "vhlo.reshape_v1"(%121) : (!vhlo.tensor_v1<8x1x1x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x!vhlo.bf16_v1>
    %123 = "vhlo.broadcast_in_dim_v1"(%122) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<8x1x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>
    %124 = "vhlo.add_v1"(%119, %123) : (!vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>
    %125 = "vhlo.compare_v1"(%124, %21) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x32x!vhlo.bool_v1>
    %126 = "vhlo.select_v1"(%125, %20, %119) : (!vhlo.tensor_v1<8x1x32x32x!vhlo.bool_v1>, !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>
    %127 = "vhlo.reshape_v1"(%126) : (!vhlo.tensor_v1<8x1x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x32x!vhlo.bf16_v1>
    %128 = "vhlo.broadcast_in_dim_v1"(%127) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<8x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1>
    %129 = "vhlo.add_v1"(%109, %128) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1>
    %130 = "vhlo.convert_v1"(%129) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>
    %131 = "vhlo.reduce_v1"(%130, %3) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg24: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %261 = "vhlo.maximum_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x!vhlo.f32_v1>
    %132 = "vhlo.broadcast_in_dim_v1"(%131) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<8x12x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>
    %133 = "vhlo.subtract_v1"(%130, %132) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>, !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>
    %134 = "vhlo.exponential_v2"(%133) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>
    %135 = "vhlo.reduce_v1"(%134, %4) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg24: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x!vhlo.f32_v1>
    %136 = "vhlo.broadcast_in_dim_v1"(%135) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<8x12x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>
    %137 = "vhlo.divide_v1"(%134, %136) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>, !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>
    %138 = "vhlo.convert_v1"(%137) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.f32_v1>) -> !vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1>
    %139 = "vhlo.reshape_v1"(%138) : (!vhlo.tensor_v1<8x12x32x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<96x32x32x!vhlo.bf16_v1>
    %140 = "vhlo.reshape_v1"(%arg13) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>
    %141 = "vhlo.custom_call_v1"(%140) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_v_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>
    %142 = "vhlo.reshape_v1"(%141) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>
    %143 = "vhlo.transpose_v1"(%142) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>
    %144 = "vhlo.dot_general_v2"(%78, %143) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %145 = "vhlo.reshape_v1"(%144) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %146 = "vhlo.reshape_v1"(%arg12) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %147 = "vhlo.custom_call_v1"(%146) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_v_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %148 = "vhlo.reshape_v1"(%147) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1>
    %149 = "vhlo.broadcast_in_dim_v1"(%148) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %150 = "vhlo.add_v1"(%145, %149) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %151 = "vhlo.reshape_v1"(%150) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1>
    %152 = "vhlo.transpose_v1"(%151) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[8,12,32,64]{3,1,2,0}">} : (!vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x32x64x!vhlo.bf16_v1>
    %153 = "vhlo.reshape_v1"(%152) : (!vhlo.tensor_v1<8x12x32x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<96x32x64x!vhlo.bf16_v1>
    %154 = "vhlo.dot_general_v2"(%139, %153) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<96x32x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<96x32x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<96x32x64x!vhlo.bf16_v1>
    %155 = "vhlo.reshape_v1"(%154) : (!vhlo.tensor_v1<96x32x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x12x32x64x!vhlo.bf16_v1>
    %156 = "vhlo.transpose_v1"(%155) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[8,32,12,64]{3,1,2,0}">} : (!vhlo.tensor_v1<8x12x32x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1>
    %157 = "vhlo.reshape_v1"(%156) : (!vhlo.tensor_v1<8x32x12x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %158 = "vhlo.reshape_v1"(%arg11) : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>
    %159 = "vhlo.custom_call_v1"(%158) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_out_proj_weight">}>} : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>
    %160 = "vhlo.reshape_v1"(%159) : (!vhlo.tensor_v1<1x768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>
    %161 = "vhlo.transpose_v1"(%160) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,768]{0,1}">} : (!vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>
    %162 = "vhlo.dot_general_v2"(%157, %161) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %163 = "vhlo.reshape_v1"(%162) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %164 = "vhlo.reshape_v1"(%arg10) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %165 = "vhlo.custom_call_v1"(%164) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_self_attn_out_proj_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %166 = "vhlo.reshape_v1"(%165) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1>
    %167 = "vhlo.broadcast_in_dim_v1"(%166) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %168 = "vhlo.add_v1"(%163, %167) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %169 = "vhlo.add_v1"(%54, %168) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %170 = "vhlo.reshape_v1"(%169) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %171 = "vhlo.reduce_v1"(%170, %16) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg24: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x!vhlo.bf16_v1>
    %172 = "vhlo.multiply_v1"(%171, %19) : (!vhlo.tensor_v1<256x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x!vhlo.bf16_v1>
    %173 = "vhlo.broadcast_in_dim_v1"(%172) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %174 = "vhlo.subtract_v1"(%170, %173) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %175 = "vhlo.multiply_v1"(%174, %174) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %176 = "vhlo.reduce_v1"(%175, %16) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg24: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x!vhlo.bf16_v1>
    %177 = "vhlo.multiply_v1"(%176, %19) : (!vhlo.tensor_v1<256x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x!vhlo.bf16_v1>
    %178 = "vhlo.reshape_v1"(%177) : (!vhlo.tensor_v1<256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x1x!vhlo.bf16_v1>
    %179 = "vhlo.add_v1"(%178, %18) : (!vhlo.tensor_v1<256x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x1x!vhlo.bf16_v1>
    %180 = "vhlo.rsqrt_v2"(%179) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<256x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x1x!vhlo.bf16_v1>
    %181 = "vhlo.reshape_v1"(%180) : (!vhlo.tensor_v1<256x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x!vhlo.bf16_v1>
    %182 = "vhlo.broadcast_in_dim_v1"(%181) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %183 = "vhlo.multiply_v1"(%174, %182) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %184 = "vhlo.reshape_v1"(%arg9) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %185 = "vhlo.custom_call_v1"(%184) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_final_layer_norm_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %186 = "vhlo.reshape_v1"(%185) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1>
    %187 = "vhlo.broadcast_in_dim_v1"(%186) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %188 = "vhlo.multiply_v1"(%183, %187) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %189 = "vhlo.reshape_v1"(%arg8) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %190 = "vhlo.custom_call_v1"(%189) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_final_layer_norm_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %191 = "vhlo.reshape_v1"(%190) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1>
    %192 = "vhlo.broadcast_in_dim_v1"(%191) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %193 = "vhlo.add_v1"(%188, %192) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %194 = "vhlo.reshape_v1"(%arg7) : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>
    %195 = "vhlo.custom_call_v1"(%194) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_fc1_weight">}>} : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>
    %196 = "vhlo.reshape_v1"(%195) : (!vhlo.tensor_v1<1x3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>
    %197 = "vhlo.transpose_v1"(%196) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,3072]{0,1}">} : (!vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>
    %198 = "vhlo.dot_general_v2"(%193, %197) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>
    %199 = "vhlo.reshape_v1"(%arg6) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %200 = "vhlo.custom_call_v1"(%199) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_fc1_bias">}>} : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %201 = "vhlo.reshape_v1"(%200) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.bf16_v1>
    %202 = "vhlo.broadcast_in_dim_v1"(%201) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>
    %203 = "vhlo.add_v1"(%198, %202) : (!vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>
    %204 = "vhlo.maximum_v1"(%203, %17) : (!vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>
    %205 = "vhlo.reshape_v1"(%arg5) : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>
    %206 = "vhlo.custom_call_v1"(%205) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_fc2_weight">}>} : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>
    %207 = "vhlo.reshape_v1"(%206) : (!vhlo.tensor_v1<1x768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>
    %208 = "vhlo.transpose_v1"(%207) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[3072,768]{0,1}">} : (!vhlo.tensor_v1<768x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>
    %209 = "vhlo.dot_general_v2"(%204, %208) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %210 = "vhlo.reshape_v1"(%arg4) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %211 = "vhlo.custom_call_v1"(%210) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_layers_0_fc2_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %212 = "vhlo.reshape_v1"(%211) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1>
    %213 = "vhlo.broadcast_in_dim_v1"(%212) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %214 = "vhlo.add_v1"(%209, %213) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %215 = "vhlo.add_v1"(%170, %214) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %216 = "vhlo.reshape_v1"(%215) : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %217 = "vhlo.reduce_v1"(%216, %16) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg24: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>
    %218 = "vhlo.multiply_v1"(%217, %27) : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>
    %219 = "vhlo.broadcast_in_dim_v1"(%218) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %220 = "vhlo.subtract_v1"(%216, %219) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %221 = "vhlo.multiply_v1"(%220, %220) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %222 = "vhlo.reduce_v1"(%221, %16) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg24: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %261 = "vhlo.add_v1"(%arg23, %arg24) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%261) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>
    %223 = "vhlo.multiply_v1"(%222, %27) : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>
    %224 = "vhlo.reshape_v1"(%223) : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>
    %225 = "vhlo.add_v1"(%224, %26) : (!vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>
    %226 = "vhlo.rsqrt_v2"(%225) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>
    %227 = "vhlo.reshape_v1"(%226) : (!vhlo.tensor_v1<8x32x1x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bf16_v1>
    %228 = "vhlo.broadcast_in_dim_v1"(%227) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<8x32x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %229 = "vhlo.multiply_v1"(%220, %228) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %230 = "vhlo.reshape_v1"(%arg3) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %231 = "vhlo.custom_call_v1"(%230) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_final_layer_norm_weight">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %232 = "vhlo.reshape_v1"(%231) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1>
    %233 = "vhlo.broadcast_in_dim_v1"(%232) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %234 = "vhlo.multiply_v1"(%229, %233) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %235 = "vhlo.reshape_v1"(%arg2) : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %236 = "vhlo.custom_call_v1"(%235) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___model_decoder_final_layer_norm_bias">}>} : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>
    %237 = "vhlo.reshape_v1"(%236) : (!vhlo.tensor_v1<1x1x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x!vhlo.bf16_v1>
    %238 = "vhlo.broadcast_in_dim_v1"(%237) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %239 = "vhlo.add_v1"(%234, %238) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>
    %240 = "vhlo.reshape_v1"(%239) : (!vhlo.tensor_v1<8x32x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x768x!vhlo.bf16_v1>
    %241 = "vhlo.reshape_v1"(%arg1) : (!vhlo.tensor_v1<2x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x2x768x!vhlo.bf16_v1>
    %242 = "vhlo.custom_call_v1"(%241) <{api_version = #vhlo<api_version_v1 API_VERSION_UNSPECIFIED>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"tt.mark_argument">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"ttcore.argument_type"> = #vhlo.string_v1<"parameter">, #vhlo.string_v1<"ttir.name"> = #vhlo.string_v1<"l__self___score_weight">}>} : (!vhlo.tensor_v1<1x2x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x2x768x!vhlo.bf16_v1>
    %243 = "vhlo.reshape_v1"(%242) : (!vhlo.tensor_v1<1x2x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<2x768x!vhlo.bf16_v1>
    %244 = "vhlo.transpose_v1"(%243) <{permutation = #vhlo.tensor_v1<dense<[1, 0]> : tensor<2xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xindex>>, xla_shape = #vhlo.string_v1<"bf16[768,2]{0,1}">} : (!vhlo.tensor_v1<2x768x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<768x2x!vhlo.bf16_v1>
    %245 = "vhlo.dot_general_v2"(%240, %244) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<256x768x!vhlo.bf16_v1>, !vhlo.tensor_v1<768x2x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<256x2x!vhlo.bf16_v1>
    %246 = "vhlo.reshape_v1"(%245) : (!vhlo.tensor_v1<256x2x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x32x2x!vhlo.bf16_v1>
    %247 = "vhlo.broadcast_in_dim_v1"(%5) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i32_v1>
    %248 = "vhlo.compare_v1"(%35, %29) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 NE>}> : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>, !vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.bool_v1>
    %249 = "vhlo.convert_v1"(%248) : (!vhlo.tensor_v1<8x32x!vhlo.bool_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i32_v1>
    %250 = "vhlo.multiply_v1"(%247, %249) : (!vhlo.tensor_v1<8x32x!vhlo.i32_v1>, !vhlo.tensor_v1<8x32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i32_v1>
    %251 = "vhlo.iota_v1"() <{iota_dimension = #vhlo.integer_v1<0 : i64>}> : () -> !vhlo.tensor_v1<32x!vhlo.i32_v1>
    %252 = "vhlo.broadcast_in_dim_v1"(%251) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<32x!vhlo.i32_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i32_v1>
    %253:2 = "vhlo.reduce_v1"(%250, %252, %6, %1) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg23: !vhlo.tensor_v1<!vhlo.i32_v1>, %arg24: !vhlo.tensor_v1<!vhlo.i32_v1>, %arg25: !vhlo.tensor_v1<!vhlo.i32_v1>, %arg26: !vhlo.tensor_v1<!vhlo.i32_v1>):
      %261 = "vhlo.compare_v1"(%arg23, %arg25) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GE>}> : (!vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
      %262 = "vhlo.select_v1"(%261, %arg23, %arg25) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1>
      %263 = "vhlo.compare_v1"(%arg23, %arg25) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 EQ>}> : (!vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.bool_v1>
      %264 = "vhlo.minimum_v1"(%arg24, %arg26) : (!vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1>
      %265 = "vhlo.select_v1"(%261, %arg24, %arg26) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1>
      %266 = "vhlo.select_v1"(%263, %264, %265) : (!vhlo.tensor_v1<!vhlo.bool_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> !vhlo.tensor_v1<!vhlo.i32_v1>
      "vhlo.return_v1"(%262, %266) : (!vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> ()
    }) : (!vhlo.tensor_v1<8x32x!vhlo.i32_v1>, !vhlo.tensor_v1<8x32x!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>, !vhlo.tensor_v1<!vhlo.i32_v1>) -> (!vhlo.tensor_v1<8x!vhlo.i32_v1>, !vhlo.tensor_v1<8x!vhlo.i32_v1>)
    %254 = "vhlo.convert_v1"(%253#1) : (!vhlo.tensor_v1<8x!vhlo.i32_v1>) -> !vhlo.tensor_v1<8x!vhlo.i64_v1>
    %255 = "vhlo.compare_v1"(%254, %14) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 LT>}> : (!vhlo.tensor_v1<8x!vhlo.i64_v1>, !vhlo.tensor_v1<8x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x!vhlo.bool_v1>
    %256 = "vhlo.add_v1"(%254, %13) : (!vhlo.tensor_v1<8x!vhlo.i64_v1>, !vhlo.tensor_v1<8x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x!vhlo.i64_v1>
    %257 = "vhlo.select_v1"(%255, %256, %254) : (!vhlo.tensor_v1<8x!vhlo.bool_v1>, !vhlo.tensor_v1<8x!vhlo.i64_v1>, !vhlo.tensor_v1<8x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x!vhlo.i64_v1>
    %258 = "vhlo.reshape_v1"(%257) : (!vhlo.tensor_v1<8x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x1x!vhlo.i64_v1>
    %259 = "vhlo.concatenate_v1"(%15, %258) <{dimension = #vhlo.integer_v1<1 : i64>}> : (!vhlo.tensor_v1<8x1x!vhlo.i64_v1>, !vhlo.tensor_v1<8x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x2x!vhlo.i64_v1>
    %260 = "vhlo.gather_v2"(%246, %259) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 1, 2]> : tensor<3xi64>>, start_index_map = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<8x32x2x!vhlo.bf16_v1>, !vhlo.tensor_v1<8x2x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x2x!vhlo.bf16_v1>
    "vhlo.return_v1"(%260) : (!vhlo.tensor_v1<8x2x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump After VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @SyncTensorsGraph.630) //----- //
module @SyncTensorsGraph.630 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg1: tensor<2x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg2: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg3: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg4: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg5: tensor<768x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg6: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg7: tensor<3072x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg8: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg9: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg10: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg11: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg12: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg13: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg14: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg15: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg16: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}, %arg17: tensor<2050x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg18: tensor<50272x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg19: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg20: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}, %arg21: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"}, %arg22: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"}) -> tensor<8x2xbf16> {
    %c = stablehlo.constant dense<0> : tensor<i64>
    %c_0 = stablehlo.constant dense<0> : tensor<i32>
    %c_1 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64>
    %cst = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c_3 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi32>
    %c_4 = stablehlo.constant dense<-2147483648> : tensor<i32>
    %c_5 = stablehlo.constant dense<2> : tensor<i64>
    %cst_6 = stablehlo.constant dense<1.250000e-01> : tensor<bf16>
    %c_7 = stablehlo.constant dense<1> : tensor<i64>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %cst_9 = stablehlo.constant dense<1.304630e-03> : tensor<bf16>
    %cst_10 = stablehlo.constant dense<1.001360e-05> : tensor<bf16>
    %c_11 = stablehlo.constant dense<32> : tensor<8xi64>
    %c_12 = stablehlo.constant dense<0> : tensor<8xi64>
    %c_13 = stablehlo.constant dense<[[0], [1], [2], [3], [4], [5], [6], [7]]> : tensor<8x1xi64>
    %cst_14 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<256x3072xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<256x1xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<256xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %4 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %5 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %7 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<32x32xi64>
    %8 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<8x32x768xbf16>
    %9 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x32x1xbf16>
    %10 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<8x32xbf16>
    %11 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<8x32xi64>
    %12 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<8x32xi64>
    %13 = stablehlo.reshape %arg18 : (tensor<50272x768xbf16>) -> tensor<1x50272x768xbf16>
    %14 = stablehlo.custom_call @tt.mark_argument(%13) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_embed_tokens_weight"}} : (tensor<1x50272x768xbf16>) -> tensor<1x50272x768xbf16>
    %15 = stablehlo.reshape %14 : (tensor<1x50272x768xbf16>) -> tensor<50272x768xbf16>
    %16 = stablehlo.reshape %arg0 : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %17 = stablehlo.custom_call @tt.mark_argument(%16) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<1x8x32xi64>) -> tensor<1x8x32xi64>
    %18 = stablehlo.reshape %17 : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %19 = stablehlo.reshape %17 : (tensor<1x8x32xi64>) -> tensor<256xi64>
    %20 = stablehlo.convert %19 : (tensor<256xi64>) -> tensor<256xui32>
    %21 = "stablehlo.gather"(%15, %20) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<50272x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %22 = stablehlo.reshape %21 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %23 = stablehlo.reshape %arg17 : (tensor<2050x768xbf16>) -> tensor<1x2050x768xbf16>
    %24 = stablehlo.custom_call @tt.mark_argument(%23) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_embed_positions_weight"}} : (tensor<1x2050x768xbf16>) -> tensor<1x2050x768xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x2050x768xbf16>) -> tensor<2050x768xbf16>
    %26 = stablehlo.reshape %arg16 : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %27 = stablehlo.custom_call @tt.mark_argument(%26) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_1"}} : (tensor<1x8x32xi64>) -> tensor<1x8x32xi64>
    %28 = stablehlo.reshape %27 : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %29 = "stablehlo.reduce_window"(%28, %c) <{padding = dense<[[0, 0], [31, 0]]> : tensor<2x2xi64>, window_dimensions = array<i64: 1, 32>}> ({
    ^bb0(%arg23: tensor<i64>, %arg24: tensor<i64>):
      %244 = stablehlo.add %arg23, %arg24 : tensor<i64>
      stablehlo.return %244 : tensor<i64>
    }) : (tensor<8x32xi64>, tensor<i64>) -> tensor<8x32xi64>
    %30 = stablehlo.multiply %29, %28 : tensor<8x32xi64>
    %31 = stablehlo.subtract %30, %12 : tensor<8x32xi64>
    %32 = stablehlo.add %31, %11 : tensor<8x32xi64>
    %33 = stablehlo.reshape %32 : (tensor<8x32xi64>) -> tensor<256xi64>
    %34 = stablehlo.convert %33 : (tensor<256xi64>) -> tensor<256xui32>
    %35 = "stablehlo.gather"(%25, %34) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<2050x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %36 = stablehlo.reshape %35 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %37 = stablehlo.add %22, %36 : tensor<8x32x768xbf16>
    %38 = stablehlo.reduce(%37 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %39 = stablehlo.multiply %38, %10 : tensor<8x32xbf16>
    %40 = stablehlo.broadcast_in_dim %39, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %41 = stablehlo.subtract %37, %40 : tensor<8x32x768xbf16>
    %42 = stablehlo.multiply %41, %41 : tensor<8x32x768xbf16>
    %43 = stablehlo.reduce(%42 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %44 = stablehlo.multiply %43, %10 : tensor<8x32xbf16>
    %45 = stablehlo.reshape %44 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %46 = stablehlo.add %45, %9 : tensor<8x32x1xbf16>
    %47 = stablehlo.rsqrt %46 : tensor<8x32x1xbf16>
    %48 = stablehlo.reshape %47 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %49 = stablehlo.broadcast_in_dim %48, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %50 = stablehlo.multiply %41, %49 : tensor<8x32x768xbf16>
    %51 = stablehlo.reshape %arg15 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %52 = stablehlo.custom_call @tt.mark_argument(%51) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16>
    %53 = stablehlo.reshape %52 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %54 = stablehlo.broadcast_in_dim %53, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %55 = stablehlo.multiply %50, %54 : tensor<8x32x768xbf16>
    %56 = stablehlo.reshape %arg14 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %57 = stablehlo.custom_call @tt.mark_argument(%56) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16>
    %58 = stablehlo.reshape %57 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %59 = stablehlo.broadcast_in_dim %58, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %60 = stablehlo.add %55, %59 : tensor<8x32x768xbf16>
    %61 = stablehlo.reshape %60 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %62 = stablehlo.reshape %arg22 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %63 = stablehlo.custom_call @tt.mark_argument(%62) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16>
    %64 = stablehlo.reshape %63 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %65 = stablehlo.transpose %64, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %66 = stablehlo.dot_general %61, %65, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %67 = stablehlo.reshape %66 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %68 = stablehlo.reshape %arg21 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %69 = stablehlo.custom_call @tt.mark_argument(%68) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16>
    %70 = stablehlo.reshape %69 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %71 = stablehlo.broadcast_in_dim %70, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %72 = stablehlo.add %67, %71 : tensor<8x32x768xbf16>
    %73 = stablehlo.multiply %72, %8 : tensor<8x32x768xbf16>
    %74 = stablehlo.reshape %73 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %75 = stablehlo.transpose %74, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %76 = stablehlo.reshape %75 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %77 = stablehlo.reshape %arg20 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %78 = stablehlo.custom_call @tt.mark_argument(%77) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16>
    %79 = stablehlo.reshape %78 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %80 = stablehlo.transpose %79, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %81 = stablehlo.dot_general %61, %80, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %82 = stablehlo.reshape %81 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %83 = stablehlo.reshape %arg19 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %84 = stablehlo.custom_call @tt.mark_argument(%83) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16>
    %85 = stablehlo.reshape %84 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %86 = stablehlo.broadcast_in_dim %85, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %87 = stablehlo.add %82, %86 : tensor<8x32x768xbf16>
    %88 = stablehlo.reshape %87 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %89 = stablehlo.transpose %88, dims = [0, 2, 3, 1] : (tensor<8x32x12x64xbf16>) -> tensor<8x12x64x32xbf16>
    %90 = stablehlo.reshape %89 : (tensor<8x12x64x32xbf16>) -> tensor<96x64x32xbf16>
    %91 = stablehlo.dot_general %76, %90, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x64xbf16>, tensor<96x64x32xbf16>) -> tensor<96x32x32xbf16>
    %92 = stablehlo.reshape %91 : (tensor<96x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %93 = stablehlo.broadcast_in_dim %c_1, dims = [1] : (tensor<32xi64>) -> tensor<32x32xi64>
    %94 = stablehlo.broadcast_in_dim %c_1, dims = [0] : (tensor<32xi64>) -> tensor<32x32xi64>
    %95 = stablehlo.subtract %93, %94 : tensor<32x32xi64>
    %96 = stablehlo.compare  GE, %95, %7 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %97 = stablehlo.select %96, %6, %5 : tensor<32x32xi1>, tensor<32x32xbf16>
    %98 = stablehlo.compare  GT, %93, %94 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %99 = stablehlo.convert %98 : (tensor<32x32xi1>) -> tensor<32x32xbf16>
    %100 = stablehlo.multiply %97, %99 : tensor<32x32xbf16>
    %101 = stablehlo.reshape %100 : (tensor<32x32xbf16>) -> tensor<1x32x32xbf16>
    %102 = stablehlo.broadcast_in_dim %101, dims = [1, 2, 3] : (tensor<1x32x32xbf16>) -> tensor<8x1x32x32xbf16>
    %103 = stablehlo.reshape %27 : (tensor<1x8x32xi64>) -> tensor<8x1x1x32xi64>
    %104 = stablehlo.convert %103 : (tensor<8x1x1x32xi64>) -> tensor<8x1x1x32xbf16>
    %105 = stablehlo.reshape %104 : (tensor<8x1x1x32xbf16>) -> tensor<8x1x32xbf16>
    %106 = stablehlo.broadcast_in_dim %105, dims = [0, 1, 3] : (tensor<8x1x32xbf16>) -> tensor<8x1x32x32xbf16>
    %107 = stablehlo.add %102, %106 : tensor<8x1x32x32xbf16>
    %108 = stablehlo.compare  EQ, %107, %4 : (tensor<8x1x32x32xbf16>, tensor<8x1x32x32xbf16>) -> tensor<8x1x32x32xi1>
    %109 = stablehlo.select %108, %3, %102 : tensor<8x1x32x32xi1>, tensor<8x1x32x32xbf16>
    %110 = stablehlo.reshape %109 : (tensor<8x1x32x32xbf16>) -> tensor<8x32x32xbf16>
    %111 = stablehlo.broadcast_in_dim %110, dims = [0, 2, 3] : (tensor<8x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %112 = stablehlo.add %92, %111 : tensor<8x12x32x32xbf16>
    %113 = stablehlo.convert %112 : (tensor<8x12x32x32xbf16>) -> tensor<8x12x32x32xf32>
    %114 = stablehlo.reduce(%113 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %115 = stablehlo.broadcast_in_dim %114, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %116 = stablehlo.subtract %113, %115 : tensor<8x12x32x32xf32>
    %117 = stablehlo.exponential %116 : tensor<8x12x32x32xf32>
    %118 = stablehlo.reduce(%117 init: %cst_2) applies stablehlo.add across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %120 = stablehlo.divide %117, %119 : tensor<8x12x32x32xf32>
    %121 = stablehlo.convert %120 : (tensor<8x12x32x32xf32>) -> tensor<8x12x32x32xbf16>
    %122 = stablehlo.reshape %121 : (tensor<8x12x32x32xbf16>) -> tensor<96x32x32xbf16>
    %123 = stablehlo.reshape %arg13 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %124 = stablehlo.custom_call @tt.mark_argument(%123) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16>
    %125 = stablehlo.reshape %124 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %126 = stablehlo.transpose %125, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %127 = stablehlo.dot_general %61, %126, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %128 = stablehlo.reshape %127 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %129 = stablehlo.reshape %arg12 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %130 = stablehlo.custom_call @tt.mark_argument(%129) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16>
    %131 = stablehlo.reshape %130 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %132 = stablehlo.broadcast_in_dim %131, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %133 = stablehlo.add %128, %132 : tensor<8x32x768xbf16>
    %134 = stablehlo.reshape %133 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %135 = stablehlo.transpose %134, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %136 = stablehlo.reshape %135 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %137 = stablehlo.dot_general %122, %136, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x32xbf16>, tensor<96x32x64xbf16>) -> tensor<96x32x64xbf16>
    %138 = stablehlo.reshape %137 : (tensor<96x32x64xbf16>) -> tensor<8x12x32x64xbf16>
    %139 = stablehlo.transpose %138, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,32,12,64]{3,1,2,0}"} : (tensor<8x12x32x64xbf16>) -> tensor<8x32x12x64xbf16>
    %140 = stablehlo.reshape %139 : (tensor<8x32x12x64xbf16>) -> tensor<256x768xbf16>
    %141 = stablehlo.reshape %arg11 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %142 = stablehlo.custom_call @tt.mark_argument(%141) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16>
    %143 = stablehlo.reshape %142 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %144 = stablehlo.transpose %143, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %145 = stablehlo.dot_general %140, %144, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %146 = stablehlo.reshape %145 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %147 = stablehlo.reshape %arg10 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %148 = stablehlo.custom_call @tt.mark_argument(%147) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16>
    %149 = stablehlo.reshape %148 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %150 = stablehlo.broadcast_in_dim %149, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %151 = stablehlo.add %146, %150 : tensor<8x32x768xbf16>
    %152 = stablehlo.add %37, %151 : tensor<8x32x768xbf16>
    %153 = stablehlo.reshape %152 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %154 = stablehlo.reduce(%153 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %155 = stablehlo.multiply %154, %2 : tensor<256xbf16>
    %156 = stablehlo.broadcast_in_dim %155, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %157 = stablehlo.subtract %153, %156 : tensor<256x768xbf16>
    %158 = stablehlo.multiply %157, %157 : tensor<256x768xbf16>
    %159 = stablehlo.reduce(%158 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %160 = stablehlo.multiply %159, %2 : tensor<256xbf16>
    %161 = stablehlo.reshape %160 : (tensor<256xbf16>) -> tensor<256x1xbf16>
    %162 = stablehlo.add %161, %1 : tensor<256x1xbf16>
    %163 = stablehlo.rsqrt %162 : tensor<256x1xbf16>
    %164 = stablehlo.reshape %163 : (tensor<256x1xbf16>) -> tensor<256xbf16>
    %165 = stablehlo.broadcast_in_dim %164, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %166 = stablehlo.multiply %157, %165 : tensor<256x768xbf16>
    %167 = stablehlo.reshape %arg9 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %168 = stablehlo.custom_call @tt.mark_argument(%167) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16>
    %169 = stablehlo.reshape %168 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %170 = stablehlo.broadcast_in_dim %169, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %171 = stablehlo.multiply %166, %170 : tensor<256x768xbf16>
    %172 = stablehlo.reshape %arg8 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %173 = stablehlo.custom_call @tt.mark_argument(%172) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16>
    %174 = stablehlo.reshape %173 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %175 = stablehlo.broadcast_in_dim %174, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %176 = stablehlo.add %171, %175 : tensor<256x768xbf16>
    %177 = stablehlo.reshape %arg7 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16>
    %178 = stablehlo.custom_call @tt.mark_argument(%177) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_fc1_weight"}} : (tensor<1x3072x768xbf16>) -> tensor<1x3072x768xbf16>
    %179 = stablehlo.reshape %178 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16>
    %180 = stablehlo.transpose %179, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16>
    %181 = stablehlo.dot_general %176, %180, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x3072xbf16>) -> tensor<256x3072xbf16>
    %182 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %183 = stablehlo.custom_call @tt.mark_argument(%182) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_fc1_bias"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %184 = stablehlo.reshape %183 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %185 = stablehlo.broadcast_in_dim %184, dims = [1] : (tensor<3072xbf16>) -> tensor<256x3072xbf16>
    %186 = stablehlo.add %181, %185 : tensor<256x3072xbf16>
    %187 = stablehlo.maximum %186, %0 : tensor<256x3072xbf16>
    %188 = stablehlo.reshape %arg5 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16>
    %189 = stablehlo.custom_call @tt.mark_argument(%188) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_fc2_weight"}} : (tensor<1x768x3072xbf16>) -> tensor<1x768x3072xbf16>
    %190 = stablehlo.reshape %189 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16>
    %191 = stablehlo.transpose %190, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16>
    %192 = stablehlo.dot_general %187, %191, contracting_dims = [1] x [0] : (tensor<256x3072xbf16>, tensor<3072x768xbf16>) -> tensor<256x768xbf16>
    %193 = stablehlo.reshape %arg4 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %194 = stablehlo.custom_call @tt.mark_argument(%193) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_fc2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16>
    %195 = stablehlo.reshape %194 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %196 = stablehlo.broadcast_in_dim %195, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %197 = stablehlo.add %192, %196 : tensor<256x768xbf16>
    %198 = stablehlo.add %153, %197 : tensor<256x768xbf16>
    %199 = stablehlo.reshape %198 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %200 = stablehlo.reduce(%199 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %201 = stablehlo.multiply %200, %10 : tensor<8x32xbf16>
    %202 = stablehlo.broadcast_in_dim %201, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %203 = stablehlo.subtract %199, %202 : tensor<8x32x768xbf16>
    %204 = stablehlo.multiply %203, %203 : tensor<8x32x768xbf16>
    %205 = stablehlo.reduce(%204 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %206 = stablehlo.multiply %205, %10 : tensor<8x32xbf16>
    %207 = stablehlo.reshape %206 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %208 = stablehlo.add %207, %9 : tensor<8x32x1xbf16>
    %209 = stablehlo.rsqrt %208 : tensor<8x32x1xbf16>
    %210 = stablehlo.reshape %209 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %211 = stablehlo.broadcast_in_dim %210, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %212 = stablehlo.multiply %203, %211 : tensor<8x32x768xbf16>
    %213 = stablehlo.reshape %arg3 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %214 = stablehlo.custom_call @tt.mark_argument(%213) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_final_layer_norm_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16>
    %215 = stablehlo.reshape %214 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %216 = stablehlo.broadcast_in_dim %215, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %217 = stablehlo.multiply %212, %216 : tensor<8x32x768xbf16>
    %218 = stablehlo.reshape %arg2 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %219 = stablehlo.custom_call @tt.mark_argument(%218) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_final_layer_norm_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16>
    %220 = stablehlo.reshape %219 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %221 = stablehlo.broadcast_in_dim %220, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %222 = stablehlo.add %217, %221 : tensor<8x32x768xbf16>
    %223 = stablehlo.reshape %222 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %224 = stablehlo.reshape %arg1 : (tensor<2x768xbf16>) -> tensor<1x2x768xbf16>
    %225 = stablehlo.custom_call @tt.mark_argument(%224) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___score_weight"}} : (tensor<1x2x768xbf16>) -> tensor<1x2x768xbf16>
    %226 = stablehlo.reshape %225 : (tensor<1x2x768xbf16>) -> tensor<2x768xbf16>
    %227 = stablehlo.transpose %226, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,2]{0,1}"} : (tensor<2x768xbf16>) -> tensor<768x2xbf16>
    %228 = stablehlo.dot_general %223, %227, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x2xbf16>) -> tensor<256x2xbf16>
    %229 = stablehlo.reshape %228 : (tensor<256x2xbf16>) -> tensor<8x32x2xbf16>
    %230 = stablehlo.broadcast_in_dim %c_3, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32>
    %231 = stablehlo.compare  NE, %18, %12 : (tensor<8x32xi64>, tensor<8x32xi64>) -> tensor<8x32xi1>
    %232 = stablehlo.convert %231 : (tensor<8x32xi1>) -> tensor<8x32xi32>
    %233 = stablehlo.multiply %230, %232 : tensor<8x32xi32>
    %234 = stablehlo.iota dim = 0 : tensor<32xi32>
    %235 = stablehlo.broadcast_in_dim %234, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32>
    %236:2 = stablehlo.reduce(%233 init: %c_4), (%235 init: %c_0) across dimensions = [1] : (tensor<8x32xi32>, tensor<8x32xi32>, tensor<i32>, tensor<i32>) -> (tensor<8xi32>, tensor<8xi32>)
     reducer(%arg23: tensor<i32>, %arg25: tensor<i32>) (%arg24: tensor<i32>, %arg26: tensor<i32>)  {
      %244 = stablehlo.compare  GE, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %245 = stablehlo.select %244, %arg23, %arg25 : tensor<i1>, tensor<i32>
      %246 = stablehlo.compare  EQ, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %247 = stablehlo.minimum %arg24, %arg26 : tensor<i32>
      %248 = stablehlo.select %244, %arg24, %arg26 : tensor<i1>, tensor<i32>
      %249 = stablehlo.select %246, %247, %248 : tensor<i1>, tensor<i32>
      stablehlo.return %245, %249 : tensor<i32>, tensor<i32>
    }
    %237 = stablehlo.convert %236#1 : (tensor<8xi32>) -> tensor<8xi64>
    %238 = stablehlo.compare  LT, %237, %c_12 : (tensor<8xi64>, tensor<8xi64>) -> tensor<8xi1>
    %239 = stablehlo.add %237, %c_11 : tensor<8xi64>
    %240 = stablehlo.select %238, %239, %237 : tensor<8xi1>, tensor<8xi64>
    %241 = stablehlo.reshape %240 : (tensor<8xi64>) -> tensor<8x1xi64>
    %242 = stablehlo.concatenate %c_13, %241, dim = 1 : (tensor<8x1xi64>, tensor<8x1xi64>) -> tensor<8x2xi64>
    %243 = "stablehlo.gather"(%229, %242) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1, 2>}> : (tensor<8x32x2xbf16>, tensor<8x2xi64>) -> tensor<8x2xbf16>
    return %243 : tensor<8x2xbf16>
  }
}


2025-10-28 19:04:57.801 (  17.425s) [        2AFB8480]      module_builder.cc:963      1| MLIR Module shlo:
#loc1 = loc("p0.2")
#loc2 = loc("p1.66")
#loc3 = loc("p2.72")
#loc4 = loc("p3.76")
#loc5 = loc("p4.85")
#loc6 = loc("p5.92")
#loc7 = loc("p6.99")
#loc8 = loc("p7.106")
#loc9 = loc("p8.112")
#loc10 = loc("p9.116")
#loc11 = loc("p10.124")
#loc12 = loc("p11.128")
#loc13 = loc("p12.134")
#loc14 = loc("p13.138")
#loc15 = loc("p14.144")
#loc16 = loc("p15.148")
#loc17 = loc("p16.159")
#loc18 = loc("p17.179")
#loc19 = loc("p18.187")
#loc20 = loc("p19.364")
#loc21 = loc("p20.368")
#loc22 = loc("p21.387")
#loc23 = loc("p22.391")
#loc40 = loc("reduce-window.168")
#loc247 = loc("reduce.39")
module @SyncTensorsGraph.630 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"} loc("p0.2"), %arg1: tensor<2x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"} loc("p1.66"), %arg2: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("p2.72"), %arg3: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("p3.76"), %arg4: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("p4.85"), %arg5: tensor<768x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"} loc("p5.92"), %arg6: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("p6.99"), %arg7: tensor<3072x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"} loc("p7.106"), %arg8: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("p8.112"), %arg9: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("p9.116"), %arg10: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("p10.124"), %arg11: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"} loc("p11.128"), %arg12: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("p12.134"), %arg13: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"} loc("p13.138"), %arg14: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("p14.144"), %arg15: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("p15.148"), %arg16: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"} loc("p16.159"), %arg17: tensor<2050x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"} loc("p17.179"), %arg18: tensor<50272x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"} loc("p18.187"), %arg19: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("p19.364"), %arg20: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"} loc("p20.368"), %arg21: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}"} loc("p21.387"), %arg22: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}"} loc("p22.391")) -> tensor<8x2xbf16> {
    %c = stablehlo.constant dense<0> : tensor<i64> loc(#loc)
    %c_0 = stablehlo.constant dense<0> : tensor<i32> loc(#loc)
    %c_1 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64> loc(#loc)
    %cst = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %c_3 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi32> loc(#loc)
    %c_4 = stablehlo.constant dense<-2147483648> : tensor<i32> loc(#loc)
    %c_5 = stablehlo.constant dense<2> : tensor<i64> loc(#loc)
    %cst_6 = stablehlo.constant dense<1.250000e-01> : tensor<bf16> loc(#loc)
    %c_7 = stablehlo.constant dense<1> : tensor<i64> loc(#loc)
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16> loc(#loc)
    %cst_9 = stablehlo.constant dense<1.304630e-03> : tensor<bf16> loc(#loc)
    %cst_10 = stablehlo.constant dense<1.001360e-05> : tensor<bf16> loc(#loc)
    %c_11 = stablehlo.constant dense<32> : tensor<8xi64> loc(#loc)
    %c_12 = stablehlo.constant dense<0> : tensor<8xi64> loc(#loc)
    %c_13 = stablehlo.constant dense<[[0], [1], [2], [3], [4], [5], [6], [7]]> : tensor<8x1xi64> loc(#loc)
    %cst_14 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<256x3072xbf16> loc(#loc)
    %1 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<256x1xbf16> loc(#loc)
    %2 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<256xbf16> loc(#loc)
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16> loc(#loc)
    %4 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16> loc(#loc)
    %5 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16> loc(#loc)
    %6 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16> loc(#loc)
    %7 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<32x32xi64> loc(#loc)
    %8 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<8x32x768xbf16> loc(#loc)
    %9 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x32x1xbf16> loc(#loc)
    %10 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<8x32xbf16> loc(#loc)
    %11 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<8x32xi64> loc(#loc)
    %12 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<8x32xi64> loc(#loc)
    %13 = stablehlo.reshape %arg18 : (tensor<50272x768xbf16>) -> tensor<1x50272x768xbf16> loc(#loc24)
    %14 = stablehlo.custom_call @tt.mark_argument(%13) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_embed_tokens_weight"}} : (tensor<1x50272x768xbf16>) -> tensor<1x50272x768xbf16> loc(#loc25)
    %15 = stablehlo.reshape %14 : (tensor<1x50272x768xbf16>) -> tensor<50272x768xbf16> loc(#loc26)
    %16 = stablehlo.reshape %arg0 : (tensor<8x32xi64>) -> tensor<1x8x32xi64> loc(#loc27)
    %17 = stablehlo.custom_call @tt.mark_argument(%16) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_0"}} : (tensor<1x8x32xi64>) -> tensor<1x8x32xi64> loc(#loc28)
    %18 = stablehlo.reshape %17 : (tensor<1x8x32xi64>) -> tensor<8x32xi64> loc(#loc29)
    %19 = stablehlo.reshape %17 : (tensor<1x8x32xi64>) -> tensor<256xi64> loc(#loc30)
    %20 = stablehlo.convert %19 : (tensor<256xi64>) -> tensor<256xui32> loc(#loc31)
    %21 = "stablehlo.gather"(%15, %20) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<50272x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16> loc(#loc32)
    %22 = stablehlo.reshape %21 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16> loc(#loc33)
    %23 = stablehlo.reshape %arg17 : (tensor<2050x768xbf16>) -> tensor<1x2050x768xbf16> loc(#loc34)
    %24 = stablehlo.custom_call @tt.mark_argument(%23) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_embed_positions_weight"}} : (tensor<1x2050x768xbf16>) -> tensor<1x2050x768xbf16> loc(#loc35)
    %25 = stablehlo.reshape %24 : (tensor<1x2050x768xbf16>) -> tensor<2050x768xbf16> loc(#loc36)
    %26 = stablehlo.reshape %arg16 : (tensor<8x32xi64>) -> tensor<1x8x32xi64> loc(#loc37)
    %27 = stablehlo.custom_call @tt.mark_argument(%26) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "input", ttir.name = "args_1"}} : (tensor<1x8x32xi64>) -> tensor<1x8x32xi64> loc(#loc38)
    %28 = stablehlo.reshape %27 : (tensor<1x8x32xi64>) -> tensor<8x32xi64> loc(#loc39)
    %29 = "stablehlo.reduce_window"(%28, %c) <{padding = dense<[[0, 0], [31, 0]]> : tensor<2x2xi64>, window_dimensions = array<i64: 1, 32>}> ({
    ^bb0(%arg23: tensor<i64> loc("reduce-window.168"), %arg24: tensor<i64> loc("reduce-window.168")):
      %244 = stablehlo.add %arg23, %arg24 : tensor<i64> loc(#loc41)
      stablehlo.return %244 : tensor<i64> loc(#loc)
    }) : (tensor<8x32xi64>, tensor<i64>) -> tensor<8x32xi64> loc(#loc40)
    %30 = stablehlo.multiply %29, %28 : tensor<8x32xi64> loc(#loc42)
    %31 = stablehlo.subtract %30, %12 : tensor<8x32xi64> loc(#loc43)
    %32 = stablehlo.add %31, %11 : tensor<8x32xi64> loc(#loc44)
    %33 = stablehlo.reshape %32 : (tensor<8x32xi64>) -> tensor<256xi64> loc(#loc45)
    %34 = stablehlo.convert %33 : (tensor<256xi64>) -> tensor<256xui32> loc(#loc46)
    %35 = "stablehlo.gather"(%25, %34) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<2050x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16> loc(#loc47)
    %36 = stablehlo.reshape %35 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16> loc(#loc48)
    %37 = stablehlo.add %22, %36 : tensor<8x32x768xbf16> loc(#loc49)
    %38 = stablehlo.reduce(%37 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16> loc(#loc50)
    %39 = stablehlo.multiply %38, %10 : tensor<8x32xbf16> loc(#loc51)
    %40 = stablehlo.broadcast_in_dim %39, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16> loc(#loc52)
    %41 = stablehlo.subtract %37, %40 : tensor<8x32x768xbf16> loc(#loc53)
    %42 = stablehlo.multiply %41, %41 : tensor<8x32x768xbf16> loc(#loc54)
    %43 = stablehlo.reduce(%42 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16> loc(#loc55)
    %44 = stablehlo.multiply %43, %10 : tensor<8x32xbf16> loc(#loc56)
    %45 = stablehlo.reshape %44 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16> loc(#loc57)
    %46 = stablehlo.add %45, %9 : tensor<8x32x1xbf16> loc(#loc58)
    %47 = stablehlo.rsqrt %46 : tensor<8x32x1xbf16> loc(#loc59)
    %48 = stablehlo.reshape %47 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16> loc(#loc60)
    %49 = stablehlo.broadcast_in_dim %48, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16> loc(#loc61)
    %50 = stablehlo.multiply %41, %49 : tensor<8x32x768xbf16> loc(#loc62)
    %51 = stablehlo.reshape %arg15 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc63)
    %52 = stablehlo.custom_call @tt.mark_argument(%51) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc64)
    %53 = stablehlo.reshape %52 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc65)
    %54 = stablehlo.broadcast_in_dim %53, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16> loc(#loc66)
    %55 = stablehlo.multiply %50, %54 : tensor<8x32x768xbf16> loc(#loc67)
    %56 = stablehlo.reshape %arg14 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc68)
    %57 = stablehlo.custom_call @tt.mark_argument(%56) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc69)
    %58 = stablehlo.reshape %57 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc70)
    %59 = stablehlo.broadcast_in_dim %58, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16> loc(#loc71)
    %60 = stablehlo.add %55, %59 : tensor<8x32x768xbf16> loc(#loc72)
    %61 = stablehlo.reshape %60 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16> loc(#loc73)
    %62 = stablehlo.reshape %arg22 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc74)
    %63 = stablehlo.custom_call @tt.mark_argument(%62) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc75)
    %64 = stablehlo.reshape %63 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc76)
    %65 = stablehlo.transpose %64, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc77)
    %66 = stablehlo.dot_general %61, %65, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16> loc(#loc78)
    %67 = stablehlo.reshape %66 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16> loc(#loc79)
    %68 = stablehlo.reshape %arg21 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc80)
    %69 = stablehlo.custom_call @tt.mark_argument(%68) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc81)
    %70 = stablehlo.reshape %69 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc82)
    %71 = stablehlo.broadcast_in_dim %70, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16> loc(#loc83)
    %72 = stablehlo.add %67, %71 : tensor<8x32x768xbf16> loc(#loc84)
    %73 = stablehlo.multiply %72, %8 : tensor<8x32x768xbf16> loc(#loc85)
    %74 = stablehlo.reshape %73 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16> loc(#loc86)
    %75 = stablehlo.transpose %74, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16> loc(#loc87)
    %76 = stablehlo.reshape %75 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16> loc(#loc88)
    %77 = stablehlo.reshape %arg20 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc89)
    %78 = stablehlo.custom_call @tt.mark_argument(%77) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc90)
    %79 = stablehlo.reshape %78 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc91)
    %80 = stablehlo.transpose %79, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc92)
    %81 = stablehlo.dot_general %61, %80, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16> loc(#loc93)
    %82 = stablehlo.reshape %81 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16> loc(#loc94)
    %83 = stablehlo.reshape %arg19 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc95)
    %84 = stablehlo.custom_call @tt.mark_argument(%83) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc96)
    %85 = stablehlo.reshape %84 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc97)
    %86 = stablehlo.broadcast_in_dim %85, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16> loc(#loc98)
    %87 = stablehlo.add %82, %86 : tensor<8x32x768xbf16> loc(#loc99)
    %88 = stablehlo.reshape %87 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16> loc(#loc100)
    %89 = stablehlo.transpose %88, dims = [0, 2, 3, 1] : (tensor<8x32x12x64xbf16>) -> tensor<8x12x64x32xbf16> loc(#loc101)
    %90 = stablehlo.reshape %89 : (tensor<8x12x64x32xbf16>) -> tensor<96x64x32xbf16> loc(#loc102)
    %91 = stablehlo.dot_general %76, %90, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x64xbf16>, tensor<96x64x32xbf16>) -> tensor<96x32x32xbf16> loc(#loc103)
    %92 = stablehlo.reshape %91 : (tensor<96x32x32xbf16>) -> tensor<8x12x32x32xbf16> loc(#loc104)
    %93 = stablehlo.broadcast_in_dim %c_1, dims = [1] : (tensor<32xi64>) -> tensor<32x32xi64> loc(#loc105)
    %94 = stablehlo.broadcast_in_dim %c_1, dims = [0] : (tensor<32xi64>) -> tensor<32x32xi64> loc(#loc106)
    %95 = stablehlo.subtract %93, %94 : tensor<32x32xi64> loc(#loc107)
    %96 = stablehlo.compare  GE, %95, %7 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1> loc(#loc108)
    %97 = stablehlo.select %96, %6, %5 : tensor<32x32xi1>, tensor<32x32xbf16> loc(#loc109)
    %98 = stablehlo.compare  GT, %93, %94 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1> loc(#loc110)
    %99 = stablehlo.convert %98 : (tensor<32x32xi1>) -> tensor<32x32xbf16> loc(#loc111)
    %100 = stablehlo.multiply %97, %99 : tensor<32x32xbf16> loc(#loc112)
    %101 = stablehlo.reshape %100 : (tensor<32x32xbf16>) -> tensor<1x32x32xbf16> loc(#loc113)
    %102 = stablehlo.broadcast_in_dim %101, dims = [1, 2, 3] : (tensor<1x32x32xbf16>) -> tensor<8x1x32x32xbf16> loc(#loc114)
    %103 = stablehlo.reshape %27 : (tensor<1x8x32xi64>) -> tensor<8x1x1x32xi64> loc(#loc115)
    %104 = stablehlo.convert %103 : (tensor<8x1x1x32xi64>) -> tensor<8x1x1x32xbf16> loc(#loc116)
    %105 = stablehlo.reshape %104 : (tensor<8x1x1x32xbf16>) -> tensor<8x1x32xbf16> loc(#loc117)
    %106 = stablehlo.broadcast_in_dim %105, dims = [0, 1, 3] : (tensor<8x1x32xbf16>) -> tensor<8x1x32x32xbf16> loc(#loc118)
    %107 = stablehlo.add %102, %106 : tensor<8x1x32x32xbf16> loc(#loc119)
    %108 = stablehlo.compare  EQ, %107, %4 : (tensor<8x1x32x32xbf16>, tensor<8x1x32x32xbf16>) -> tensor<8x1x32x32xi1> loc(#loc120)
    %109 = stablehlo.select %108, %3, %102 : tensor<8x1x32x32xi1>, tensor<8x1x32x32xbf16> loc(#loc121)
    %110 = stablehlo.reshape %109 : (tensor<8x1x32x32xbf16>) -> tensor<8x32x32xbf16> loc(#loc122)
    %111 = stablehlo.broadcast_in_dim %110, dims = [0, 2, 3] : (tensor<8x32x32xbf16>) -> tensor<8x12x32x32xbf16> loc(#loc123)
    %112 = stablehlo.add %92, %111 : tensor<8x12x32x32xbf16> loc(#loc124)
    %113 = stablehlo.convert %112 : (tensor<8x12x32x32xbf16>) -> tensor<8x12x32x32xf32> loc(#loc125)
    %114 = stablehlo.reduce(%113 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32> loc(#loc126)
    %115 = stablehlo.broadcast_in_dim %114, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32> loc(#loc127)
    %116 = stablehlo.subtract %113, %115 : tensor<8x12x32x32xf32> loc(#loc128)
    %117 = stablehlo.exponential %116 : tensor<8x12x32x32xf32> loc(#loc129)
    %118 = stablehlo.reduce(%117 init: %cst_2) applies stablehlo.add across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32> loc(#loc130)
    %119 = stablehlo.broadcast_in_dim %118, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32> loc(#loc131)
    %120 = stablehlo.divide %117, %119 : tensor<8x12x32x32xf32> loc(#loc132)
    %121 = stablehlo.convert %120 : (tensor<8x12x32x32xf32>) -> tensor<8x12x32x32xbf16> loc(#loc133)
    %122 = stablehlo.reshape %121 : (tensor<8x12x32x32xbf16>) -> tensor<96x32x32xbf16> loc(#loc134)
    %123 = stablehlo.reshape %arg13 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc135)
    %124 = stablehlo.custom_call @tt.mark_argument(%123) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc136)
    %125 = stablehlo.reshape %124 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc137)
    %126 = stablehlo.transpose %125, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc138)
    %127 = stablehlo.dot_general %61, %126, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16> loc(#loc139)
    %128 = stablehlo.reshape %127 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16> loc(#loc140)
    %129 = stablehlo.reshape %arg12 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc141)
    %130 = stablehlo.custom_call @tt.mark_argument(%129) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc142)
    %131 = stablehlo.reshape %130 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc143)
    %132 = stablehlo.broadcast_in_dim %131, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16> loc(#loc144)
    %133 = stablehlo.add %128, %132 : tensor<8x32x768xbf16> loc(#loc145)
    %134 = stablehlo.reshape %133 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16> loc(#loc146)
    %135 = stablehlo.transpose %134, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16> loc(#loc147)
    %136 = stablehlo.reshape %135 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16> loc(#loc148)
    %137 = stablehlo.dot_general %122, %136, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x32xbf16>, tensor<96x32x64xbf16>) -> tensor<96x32x64xbf16> loc(#loc149)
    %138 = stablehlo.reshape %137 : (tensor<96x32x64xbf16>) -> tensor<8x12x32x64xbf16> loc(#loc150)
    %139 = stablehlo.transpose %138, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,32,12,64]{3,1,2,0}"} : (tensor<8x12x32x64xbf16>) -> tensor<8x32x12x64xbf16> loc(#loc151)
    %140 = stablehlo.reshape %139 : (tensor<8x32x12x64xbf16>) -> tensor<256x768xbf16> loc(#loc152)
    %141 = stablehlo.reshape %arg11 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc153)
    %142 = stablehlo.custom_call @tt.mark_argument(%141) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_weight"}} : (tensor<1x768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc154)
    %143 = stablehlo.reshape %142 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc155)
    %144 = stablehlo.transpose %143, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc156)
    %145 = stablehlo.dot_general %140, %144, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16> loc(#loc157)
    %146 = stablehlo.reshape %145 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16> loc(#loc158)
    %147 = stablehlo.reshape %arg10 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc159)
    %148 = stablehlo.custom_call @tt.mark_argument(%147) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc160)
    %149 = stablehlo.reshape %148 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc161)
    %150 = stablehlo.broadcast_in_dim %149, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16> loc(#loc162)
    %151 = stablehlo.add %146, %150 : tensor<8x32x768xbf16> loc(#loc163)
    %152 = stablehlo.add %37, %151 : tensor<8x32x768xbf16> loc(#loc164)
    %153 = stablehlo.reshape %152 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16> loc(#loc165)
    %154 = stablehlo.reduce(%153 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16> loc(#loc166)
    %155 = stablehlo.multiply %154, %2 : tensor<256xbf16> loc(#loc167)
    %156 = stablehlo.broadcast_in_dim %155, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16> loc(#loc168)
    %157 = stablehlo.subtract %153, %156 : tensor<256x768xbf16> loc(#loc169)
    %158 = stablehlo.multiply %157, %157 : tensor<256x768xbf16> loc(#loc170)
    %159 = stablehlo.reduce(%158 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16> loc(#loc171)
    %160 = stablehlo.multiply %159, %2 : tensor<256xbf16> loc(#loc172)
    %161 = stablehlo.reshape %160 : (tensor<256xbf16>) -> tensor<256x1xbf16> loc(#loc173)
    %162 = stablehlo.add %161, %1 : tensor<256x1xbf16> loc(#loc174)
    %163 = stablehlo.rsqrt %162 : tensor<256x1xbf16> loc(#loc175)
    %164 = stablehlo.reshape %163 : (tensor<256x1xbf16>) -> tensor<256xbf16> loc(#loc176)
    %165 = stablehlo.broadcast_in_dim %164, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16> loc(#loc177)
    %166 = stablehlo.multiply %157, %165 : tensor<256x768xbf16> loc(#loc178)
    %167 = stablehlo.reshape %arg9 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc179)
    %168 = stablehlo.custom_call @tt.mark_argument(%167) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc180)
    %169 = stablehlo.reshape %168 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc181)
    %170 = stablehlo.broadcast_in_dim %169, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16> loc(#loc182)
    %171 = stablehlo.multiply %166, %170 : tensor<256x768xbf16> loc(#loc183)
    %172 = stablehlo.reshape %arg8 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc184)
    %173 = stablehlo.custom_call @tt.mark_argument(%172) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc185)
    %174 = stablehlo.reshape %173 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc186)
    %175 = stablehlo.broadcast_in_dim %174, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16> loc(#loc187)
    %176 = stablehlo.add %171, %175 : tensor<256x768xbf16> loc(#loc188)
    %177 = stablehlo.reshape %arg7 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc189)
    %178 = stablehlo.custom_call @tt.mark_argument(%177) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_fc1_weight"}} : (tensor<1x3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc190)
    %179 = stablehlo.reshape %178 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc191)
    %180 = stablehlo.transpose %179, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc192)
    %181 = stablehlo.dot_general %176, %180, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x3072xbf16>) -> tensor<256x3072xbf16> loc(#loc193)
    %182 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc194)
    %183 = stablehlo.custom_call @tt.mark_argument(%182) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_fc1_bias"}} : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc195)
    %184 = stablehlo.reshape %183 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc196)
    %185 = stablehlo.broadcast_in_dim %184, dims = [1] : (tensor<3072xbf16>) -> tensor<256x3072xbf16> loc(#loc197)
    %186 = stablehlo.add %181, %185 : tensor<256x3072xbf16> loc(#loc198)
    %187 = stablehlo.maximum %186, %0 : tensor<256x3072xbf16> loc(#loc199)
    %188 = stablehlo.reshape %arg5 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc200)
    %189 = stablehlo.custom_call @tt.mark_argument(%188) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_fc2_weight"}} : (tensor<1x768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc201)
    %190 = stablehlo.reshape %189 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc202)
    %191 = stablehlo.transpose %190, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc203)
    %192 = stablehlo.dot_general %187, %191, contracting_dims = [1] x [0] : (tensor<256x3072xbf16>, tensor<3072x768xbf16>) -> tensor<256x768xbf16> loc(#loc204)
    %193 = stablehlo.reshape %arg4 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc205)
    %194 = stablehlo.custom_call @tt.mark_argument(%193) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_layers_0_fc2_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc206)
    %195 = stablehlo.reshape %194 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc207)
    %196 = stablehlo.broadcast_in_dim %195, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16> loc(#loc208)
    %197 = stablehlo.add %192, %196 : tensor<256x768xbf16> loc(#loc209)
    %198 = stablehlo.add %153, %197 : tensor<256x768xbf16> loc(#loc210)
    %199 = stablehlo.reshape %198 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16> loc(#loc211)
    %200 = stablehlo.reduce(%199 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16> loc(#loc212)
    %201 = stablehlo.multiply %200, %10 : tensor<8x32xbf16> loc(#loc213)
    %202 = stablehlo.broadcast_in_dim %201, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16> loc(#loc214)
    %203 = stablehlo.subtract %199, %202 : tensor<8x32x768xbf16> loc(#loc215)
    %204 = stablehlo.multiply %203, %203 : tensor<8x32x768xbf16> loc(#loc216)
    %205 = stablehlo.reduce(%204 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16> loc(#loc217)
    %206 = stablehlo.multiply %205, %10 : tensor<8x32xbf16> loc(#loc218)
    %207 = stablehlo.reshape %206 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16> loc(#loc219)
    %208 = stablehlo.add %207, %9 : tensor<8x32x1xbf16> loc(#loc220)
    %209 = stablehlo.rsqrt %208 : tensor<8x32x1xbf16> loc(#loc221)
    %210 = stablehlo.reshape %209 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16> loc(#loc222)
    %211 = stablehlo.broadcast_in_dim %210, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16> loc(#loc223)
    %212 = stablehlo.multiply %203, %211 : tensor<8x32x768xbf16> loc(#loc224)
    %213 = stablehlo.reshape %arg3 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc225)
    %214 = stablehlo.custom_call @tt.mark_argument(%213) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_final_layer_norm_weight"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc226)
    %215 = stablehlo.reshape %214 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc227)
    %216 = stablehlo.broadcast_in_dim %215, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16> loc(#loc228)
    %217 = stablehlo.multiply %212, %216 : tensor<8x32x768xbf16> loc(#loc229)
    %218 = stablehlo.reshape %arg2 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc230)
    %219 = stablehlo.custom_call @tt.mark_argument(%218) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___model_decoder_final_layer_norm_bias"}} : (tensor<1x1x768xbf16>) -> tensor<1x1x768xbf16> loc(#loc231)
    %220 = stablehlo.reshape %219 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc232)
    %221 = stablehlo.broadcast_in_dim %220, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16> loc(#loc233)
    %222 = stablehlo.add %217, %221 : tensor<8x32x768xbf16> loc(#loc234)
    %223 = stablehlo.reshape %222 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16> loc(#loc235)
    %224 = stablehlo.reshape %arg1 : (tensor<2x768xbf16>) -> tensor<1x2x768xbf16> loc(#loc236)
    %225 = stablehlo.custom_call @tt.mark_argument(%224) {api_version = 0 : i32, mhlo.frontend_attributes = {ttcore.argument_type = "parameter", ttir.name = "l__self___score_weight"}} : (tensor<1x2x768xbf16>) -> tensor<1x2x768xbf16> loc(#loc237)
    %226 = stablehlo.reshape %225 : (tensor<1x2x768xbf16>) -> tensor<2x768xbf16> loc(#loc238)
    %227 = stablehlo.transpose %226, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,2]{0,1}"} : (tensor<2x768xbf16>) -> tensor<768x2xbf16> loc(#loc239)
    %228 = stablehlo.dot_general %223, %227, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x2xbf16>) -> tensor<256x2xbf16> loc(#loc240)
    %229 = stablehlo.reshape %228 : (tensor<256x2xbf16>) -> tensor<8x32x2xbf16> loc(#loc241)
    %230 = stablehlo.broadcast_in_dim %c_3, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32> loc(#loc242)
    %231 = stablehlo.compare  NE, %18, %12 : (tensor<8x32xi64>, tensor<8x32xi64>) -> tensor<8x32xi1> loc(#loc243)
    %232 = stablehlo.convert %231 : (tensor<8x32xi1>) -> tensor<8x32xi32> loc(#loc244)
    %233 = stablehlo.multiply %230, %232 : tensor<8x32xi32> loc(#loc245)
    %234 = stablehlo.iota dim = 0 : tensor<32xi32> loc(#loc246)
    %235 = stablehlo.broadcast_in_dim %234, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32> loc(#loc246)
    %236:2 = stablehlo.reduce(%233 init: %c_4), (%235 init: %c_0) across dimensions = [1] : (tensor<8x32xi32>, tensor<8x32xi32>, tensor<i32>, tensor<i32>) -> (tensor<8xi32>, tensor<8xi32>)
     reducer(%arg23: tensor<i32> loc("reduce.39"), %arg25: tensor<i32> loc("reduce.39")) (%arg24: tensor<i32> loc("reduce.39"), %arg26: tensor<i32> loc("reduce.39"))  {
      %244 = stablehlo.compare  GE, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc248)
      %245 = stablehlo.select %244, %arg23, %arg25 : tensor<i1>, tensor<i32> loc(#loc249)
      %246 = stablehlo.compare  EQ, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc250)
      %247 = stablehlo.minimum %arg24, %arg26 : tensor<i32> loc(#loc251)
      %248 = stablehlo.select %244, %arg24, %arg26 : tensor<i1>, tensor<i32> loc(#loc252)
      %249 = stablehlo.select %246, %247, %248 : tensor<i1>, tensor<i32> loc(#loc253)
      stablehlo.return %245, %249 : tensor<i32>, tensor<i32> loc(#loc)
    } loc(#loc247)
    %237 = stablehlo.convert %236#1 : (tensor<8xi32>) -> tensor<8xi64> loc(#loc254)
    %238 = stablehlo.compare  LT, %237, %c_12 : (tensor<8xi64>, tensor<8xi64>) -> tensor<8xi1> loc(#loc255)
    %239 = stablehlo.add %237, %c_11 : tensor<8xi64> loc(#loc256)
    %240 = stablehlo.select %238, %239, %237 : tensor<8xi1>, tensor<8xi64> loc(#loc257)
    %241 = stablehlo.reshape %240 : (tensor<8xi64>) -> tensor<8x1xi64> loc(#loc258)
    %242 = stablehlo.concatenate %c_13, %241, dim = 1 : (tensor<8x1xi64>, tensor<8x1xi64>) -> tensor<8x2xi64> loc(#loc259)
    %243 = "stablehlo.gather"(%229, %242) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1, 2>}> : (tensor<8x32x2xbf16>, tensor<8x2xi64>) -> tensor<8x2xbf16> loc(#loc260)
    return %243 : tensor<8x2xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc24 = loc("reshape.188")
#loc25 = loc("custom-call.189")
#loc26 = loc("reshape.190")
#loc27 = loc("reshape.3")
#loc28 = loc("custom-call.4")
#loc29 = loc("reshape.5")
#loc30 = loc("reshape.186")
#loc31 = loc("convert.191")
#loc32 = loc("gather.192")
#loc33 = loc("reshape.193")
#loc34 = loc("reshape.180")
#loc35 = loc("custom-call.181")
#loc36 = loc("reshape.182")
#loc37 = loc("reshape.160")
#loc38 = loc("custom-call.161")
#loc39 = loc("reshape.162")
#loc41 = loc("add.167")
#loc42 = loc("multiply.169")
#loc43 = loc("subtract.172")
#loc44 = loc("add.177")
#loc45 = loc("reshape.178")
#loc46 = loc("convert.183")
#loc47 = loc("gather.184")
#loc48 = loc("reshape.185")
#loc49 = loc("add.196")
#loc50 = loc("reduce.242")
#loc51 = loc("multiply.251")
#loc52 = loc("broadcast.261")
#loc53 = loc("subtract.262")
#loc54 = loc("multiply.218")
#loc55 = loc("reduce.225")
#loc56 = loc("multiply.234")
#loc57 = loc("reshape.235")
#loc58 = loc("add.255")
#loc59 = loc("rsqrt.256")
#loc60 = loc("reshape.263")
#loc61 = loc("broadcast.264")
#loc62 = loc("multiply.265")
#loc63 = loc("reshape.149")
#loc64 = loc("custom-call.150")
#loc65 = loc("reshape.151")
#loc66 = loc("broadcast.266")
#loc67 = loc("multiply.267")
#loc68 = loc("reshape.145")
#loc69 = loc("custom-call.146")
#loc70 = loc("reshape.147")
#loc71 = loc("broadcast.270")
#loc72 = loc("add.271")
#loc73 = loc("reshape.396")
#loc74 = loc("reshape.392")
#loc75 = loc("custom-call.393")
#loc76 = loc("reshape.394")
#loc77 = loc("transpose.395")
#loc78 = loc("dot.397")
#loc79 = loc("reshape.398")
#loc80 = loc("reshape.388")
#loc81 = loc("custom-call.389")
#loc82 = loc("reshape.390")
#loc83 = loc("broadcast.401")
#loc84 = loc("add.402")
#loc85 = loc("multiply.404")
#loc86 = loc("reshape.405")
#loc87 = loc("transpose.406")
#loc88 = loc("reshape.408")
#loc89 = loc("reshape.369")
#loc90 = loc("custom-call.370")
#loc91 = loc("reshape.371")
#loc92 = loc("transpose.372")
#loc93 = loc("dot.374")
#loc94 = loc("reshape.375")
#loc95 = loc("reshape.365")
#loc96 = loc("custom-call.366")
#loc97 = loc("reshape.367")
#loc98 = loc("broadcast.378")
#loc99 = loc("add.379")
#loc100 = loc("reshape.380")
#loc101 = loc("transpose.382")
#loc102 = loc("reshape.384")
#loc103 = loc("dot.409")
#loc104 = loc("reshape.410")
#loc105 = loc("broadcast.313")
#loc106 = loc("broadcast.315")
#loc107 = loc("subtract.316")
#loc108 = loc("compare.318")
#loc109 = loc("select.320")
#loc110 = loc("compare.290")
#loc111 = loc("convert.291")
#loc112 = loc("multiply.321")
#loc113 = loc("reshape.322")
#loc114 = loc("broadcast.328")
#loc115 = loc("reshape.343")
#loc116 = loc("convert.348")
#loc117 = loc("reshape.351")
#loc118 = loc("broadcast.352")
#loc119 = loc("add.353")
#loc120 = loc("compare.356")
#loc121 = loc("select.358")
#loc122 = loc("reshape.415")
#loc123 = loc("broadcast.416")
#loc124 = loc("add.417")
#loc125 = loc("convert.418")
#loc126 = loc("reduce.424")
#loc127 = loc("broadcast.425")
#loc128 = loc("subtract.426")
#loc129 = loc("exponential.427")
#loc130 = loc("reduce.433")
#loc131 = loc("broadcast.434")
#loc132 = loc("divide.435")
#loc133 = loc("convert.436")
#loc134 = loc("reshape.438")
#loc135 = loc("reshape.139")
#loc136 = loc("custom-call.140")
#loc137 = loc("reshape.141")
#loc138 = loc("transpose.142")
#loc139 = loc("dot.273")
#loc140 = loc("reshape.274")
#loc141 = loc("reshape.135")
#loc142 = loc("custom-call.136")
#loc143 = loc("reshape.137")
#loc144 = loc("broadcast.277")
#loc145 = loc("add.278")
#loc146 = loc("reshape.279")
#loc147 = loc("transpose.280")
#loc148 = loc("reshape.282")
#loc149 = loc("dot.439")
#loc150 = loc("reshape.440")
#loc151 = loc("transpose.441")
#loc152 = loc("reshape.443")
#loc153 = loc("reshape.129")
#loc154 = loc("custom-call.130")
#loc155 = loc("reshape.131")
#loc156 = loc("transpose.132")
#loc157 = loc("dot.444")
#loc158 = loc("reshape.445")
#loc159 = loc("reshape.125")
#loc160 = loc("custom-call.126")
#loc161 = loc("reshape.127")
#loc162 = loc("broadcast.448")
#loc163 = loc("add.449")
#loc164 = loc("add.452")
#loc165 = loc("reshape.453")
#loc166 = loc("reduce.499")
#loc167 = loc("multiply.508")
#loc168 = loc("broadcast.518")
#loc169 = loc("subtract.519")
#loc170 = loc("multiply.475")
#loc171 = loc("reduce.482")
#loc172 = loc("multiply.491")
#loc173 = loc("reshape.492")
#loc174 = loc("add.512")
#loc175 = loc("rsqrt.513")
#loc176 = loc("reshape.520")
#loc177 = loc("broadcast.521")
#loc178 = loc("multiply.522")
#loc179 = loc("reshape.117")
#loc180 = loc("custom-call.118")
#loc181 = loc("reshape.119")
#loc182 = loc("broadcast.523")
#loc183 = loc("multiply.524")
#loc184 = loc("reshape.113")
#loc185 = loc("custom-call.114")
#loc186 = loc("reshape.115")
#loc187 = loc("broadcast.527")
#loc188 = loc("add.528")
#loc189 = loc("reshape.107")
#loc190 = loc("custom-call.108")
#loc191 = loc("reshape.109")
#loc192 = loc("transpose.110")
#loc193 = loc("dot.529")
#loc194 = loc("reshape.100")
#loc195 = loc("custom-call.101")
#loc196 = loc("reshape.102")
#loc197 = loc("broadcast.534")
#loc198 = loc("add.535")
#loc199 = loc("maximum.538")
#loc200 = loc("reshape.93")
#loc201 = loc("custom-call.94")
#loc202 = loc("reshape.95")
#loc203 = loc("transpose.96")
#loc204 = loc("dot.539")
#loc205 = loc("reshape.86")
#loc206 = loc("custom-call.87")
#loc207 = loc("reshape.88")
#loc208 = loc("broadcast.544")
#loc209 = loc("add.545")
#loc210 = loc("add.548")
#loc211 = loc("reshape.549")
#loc212 = loc("reduce.595")
#loc213 = loc("multiply.604")
#loc214 = loc("broadcast.614")
#loc215 = loc("subtract.615")
#loc216 = loc("multiply.571")
#loc217 = loc("reduce.578")
#loc218 = loc("multiply.587")
#loc219 = loc("reshape.588")
#loc220 = loc("add.608")
#loc221 = loc("rsqrt.609")
#loc222 = loc("reshape.616")
#loc223 = loc("broadcast.617")
#loc224 = loc("multiply.618")
#loc225 = loc("reshape.77")
#loc226 = loc("custom-call.78")
#loc227 = loc("reshape.79")
#loc228 = loc("broadcast.619")
#loc229 = loc("multiply.620")
#loc230 = loc("reshape.73")
#loc231 = loc("custom-call.74")
#loc232 = loc("reshape.75")
#loc233 = loc("broadcast.623")
#loc234 = loc("add.624")
#loc235 = loc("reshape.625")
#loc236 = loc("reshape.67")
#loc237 = loc("custom-call.68")
#loc238 = loc("reshape.69")
#loc239 = loc("transpose.70")
#loc240 = loc("dot.626")
#loc241 = loc("reshape.627")
#loc242 = loc("broadcast.10")
#loc243 = loc("compare.7")
#loc244 = loc("convert.8")
#loc245 = loc("multiply.11")
#loc246 = loc("iota.14")
#loc248 = loc("compare.32")
#loc249 = loc("select.33")
#loc250 = loc("compare.35")
#loc251 = loc("minimum.36")
#loc252 = loc("select.34")
#loc253 = loc("select.37")
#loc254 = loc("convert.41")
#loc255 = loc("compare.50")
#loc256 = loc("add.47")
#loc257 = loc("select.51")
#loc258 = loc("reshape.64")
#loc259 = loc("concatenate.65")
#loc260 = loc("gather.628")
------------------ END OF MLIR MODULE ------------------
2025-10-28 19:04:57.823 (  17.447s) [        2AFB8480]      module_builder.cc:963      1| MLIR Module shlo_frontend:
#loc1 = loc("p0.2")
#loc2 = loc("p1.66")
#loc3 = loc("p2.72")
#loc4 = loc("p3.76")
#loc5 = loc("p4.85")
#loc6 = loc("p5.92")
#loc7 = loc("p6.99")
#loc8 = loc("p7.106")
#loc9 = loc("p8.112")
#loc10 = loc("p9.116")
#loc11 = loc("p10.124")
#loc12 = loc("p11.128")
#loc13 = loc("p12.134")
#loc14 = loc("p13.138")
#loc15 = loc("p14.144")
#loc16 = loc("p15.148")
#loc17 = loc("p16.159")
#loc18 = loc("p17.179")
#loc19 = loc("p18.187")
#loc20 = loc("p19.364")
#loc21 = loc("p20.368")
#loc22 = loc("p21.387")
#loc23 = loc("p22.391")
#loc36 = loc("reduce-window.168")
#loc224 = loc("reduce.39")
module @SyncTensorsGraph.630 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"} loc("p0.2"), %arg1: tensor<2x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___score_weight"} loc("p1.66"), %arg2: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_final_layer_norm_bias"} loc("p2.72"), %arg3: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_final_layer_norm_weight"} loc("p3.76"), %arg4: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_fc2_bias"} loc("p4.85"), %arg5: tensor<768x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_fc2_weight"} loc("p5.92"), %arg6: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_fc1_bias"} loc("p6.99"), %arg7: tensor<3072x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_fc1_weight"} loc("p7.106"), %arg8: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_bias"} loc("p8.112"), %arg9: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_weight"} loc("p9.116"), %arg10: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_bias"} loc("p10.124"), %arg11: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_weight"} loc("p11.128"), %arg12: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_bias"} loc("p12.134"), %arg13: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_weight"} loc("p13.138"), %arg14: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_bias"} loc("p14.144"), %arg15: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_weight"} loc("p15.148"), %arg16: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_1"} loc("p16.159"), %arg17: tensor<2050x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_embed_positions_weight"} loc("p17.179"), %arg18: tensor<50272x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_embed_tokens_weight"} loc("p18.187"), %arg19: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_bias"} loc("p19.364"), %arg20: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_weight"} loc("p20.368"), %arg21: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_bias"} loc("p21.387"), %arg22: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_weight"} loc("p22.391")) -> tensor<8x2xbf16> {
    %c = stablehlo.constant dense<0> : tensor<i64> loc(#loc)
    %c_0 = stablehlo.constant dense<0> : tensor<i32> loc(#loc)
    %c_1 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64> loc(#loc)
    %cst = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc)
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc)
    %c_3 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi32> loc(#loc)
    %c_4 = stablehlo.constant dense<-2147483648> : tensor<i32> loc(#loc)
    %c_5 = stablehlo.constant dense<2> : tensor<i64> loc(#loc)
    %cst_6 = stablehlo.constant dense<1.250000e-01> : tensor<bf16> loc(#loc)
    %c_7 = stablehlo.constant dense<1> : tensor<i64> loc(#loc)
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16> loc(#loc)
    %cst_9 = stablehlo.constant dense<1.304630e-03> : tensor<bf16> loc(#loc)
    %cst_10 = stablehlo.constant dense<1.001360e-05> : tensor<bf16> loc(#loc)
    %c_11 = stablehlo.constant dense<32> : tensor<8xi64> loc(#loc)
    %c_12 = stablehlo.constant dense<0> : tensor<8xi64> loc(#loc)
    %c_13 = stablehlo.constant dense<[[0], [1], [2], [3], [4], [5], [6], [7]]> : tensor<8x1xi64> loc(#loc)
    %cst_14 = stablehlo.constant dense<0.000000e+00> : tensor<bf16> loc(#loc)
    %0 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<256x3072xbf16> loc(#loc)
    %1 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<256x1xbf16> loc(#loc)
    %2 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<256xbf16> loc(#loc)
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16> loc(#loc)
    %4 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16> loc(#loc)
    %5 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16> loc(#loc)
    %6 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16> loc(#loc)
    %7 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<32x32xi64> loc(#loc)
    %8 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<8x32x768xbf16> loc(#loc)
    %9 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x32x1xbf16> loc(#loc)
    %10 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<8x32xbf16> loc(#loc)
    %11 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<8x32xi64> loc(#loc)
    %12 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<8x32xi64> loc(#loc)
    %13 = stablehlo.reshape %arg18 : (tensor<50272x768xbf16>) -> tensor<1x50272x768xbf16> loc(#loc24)
    %14 = stablehlo.reshape %13 : (tensor<1x50272x768xbf16>) -> tensor<50272x768xbf16> loc(#loc25)
    %15 = stablehlo.reshape %arg0 : (tensor<8x32xi64>) -> tensor<1x8x32xi64> loc(#loc26)
    %16 = stablehlo.reshape %15 : (tensor<1x8x32xi64>) -> tensor<8x32xi64> loc(#loc27)
    %17 = stablehlo.reshape %15 : (tensor<1x8x32xi64>) -> tensor<256xi64> loc(#loc28)
    %18 = stablehlo.convert %17 : (tensor<256xi64>) -> tensor<256xui32> loc(#loc29)
    %19 = "stablehlo.gather"(%14, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<50272x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16> loc(#loc30)
    %20 = stablehlo.reshape %19 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16> loc(#loc31)
    %21 = stablehlo.reshape %arg17 : (tensor<2050x768xbf16>) -> tensor<1x2050x768xbf16> loc(#loc32)
    %22 = stablehlo.reshape %21 : (tensor<1x2050x768xbf16>) -> tensor<2050x768xbf16> loc(#loc33)
    %23 = stablehlo.reshape %arg16 : (tensor<8x32xi64>) -> tensor<1x8x32xi64> loc(#loc34)
    %24 = stablehlo.reshape %23 : (tensor<1x8x32xi64>) -> tensor<8x32xi64> loc(#loc35)
    %25 = "stablehlo.reduce_window"(%24, %c) <{padding = dense<[[0, 0], [31, 0]]> : tensor<2x2xi64>, window_dimensions = array<i64: 1, 32>}> ({
    ^bb0(%arg23: tensor<i64> loc("reduce-window.168"), %arg24: tensor<i64> loc("reduce-window.168")):
      %221 = stablehlo.add %arg23, %arg24 : tensor<i64> loc(#loc37)
      stablehlo.return %221 : tensor<i64> loc(#loc)
    }) : (tensor<8x32xi64>, tensor<i64>) -> tensor<8x32xi64> loc(#loc36)
    %26 = stablehlo.multiply %25, %24 : tensor<8x32xi64> loc(#loc38)
    %27 = stablehlo.subtract %26, %12 : tensor<8x32xi64> loc(#loc39)
    %28 = stablehlo.add %27, %11 : tensor<8x32xi64> loc(#loc40)
    %29 = stablehlo.reshape %28 : (tensor<8x32xi64>) -> tensor<256xi64> loc(#loc41)
    %30 = stablehlo.convert %29 : (tensor<256xi64>) -> tensor<256xui32> loc(#loc42)
    %31 = "stablehlo.gather"(%22, %30) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<2050x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16> loc(#loc43)
    %32 = stablehlo.reshape %31 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16> loc(#loc44)
    %33 = stablehlo.add %20, %32 : tensor<8x32x768xbf16> loc(#loc45)
    %34 = stablehlo.reduce(%33 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16> loc(#loc46)
    %35 = stablehlo.multiply %34, %10 : tensor<8x32xbf16> loc(#loc47)
    %36 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16> loc(#loc48)
    %37 = stablehlo.subtract %33, %36 : tensor<8x32x768xbf16> loc(#loc49)
    %38 = stablehlo.multiply %37, %37 : tensor<8x32x768xbf16> loc(#loc50)
    %39 = stablehlo.reduce(%38 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16> loc(#loc51)
    %40 = stablehlo.multiply %39, %10 : tensor<8x32xbf16> loc(#loc52)
    %41 = stablehlo.reshape %40 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16> loc(#loc53)
    %42 = stablehlo.add %41, %9 : tensor<8x32x1xbf16> loc(#loc54)
    %43 = stablehlo.rsqrt %42 : tensor<8x32x1xbf16> loc(#loc55)
    %44 = stablehlo.reshape %43 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16> loc(#loc56)
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16> loc(#loc57)
    %46 = stablehlo.multiply %37, %45 : tensor<8x32x768xbf16> loc(#loc58)
    %47 = stablehlo.reshape %arg15 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc59)
    %48 = stablehlo.reshape %47 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc60)
    %49 = stablehlo.broadcast_in_dim %48, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16> loc(#loc61)
    %50 = stablehlo.multiply %46, %49 : tensor<8x32x768xbf16> loc(#loc62)
    %51 = stablehlo.reshape %arg14 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc63)
    %52 = stablehlo.reshape %51 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc64)
    %53 = stablehlo.broadcast_in_dim %52, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16> loc(#loc65)
    %54 = stablehlo.add %50, %53 : tensor<8x32x768xbf16> loc(#loc66)
    %55 = stablehlo.reshape %54 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16> loc(#loc67)
    %56 = stablehlo.reshape %arg22 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc68)
    %57 = stablehlo.reshape %56 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc69)
    %58 = stablehlo.transpose %57, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc70)
    %59 = stablehlo.dot_general %55, %58, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16> loc(#loc71)
    %60 = stablehlo.reshape %59 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16> loc(#loc72)
    %61 = stablehlo.reshape %arg21 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc73)
    %62 = stablehlo.reshape %61 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc74)
    %63 = stablehlo.broadcast_in_dim %62, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16> loc(#loc75)
    %64 = stablehlo.add %60, %63 : tensor<8x32x768xbf16> loc(#loc76)
    %65 = stablehlo.multiply %64, %8 : tensor<8x32x768xbf16> loc(#loc77)
    %66 = stablehlo.reshape %65 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16> loc(#loc78)
    %67 = stablehlo.transpose %66, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16> loc(#loc79)
    %68 = stablehlo.reshape %67 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16> loc(#loc80)
    %69 = stablehlo.reshape %arg20 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc81)
    %70 = stablehlo.reshape %69 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc82)
    %71 = stablehlo.transpose %70, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc83)
    %72 = stablehlo.dot_general %55, %71, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16> loc(#loc84)
    %73 = stablehlo.reshape %72 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16> loc(#loc85)
    %74 = stablehlo.reshape %arg19 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc86)
    %75 = stablehlo.reshape %74 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc87)
    %76 = stablehlo.broadcast_in_dim %75, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16> loc(#loc88)
    %77 = stablehlo.add %73, %76 : tensor<8x32x768xbf16> loc(#loc89)
    %78 = stablehlo.reshape %77 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16> loc(#loc90)
    %79 = stablehlo.transpose %78, dims = [0, 2, 3, 1] : (tensor<8x32x12x64xbf16>) -> tensor<8x12x64x32xbf16> loc(#loc91)
    %80 = stablehlo.reshape %79 : (tensor<8x12x64x32xbf16>) -> tensor<96x64x32xbf16> loc(#loc92)
    %81 = stablehlo.dot_general %68, %80, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x64xbf16>, tensor<96x64x32xbf16>) -> tensor<96x32x32xbf16> loc(#loc93)
    %82 = stablehlo.reshape %81 : (tensor<96x32x32xbf16>) -> tensor<8x12x32x32xbf16> loc(#loc94)
    %83 = stablehlo.broadcast_in_dim %c_1, dims = [1] : (tensor<32xi64>) -> tensor<32x32xi64> loc(#loc95)
    %84 = stablehlo.broadcast_in_dim %c_1, dims = [0] : (tensor<32xi64>) -> tensor<32x32xi64> loc(#loc96)
    %85 = stablehlo.subtract %83, %84 : tensor<32x32xi64> loc(#loc97)
    %86 = stablehlo.compare  GE, %85, %7 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1> loc(#loc98)
    %87 = stablehlo.select %86, %6, %5 : tensor<32x32xi1>, tensor<32x32xbf16> loc(#loc99)
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1> loc(#loc100)
    %89 = stablehlo.convert %88 : (tensor<32x32xi1>) -> tensor<32x32xbf16> loc(#loc101)
    %90 = stablehlo.multiply %87, %89 : tensor<32x32xbf16> loc(#loc102)
    %91 = stablehlo.reshape %90 : (tensor<32x32xbf16>) -> tensor<1x32x32xbf16> loc(#loc103)
    %92 = stablehlo.broadcast_in_dim %91, dims = [1, 2, 3] : (tensor<1x32x32xbf16>) -> tensor<8x1x32x32xbf16> loc(#loc104)
    %93 = stablehlo.reshape %23 : (tensor<1x8x32xi64>) -> tensor<8x1x1x32xi64> loc(#loc105)
    %94 = stablehlo.convert %93 : (tensor<8x1x1x32xi64>) -> tensor<8x1x1x32xbf16> loc(#loc106)
    %95 = stablehlo.reshape %94 : (tensor<8x1x1x32xbf16>) -> tensor<8x1x32xbf16> loc(#loc107)
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] : (tensor<8x1x32xbf16>) -> tensor<8x1x32x32xbf16> loc(#loc108)
    %97 = stablehlo.add %92, %96 : tensor<8x1x32x32xbf16> loc(#loc109)
    %98 = stablehlo.compare  EQ, %97, %4 : (tensor<8x1x32x32xbf16>, tensor<8x1x32x32xbf16>) -> tensor<8x1x32x32xi1> loc(#loc110)
    %99 = stablehlo.select %98, %3, %92 : tensor<8x1x32x32xi1>, tensor<8x1x32x32xbf16> loc(#loc111)
    %100 = stablehlo.reshape %99 : (tensor<8x1x32x32xbf16>) -> tensor<8x32x32xbf16> loc(#loc112)
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] : (tensor<8x32x32xbf16>) -> tensor<8x12x32x32xbf16> loc(#loc113)
    %102 = stablehlo.add %82, %101 : tensor<8x12x32x32xbf16> loc(#loc114)
    %103 = stablehlo.convert %102 : (tensor<8x12x32x32xbf16>) -> tensor<8x12x32x32xf32> loc(#loc115)
    %104 = stablehlo.reduce(%103 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32> loc(#loc116)
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32> loc(#loc117)
    %106 = stablehlo.subtract %103, %105 : tensor<8x12x32x32xf32> loc(#loc118)
    %107 = stablehlo.exponential %106 : tensor<8x12x32x32xf32> loc(#loc119)
    %108 = stablehlo.reduce(%107 init: %cst_2) applies stablehlo.add across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32> loc(#loc120)
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32> loc(#loc121)
    %110 = stablehlo.divide %107, %109 : tensor<8x12x32x32xf32> loc(#loc122)
    %111 = stablehlo.convert %110 : (tensor<8x12x32x32xf32>) -> tensor<8x12x32x32xbf16> loc(#loc123)
    %112 = stablehlo.reshape %111 : (tensor<8x12x32x32xbf16>) -> tensor<96x32x32xbf16> loc(#loc124)
    %113 = stablehlo.reshape %arg13 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc125)
    %114 = stablehlo.reshape %113 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc126)
    %115 = stablehlo.transpose %114, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc127)
    %116 = stablehlo.dot_general %55, %115, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16> loc(#loc128)
    %117 = stablehlo.reshape %116 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16> loc(#loc129)
    %118 = stablehlo.reshape %arg12 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc130)
    %119 = stablehlo.reshape %118 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc131)
    %120 = stablehlo.broadcast_in_dim %119, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16> loc(#loc132)
    %121 = stablehlo.add %117, %120 : tensor<8x32x768xbf16> loc(#loc133)
    %122 = stablehlo.reshape %121 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16> loc(#loc134)
    %123 = stablehlo.transpose %122, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16> loc(#loc135)
    %124 = stablehlo.reshape %123 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16> loc(#loc136)
    %125 = stablehlo.dot_general %112, %124, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x32xbf16>, tensor<96x32x64xbf16>) -> tensor<96x32x64xbf16> loc(#loc137)
    %126 = stablehlo.reshape %125 : (tensor<96x32x64xbf16>) -> tensor<8x12x32x64xbf16> loc(#loc138)
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,32,12,64]{3,1,2,0}"} : (tensor<8x12x32x64xbf16>) -> tensor<8x32x12x64xbf16> loc(#loc139)
    %128 = stablehlo.reshape %127 : (tensor<8x32x12x64xbf16>) -> tensor<256x768xbf16> loc(#loc140)
    %129 = stablehlo.reshape %arg11 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16> loc(#loc141)
    %130 = stablehlo.reshape %129 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16> loc(#loc142)
    %131 = stablehlo.transpose %130, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16> loc(#loc143)
    %132 = stablehlo.dot_general %128, %131, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16> loc(#loc144)
    %133 = stablehlo.reshape %132 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16> loc(#loc145)
    %134 = stablehlo.reshape %arg10 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc146)
    %135 = stablehlo.reshape %134 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc147)
    %136 = stablehlo.broadcast_in_dim %135, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16> loc(#loc148)
    %137 = stablehlo.add %133, %136 : tensor<8x32x768xbf16> loc(#loc149)
    %138 = stablehlo.add %33, %137 : tensor<8x32x768xbf16> loc(#loc150)
    %139 = stablehlo.reshape %138 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16> loc(#loc151)
    %140 = stablehlo.reduce(%139 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16> loc(#loc152)
    %141 = stablehlo.multiply %140, %2 : tensor<256xbf16> loc(#loc153)
    %142 = stablehlo.broadcast_in_dim %141, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16> loc(#loc154)
    %143 = stablehlo.subtract %139, %142 : tensor<256x768xbf16> loc(#loc155)
    %144 = stablehlo.multiply %143, %143 : tensor<256x768xbf16> loc(#loc156)
    %145 = stablehlo.reduce(%144 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16> loc(#loc157)
    %146 = stablehlo.multiply %145, %2 : tensor<256xbf16> loc(#loc158)
    %147 = stablehlo.reshape %146 : (tensor<256xbf16>) -> tensor<256x1xbf16> loc(#loc159)
    %148 = stablehlo.add %147, %1 : tensor<256x1xbf16> loc(#loc160)
    %149 = stablehlo.rsqrt %148 : tensor<256x1xbf16> loc(#loc161)
    %150 = stablehlo.reshape %149 : (tensor<256x1xbf16>) -> tensor<256xbf16> loc(#loc162)
    %151 = stablehlo.broadcast_in_dim %150, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16> loc(#loc163)
    %152 = stablehlo.multiply %143, %151 : tensor<256x768xbf16> loc(#loc164)
    %153 = stablehlo.reshape %arg9 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc165)
    %154 = stablehlo.reshape %153 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc166)
    %155 = stablehlo.broadcast_in_dim %154, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16> loc(#loc167)
    %156 = stablehlo.multiply %152, %155 : tensor<256x768xbf16> loc(#loc168)
    %157 = stablehlo.reshape %arg8 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc169)
    %158 = stablehlo.reshape %157 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc170)
    %159 = stablehlo.broadcast_in_dim %158, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16> loc(#loc171)
    %160 = stablehlo.add %156, %159 : tensor<256x768xbf16> loc(#loc172)
    %161 = stablehlo.reshape %arg7 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16> loc(#loc173)
    %162 = stablehlo.reshape %161 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16> loc(#loc174)
    %163 = stablehlo.transpose %162, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16> loc(#loc175)
    %164 = stablehlo.dot_general %160, %163, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x3072xbf16>) -> tensor<256x3072xbf16> loc(#loc176)
    %165 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16> loc(#loc177)
    %166 = stablehlo.reshape %165 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16> loc(#loc178)
    %167 = stablehlo.broadcast_in_dim %166, dims = [1] : (tensor<3072xbf16>) -> tensor<256x3072xbf16> loc(#loc179)
    %168 = stablehlo.add %164, %167 : tensor<256x3072xbf16> loc(#loc180)
    %169 = stablehlo.maximum %168, %0 : tensor<256x3072xbf16> loc(#loc181)
    %170 = stablehlo.reshape %arg5 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16> loc(#loc182)
    %171 = stablehlo.reshape %170 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16> loc(#loc183)
    %172 = stablehlo.transpose %171, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16> loc(#loc184)
    %173 = stablehlo.dot_general %169, %172, contracting_dims = [1] x [0] : (tensor<256x3072xbf16>, tensor<3072x768xbf16>) -> tensor<256x768xbf16> loc(#loc185)
    %174 = stablehlo.reshape %arg4 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc186)
    %175 = stablehlo.reshape %174 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc187)
    %176 = stablehlo.broadcast_in_dim %175, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16> loc(#loc188)
    %177 = stablehlo.add %173, %176 : tensor<256x768xbf16> loc(#loc189)
    %178 = stablehlo.add %139, %177 : tensor<256x768xbf16> loc(#loc190)
    %179 = stablehlo.reshape %178 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16> loc(#loc191)
    %180 = stablehlo.reduce(%179 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16> loc(#loc192)
    %181 = stablehlo.multiply %180, %10 : tensor<8x32xbf16> loc(#loc193)
    %182 = stablehlo.broadcast_in_dim %181, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16> loc(#loc194)
    %183 = stablehlo.subtract %179, %182 : tensor<8x32x768xbf16> loc(#loc195)
    %184 = stablehlo.multiply %183, %183 : tensor<8x32x768xbf16> loc(#loc196)
    %185 = stablehlo.reduce(%184 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16> loc(#loc197)
    %186 = stablehlo.multiply %185, %10 : tensor<8x32xbf16> loc(#loc198)
    %187 = stablehlo.reshape %186 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16> loc(#loc199)
    %188 = stablehlo.add %187, %9 : tensor<8x32x1xbf16> loc(#loc200)
    %189 = stablehlo.rsqrt %188 : tensor<8x32x1xbf16> loc(#loc201)
    %190 = stablehlo.reshape %189 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16> loc(#loc202)
    %191 = stablehlo.broadcast_in_dim %190, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16> loc(#loc203)
    %192 = stablehlo.multiply %183, %191 : tensor<8x32x768xbf16> loc(#loc204)
    %193 = stablehlo.reshape %arg3 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc205)
    %194 = stablehlo.reshape %193 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc206)
    %195 = stablehlo.broadcast_in_dim %194, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16> loc(#loc207)
    %196 = stablehlo.multiply %192, %195 : tensor<8x32x768xbf16> loc(#loc208)
    %197 = stablehlo.reshape %arg2 : (tensor<768xbf16>) -> tensor<1x1x768xbf16> loc(#loc209)
    %198 = stablehlo.reshape %197 : (tensor<1x1x768xbf16>) -> tensor<768xbf16> loc(#loc210)
    %199 = stablehlo.broadcast_in_dim %198, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16> loc(#loc211)
    %200 = stablehlo.add %196, %199 : tensor<8x32x768xbf16> loc(#loc212)
    %201 = stablehlo.reshape %200 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16> loc(#loc213)
    %202 = stablehlo.reshape %arg1 : (tensor<2x768xbf16>) -> tensor<1x2x768xbf16> loc(#loc214)
    %203 = stablehlo.reshape %202 : (tensor<1x2x768xbf16>) -> tensor<2x768xbf16> loc(#loc215)
    %204 = stablehlo.transpose %203, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,2]{0,1}"} : (tensor<2x768xbf16>) -> tensor<768x2xbf16> loc(#loc216)
    %205 = stablehlo.dot_general %201, %204, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x2xbf16>) -> tensor<256x2xbf16> loc(#loc217)
    %206 = stablehlo.reshape %205 : (tensor<256x2xbf16>) -> tensor<8x32x2xbf16> loc(#loc218)
    %207 = stablehlo.broadcast_in_dim %c_3, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32> loc(#loc219)
    %208 = stablehlo.compare  NE, %16, %12 : (tensor<8x32xi64>, tensor<8x32xi64>) -> tensor<8x32xi1> loc(#loc220)
    %209 = stablehlo.convert %208 : (tensor<8x32xi1>) -> tensor<8x32xi32> loc(#loc221)
    %210 = stablehlo.multiply %207, %209 : tensor<8x32xi32> loc(#loc222)
    %211 = stablehlo.iota dim = 0 : tensor<32xi32> loc(#loc223)
    %212 = stablehlo.broadcast_in_dim %211, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32> loc(#loc223)
    %213:2 = stablehlo.reduce(%210 init: %c_4), (%212 init: %c_0) across dimensions = [1] : (tensor<8x32xi32>, tensor<8x32xi32>, tensor<i32>, tensor<i32>) -> (tensor<8xi32>, tensor<8xi32>)
     reducer(%arg23: tensor<i32> loc("reduce.39"), %arg25: tensor<i32> loc("reduce.39")) (%arg24: tensor<i32> loc("reduce.39"), %arg26: tensor<i32> loc("reduce.39"))  {
      %221 = stablehlo.compare  GE, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc225)
      %222 = stablehlo.select %221, %arg23, %arg25 : tensor<i1>, tensor<i32> loc(#loc226)
      %223 = stablehlo.compare  EQ, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1> loc(#loc227)
      %224 = stablehlo.minimum %arg24, %arg26 : tensor<i32> loc(#loc228)
      %225 = stablehlo.select %221, %arg24, %arg26 : tensor<i1>, tensor<i32> loc(#loc229)
      %226 = stablehlo.select %223, %224, %225 : tensor<i1>, tensor<i32> loc(#loc230)
      stablehlo.return %222, %226 : tensor<i32>, tensor<i32> loc(#loc)
    } loc(#loc224)
    %214 = stablehlo.convert %213#1 : (tensor<8xi32>) -> tensor<8xi64> loc(#loc231)
    %215 = stablehlo.compare  LT, %214, %c_12 : (tensor<8xi64>, tensor<8xi64>) -> tensor<8xi1> loc(#loc232)
    %216 = stablehlo.add %214, %c_11 : tensor<8xi64> loc(#loc233)
    %217 = stablehlo.select %215, %216, %214 : tensor<8xi1>, tensor<8xi64> loc(#loc234)
    %218 = stablehlo.reshape %217 : (tensor<8xi64>) -> tensor<8x1xi64> loc(#loc235)
    %219 = stablehlo.concatenate %c_13, %218, dim = 1 : (tensor<8x1xi64>, tensor<8x1xi64>) -> tensor<8x2xi64> loc(#loc236)
    %220 = "stablehlo.gather"(%206, %219) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1, 2>}> : (tensor<8x32x2xbf16>, tensor<8x2xi64>) -> tensor<8x2xbf16> loc(#loc237)
    return %220 : tensor<8x2xbf16> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
#loc24 = loc("reshape.188")
#loc25 = loc("reshape.190")
#loc26 = loc("reshape.3")
#loc27 = loc("reshape.5")
#loc28 = loc("reshape.186")
#loc29 = loc("convert.191")
#loc30 = loc("gather.192")
#loc31 = loc("reshape.193")
#loc32 = loc("reshape.180")
#loc33 = loc("reshape.182")
#loc34 = loc("reshape.160")
#loc35 = loc("reshape.162")
#loc37 = loc("add.167")
#loc38 = loc("multiply.169")
#loc39 = loc("subtract.172")
#loc40 = loc("add.177")
#loc41 = loc("reshape.178")
#loc42 = loc("convert.183")
#loc43 = loc("gather.184")
#loc44 = loc("reshape.185")
#loc45 = loc("add.196")
#loc46 = loc("reduce.242")
#loc47 = loc("multiply.251")
#loc48 = loc("broadcast.261")
#loc49 = loc("subtract.262")
#loc50 = loc("multiply.218")
#loc51 = loc("reduce.225")
#loc52 = loc("multiply.234")
#loc53 = loc("reshape.235")
#loc54 = loc("add.255")
#loc55 = loc("rsqrt.256")
#loc56 = loc("reshape.263")
#loc57 = loc("broadcast.264")
#loc58 = loc("multiply.265")
#loc59 = loc("reshape.149")
#loc60 = loc("reshape.151")
#loc61 = loc("broadcast.266")
#loc62 = loc("multiply.267")
#loc63 = loc("reshape.145")
#loc64 = loc("reshape.147")
#loc65 = loc("broadcast.270")
#loc66 = loc("add.271")
#loc67 = loc("reshape.396")
#loc68 = loc("reshape.392")
#loc69 = loc("reshape.394")
#loc70 = loc("transpose.395")
#loc71 = loc("dot.397")
#loc72 = loc("reshape.398")
#loc73 = loc("reshape.388")
#loc74 = loc("reshape.390")
#loc75 = loc("broadcast.401")
#loc76 = loc("add.402")
#loc77 = loc("multiply.404")
#loc78 = loc("reshape.405")
#loc79 = loc("transpose.406")
#loc80 = loc("reshape.408")
#loc81 = loc("reshape.369")
#loc82 = loc("reshape.371")
#loc83 = loc("transpose.372")
#loc84 = loc("dot.374")
#loc85 = loc("reshape.375")
#loc86 = loc("reshape.365")
#loc87 = loc("reshape.367")
#loc88 = loc("broadcast.378")
#loc89 = loc("add.379")
#loc90 = loc("reshape.380")
#loc91 = loc("transpose.382")
#loc92 = loc("reshape.384")
#loc93 = loc("dot.409")
#loc94 = loc("reshape.410")
#loc95 = loc("broadcast.313")
#loc96 = loc("broadcast.315")
#loc97 = loc("subtract.316")
#loc98 = loc("compare.318")
#loc99 = loc("select.320")
#loc100 = loc("compare.290")
#loc101 = loc("convert.291")
#loc102 = loc("multiply.321")
#loc103 = loc("reshape.322")
#loc104 = loc("broadcast.328")
#loc105 = loc("reshape.343")
#loc106 = loc("convert.348")
#loc107 = loc("reshape.351")
#loc108 = loc("broadcast.352")
#loc109 = loc("add.353")
#loc110 = loc("compare.356")
#loc111 = loc("select.358")
#loc112 = loc("reshape.415")
#loc113 = loc("broadcast.416")
#loc114 = loc("add.417")
#loc115 = loc("convert.418")
#loc116 = loc("reduce.424")
#loc117 = loc("broadcast.425")
#loc118 = loc("subtract.426")
#loc119 = loc("exponential.427")
#loc120 = loc("reduce.433")
#loc121 = loc("broadcast.434")
#loc122 = loc("divide.435")
#loc123 = loc("convert.436")
#loc124 = loc("reshape.438")
#loc125 = loc("reshape.139")
#loc126 = loc("reshape.141")
#loc127 = loc("transpose.142")
#loc128 = loc("dot.273")
#loc129 = loc("reshape.274")
#loc130 = loc("reshape.135")
#loc131 = loc("reshape.137")
#loc132 = loc("broadcast.277")
#loc133 = loc("add.278")
#loc134 = loc("reshape.279")
#loc135 = loc("transpose.280")
#loc136 = loc("reshape.282")
#loc137 = loc("dot.439")
#loc138 = loc("reshape.440")
#loc139 = loc("transpose.441")
#loc140 = loc("reshape.443")
#loc141 = loc("reshape.129")
#loc142 = loc("reshape.131")
#loc143 = loc("transpose.132")
#loc144 = loc("dot.444")
#loc145 = loc("reshape.445")
#loc146 = loc("reshape.125")
#loc147 = loc("reshape.127")
#loc148 = loc("broadcast.448")
#loc149 = loc("add.449")
#loc150 = loc("add.452")
#loc151 = loc("reshape.453")
#loc152 = loc("reduce.499")
#loc153 = loc("multiply.508")
#loc154 = loc("broadcast.518")
#loc155 = loc("subtract.519")
#loc156 = loc("multiply.475")
#loc157 = loc("reduce.482")
#loc158 = loc("multiply.491")
#loc159 = loc("reshape.492")
#loc160 = loc("add.512")
#loc161 = loc("rsqrt.513")
#loc162 = loc("reshape.520")
#loc163 = loc("broadcast.521")
#loc164 = loc("multiply.522")
#loc165 = loc("reshape.117")
#loc166 = loc("reshape.119")
#loc167 = loc("broadcast.523")
#loc168 = loc("multiply.524")
#loc169 = loc("reshape.113")
#loc170 = loc("reshape.115")
#loc171 = loc("broadcast.527")
#loc172 = loc("add.528")
#loc173 = loc("reshape.107")
#loc174 = loc("reshape.109")
#loc175 = loc("transpose.110")
#loc176 = loc("dot.529")
#loc177 = loc("reshape.100")
#loc178 = loc("reshape.102")
#loc179 = loc("broadcast.534")
#loc180 = loc("add.535")
#loc181 = loc("maximum.538")
#loc182 = loc("reshape.93")
#loc183 = loc("reshape.95")
#loc184 = loc("transpose.96")
#loc185 = loc("dot.539")
#loc186 = loc("reshape.86")
#loc187 = loc("reshape.88")
#loc188 = loc("broadcast.544")
#loc189 = loc("add.545")
#loc190 = loc("add.548")
#loc191 = loc("reshape.549")
#loc192 = loc("reduce.595")
#loc193 = loc("multiply.604")
#loc194 = loc("broadcast.614")
#loc195 = loc("subtract.615")
#loc196 = loc("multiply.571")
#loc197 = loc("reduce.578")
#loc198 = loc("multiply.587")
#loc199 = loc("reshape.588")
#loc200 = loc("add.608")
#loc201 = loc("rsqrt.609")
#loc202 = loc("reshape.616")
#loc203 = loc("broadcast.617")
#loc204 = loc("multiply.618")
#loc205 = loc("reshape.77")
#loc206 = loc("reshape.79")
#loc207 = loc("broadcast.619")
#loc208 = loc("multiply.620")
#loc209 = loc("reshape.73")
#loc210 = loc("reshape.75")
#loc211 = loc("broadcast.623")
#loc212 = loc("add.624")
#loc213 = loc("reshape.625")
#loc214 = loc("reshape.67")
#loc215 = loc("reshape.69")
#loc216 = loc("transpose.70")
#loc217 = loc("dot.626")
#loc218 = loc("reshape.627")
#loc219 = loc("broadcast.10")
#loc220 = loc("compare.7")
#loc221 = loc("convert.8")
#loc222 = loc("multiply.11")
#loc223 = loc("iota.14")
#loc225 = loc("compare.32")
#loc226 = loc("select.33")
#loc227 = loc("compare.35")
#loc228 = loc("minimum.36")
#loc229 = loc("select.34")
#loc230 = loc("select.37")
#loc231 = loc("convert.41")
#loc232 = loc("compare.50")
#loc233 = loc("add.47")
#loc234 = loc("select.51")
#loc235 = loc("reshape.64")
#loc236 = loc("concatenate.65")
#loc237 = loc("gather.628")
------------------ END OF MLIR MODULE ------------------
// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @SyncTensorsGraph.630) //----- //
module @SyncTensorsGraph.630 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"}, %arg1: tensor<2x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___score_weight"}, %arg2: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_final_layer_norm_bias"}, %arg3: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_final_layer_norm_weight"}, %arg4: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_fc2_bias"}, %arg5: tensor<768x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_fc2_weight"}, %arg6: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_fc1_bias"}, %arg7: tensor<3072x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_fc1_weight"}, %arg8: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_bias"}, %arg9: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_weight"}, %arg10: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_bias"}, %arg11: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_weight"}, %arg12: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_bias"}, %arg13: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_weight"}, %arg14: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_bias"}, %arg15: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_weight"}, %arg16: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_1"}, %arg17: tensor<2050x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_embed_positions_weight"}, %arg18: tensor<50272x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_embed_tokens_weight"}, %arg19: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_bias"}, %arg20: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_weight"}, %arg21: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_bias"}, %arg22: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_weight"}) -> tensor<8x2xbf16> {
    %c = stablehlo.constant dense<0> : tensor<i64>
    %c_0 = stablehlo.constant dense<0> : tensor<i32>
    %c_1 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64>
    %cst = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c_3 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi32>
    %c_4 = stablehlo.constant dense<-2147483648> : tensor<i32>
    %c_5 = stablehlo.constant dense<2> : tensor<i64>
    %cst_6 = stablehlo.constant dense<1.250000e-01> : tensor<bf16>
    %c_7 = stablehlo.constant dense<1> : tensor<i64>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %cst_9 = stablehlo.constant dense<1.304630e-03> : tensor<bf16>
    %cst_10 = stablehlo.constant dense<1.001360e-05> : tensor<bf16>
    %c_11 = stablehlo.constant dense<32> : tensor<8xi64>
    %c_12 = stablehlo.constant dense<0> : tensor<8xi64>
    %c_13 = stablehlo.constant dense<[[0], [1], [2], [3], [4], [5], [6], [7]]> : tensor<8x1xi64>
    %cst_14 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<256x3072xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<256x1xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<256xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %4 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %5 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %7 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<32x32xi64>
    %8 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<8x32x768xbf16>
    %9 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x32x1xbf16>
    %10 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<8x32xbf16>
    %11 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<8x32xi64>
    %12 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<8x32xi64>
    %13 = stablehlo.reshape %arg18 : (tensor<50272x768xbf16>) -> tensor<1x50272x768xbf16>
    %14 = stablehlo.reshape %13 : (tensor<1x50272x768xbf16>) -> tensor<50272x768xbf16>
    %15 = stablehlo.reshape %arg0 : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %16 = stablehlo.reshape %15 : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %17 = stablehlo.reshape %15 : (tensor<1x8x32xi64>) -> tensor<256xi64>
    %18 = stablehlo.convert %17 : (tensor<256xi64>) -> tensor<256xui32>
    %19 = "stablehlo.gather"(%14, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<50272x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %20 = stablehlo.reshape %19 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %21 = stablehlo.reshape %arg17 : (tensor<2050x768xbf16>) -> tensor<1x2050x768xbf16>
    %22 = stablehlo.reshape %21 : (tensor<1x2050x768xbf16>) -> tensor<2050x768xbf16>
    %23 = stablehlo.reshape %arg16 : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %24 = stablehlo.reshape %23 : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %25 = "stablehlo.reduce_window"(%24, %c) <{padding = dense<[[0, 0], [31, 0]]> : tensor<2x2xi64>, window_dimensions = array<i64: 1, 32>}> ({
    ^bb0(%arg23: tensor<i64>, %arg24: tensor<i64>):
      %221 = stablehlo.add %arg23, %arg24 : tensor<i64>
      stablehlo.return %221 : tensor<i64>
    }) : (tensor<8x32xi64>, tensor<i64>) -> tensor<8x32xi64>
    %26 = stablehlo.multiply %25, %24 : tensor<8x32xi64>
    %27 = stablehlo.subtract %26, %12 : tensor<8x32xi64>
    %28 = stablehlo.add %27, %11 : tensor<8x32xi64>
    %29 = stablehlo.reshape %28 : (tensor<8x32xi64>) -> tensor<256xi64>
    %30 = stablehlo.convert %29 : (tensor<256xi64>) -> tensor<256xui32>
    %31 = "stablehlo.gather"(%22, %30) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<2050x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %32 = stablehlo.reshape %31 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %33 = stablehlo.add %20, %32 : tensor<8x32x768xbf16>
    %34 = stablehlo.reduce(%33 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %35 = stablehlo.multiply %34, %10 : tensor<8x32xbf16>
    %36 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %37 = stablehlo.subtract %33, %36 : tensor<8x32x768xbf16>
    %38 = stablehlo.multiply %37, %37 : tensor<8x32x768xbf16>
    %39 = stablehlo.reduce(%38 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %40 = stablehlo.multiply %39, %10 : tensor<8x32xbf16>
    %41 = stablehlo.reshape %40 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %42 = stablehlo.add %41, %9 : tensor<8x32x1xbf16>
    %43 = stablehlo.rsqrt %42 : tensor<8x32x1xbf16>
    %44 = stablehlo.reshape %43 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %46 = stablehlo.multiply %37, %45 : tensor<8x32x768xbf16>
    %47 = stablehlo.reshape %arg15 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %48 = stablehlo.reshape %47 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %49 = stablehlo.broadcast_in_dim %48, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %50 = stablehlo.multiply %46, %49 : tensor<8x32x768xbf16>
    %51 = stablehlo.reshape %arg14 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %53 = stablehlo.broadcast_in_dim %52, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %54 = stablehlo.add %50, %53 : tensor<8x32x768xbf16>
    %55 = stablehlo.reshape %54 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %56 = stablehlo.reshape %arg22 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %57 = stablehlo.reshape %56 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %58 = stablehlo.transpose %57, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %59 = stablehlo.dot_general %55, %58, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %60 = stablehlo.reshape %59 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %61 = stablehlo.reshape %arg21 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %63 = stablehlo.broadcast_in_dim %62, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %64 = stablehlo.add %60, %63 : tensor<8x32x768xbf16>
    %65 = stablehlo.multiply %64, %8 : tensor<8x32x768xbf16>
    %66 = stablehlo.reshape %65 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %67 = stablehlo.transpose %66, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %68 = stablehlo.reshape %67 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %69 = stablehlo.reshape %arg20 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %70 = stablehlo.reshape %69 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %71 = stablehlo.transpose %70, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %72 = stablehlo.dot_general %55, %71, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %73 = stablehlo.reshape %72 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %74 = stablehlo.reshape %arg19 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %76 = stablehlo.broadcast_in_dim %75, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %77 = stablehlo.add %73, %76 : tensor<8x32x768xbf16>
    %78 = stablehlo.reshape %77 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 3, 1] : (tensor<8x32x12x64xbf16>) -> tensor<8x12x64x32xbf16>
    %80 = stablehlo.reshape %79 : (tensor<8x12x64x32xbf16>) -> tensor<96x64x32xbf16>
    %81 = stablehlo.dot_general %68, %80, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x64xbf16>, tensor<96x64x32xbf16>) -> tensor<96x32x32xbf16>
    %82 = stablehlo.reshape %81 : (tensor<96x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %83 = stablehlo.broadcast_in_dim %c_1, dims = [1] : (tensor<32xi64>) -> tensor<32x32xi64>
    %84 = stablehlo.broadcast_in_dim %c_1, dims = [0] : (tensor<32xi64>) -> tensor<32x32xi64>
    %85 = stablehlo.subtract %83, %84 : tensor<32x32xi64>
    %86 = stablehlo.compare  GE, %85, %7 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %87 = stablehlo.select %86, %6, %5 : tensor<32x32xi1>, tensor<32x32xbf16>
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %89 = stablehlo.convert %88 : (tensor<32x32xi1>) -> tensor<32x32xbf16>
    %90 = stablehlo.multiply %87, %89 : tensor<32x32xbf16>
    %91 = stablehlo.reshape %90 : (tensor<32x32xbf16>) -> tensor<1x32x32xbf16>
    %92 = stablehlo.broadcast_in_dim %91, dims = [1, 2, 3] : (tensor<1x32x32xbf16>) -> tensor<8x1x32x32xbf16>
    %93 = stablehlo.reshape %23 : (tensor<1x8x32xi64>) -> tensor<8x1x1x32xi64>
    %94 = stablehlo.convert %93 : (tensor<8x1x1x32xi64>) -> tensor<8x1x1x32xbf16>
    %95 = stablehlo.reshape %94 : (tensor<8x1x1x32xbf16>) -> tensor<8x1x32xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] : (tensor<8x1x32xbf16>) -> tensor<8x1x32x32xbf16>
    %97 = stablehlo.add %92, %96 : tensor<8x1x32x32xbf16>
    %98 = stablehlo.compare  EQ, %97, %4 : (tensor<8x1x32x32xbf16>, tensor<8x1x32x32xbf16>) -> tensor<8x1x32x32xi1>
    %99 = stablehlo.select %98, %3, %92 : tensor<8x1x32x32xi1>, tensor<8x1x32x32xbf16>
    %100 = stablehlo.reshape %99 : (tensor<8x1x32x32xbf16>) -> tensor<8x32x32xbf16>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] : (tensor<8x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %102 = stablehlo.add %82, %101 : tensor<8x12x32x32xbf16>
    %103 = stablehlo.convert %102 : (tensor<8x12x32x32xbf16>) -> tensor<8x12x32x32xf32>
    %104 = stablehlo.reduce(%103 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %106 = stablehlo.subtract %103, %105 : tensor<8x12x32x32xf32>
    %107 = stablehlo.exponential %106 : tensor<8x12x32x32xf32>
    %108 = stablehlo.reduce(%107 init: %cst_2) applies stablehlo.add across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %110 = stablehlo.divide %107, %109 : tensor<8x12x32x32xf32>
    %111 = stablehlo.convert %110 : (tensor<8x12x32x32xf32>) -> tensor<8x12x32x32xbf16>
    %112 = stablehlo.reshape %111 : (tensor<8x12x32x32xbf16>) -> tensor<96x32x32xbf16>
    %113 = stablehlo.reshape %arg13 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %114 = stablehlo.reshape %113 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %115 = stablehlo.transpose %114, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %116 = stablehlo.dot_general %55, %115, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %117 = stablehlo.reshape %116 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %118 = stablehlo.reshape %arg12 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %121 = stablehlo.add %117, %120 : tensor<8x32x768xbf16>
    %122 = stablehlo.reshape %121 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %123 = stablehlo.transpose %122, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %124 = stablehlo.reshape %123 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %125 = stablehlo.dot_general %112, %124, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x32xbf16>, tensor<96x32x64xbf16>) -> tensor<96x32x64xbf16>
    %126 = stablehlo.reshape %125 : (tensor<96x32x64xbf16>) -> tensor<8x12x32x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,32,12,64]{3,1,2,0}"} : (tensor<8x12x32x64xbf16>) -> tensor<8x32x12x64xbf16>
    %128 = stablehlo.reshape %127 : (tensor<8x32x12x64xbf16>) -> tensor<256x768xbf16>
    %129 = stablehlo.reshape %arg11 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %131 = stablehlo.transpose %130, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %132 = stablehlo.dot_general %128, %131, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %133 = stablehlo.reshape %132 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %134 = stablehlo.reshape %arg10 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %135 = stablehlo.reshape %134 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %136 = stablehlo.broadcast_in_dim %135, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %137 = stablehlo.add %133, %136 : tensor<8x32x768xbf16>
    %138 = stablehlo.add %33, %137 : tensor<8x32x768xbf16>
    %139 = stablehlo.reshape %138 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %140 = stablehlo.reduce(%139 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %141 = stablehlo.multiply %140, %2 : tensor<256xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %143 = stablehlo.subtract %139, %142 : tensor<256x768xbf16>
    %144 = stablehlo.multiply %143, %143 : tensor<256x768xbf16>
    %145 = stablehlo.reduce(%144 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %146 = stablehlo.multiply %145, %2 : tensor<256xbf16>
    %147 = stablehlo.reshape %146 : (tensor<256xbf16>) -> tensor<256x1xbf16>
    %148 = stablehlo.add %147, %1 : tensor<256x1xbf16>
    %149 = stablehlo.rsqrt %148 : tensor<256x1xbf16>
    %150 = stablehlo.reshape %149 : (tensor<256x1xbf16>) -> tensor<256xbf16>
    %151 = stablehlo.broadcast_in_dim %150, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %152 = stablehlo.multiply %143, %151 : tensor<256x768xbf16>
    %153 = stablehlo.reshape %arg9 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %155 = stablehlo.broadcast_in_dim %154, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %156 = stablehlo.multiply %152, %155 : tensor<256x768xbf16>
    %157 = stablehlo.reshape %arg8 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %158 = stablehlo.reshape %157 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %159 = stablehlo.broadcast_in_dim %158, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %160 = stablehlo.add %156, %159 : tensor<256x768xbf16>
    %161 = stablehlo.reshape %arg7 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16>
    %162 = stablehlo.reshape %161 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16>
    %163 = stablehlo.transpose %162, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16>
    %164 = stablehlo.dot_general %160, %163, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x3072xbf16>) -> tensor<256x3072xbf16>
    %165 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %166 = stablehlo.reshape %165 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %167 = stablehlo.broadcast_in_dim %166, dims = [1] : (tensor<3072xbf16>) -> tensor<256x3072xbf16>
    %168 = stablehlo.add %164, %167 : tensor<256x3072xbf16>
    %169 = stablehlo.maximum %168, %0 : tensor<256x3072xbf16>
    %170 = stablehlo.reshape %arg5 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16>
    %172 = stablehlo.transpose %171, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16>
    %173 = stablehlo.dot_general %169, %172, contracting_dims = [1] x [0] : (tensor<256x3072xbf16>, tensor<3072x768xbf16>) -> tensor<256x768xbf16>
    %174 = stablehlo.reshape %arg4 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %175 = stablehlo.reshape %174 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %176 = stablehlo.broadcast_in_dim %175, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %177 = stablehlo.add %173, %176 : tensor<256x768xbf16>
    %178 = stablehlo.add %139, %177 : tensor<256x768xbf16>
    %179 = stablehlo.reshape %178 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %180 = stablehlo.reduce(%179 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %181 = stablehlo.multiply %180, %10 : tensor<8x32xbf16>
    %182 = stablehlo.broadcast_in_dim %181, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %183 = stablehlo.subtract %179, %182 : tensor<8x32x768xbf16>
    %184 = stablehlo.multiply %183, %183 : tensor<8x32x768xbf16>
    %185 = stablehlo.reduce(%184 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %186 = stablehlo.multiply %185, %10 : tensor<8x32xbf16>
    %187 = stablehlo.reshape %186 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %188 = stablehlo.add %187, %9 : tensor<8x32x1xbf16>
    %189 = stablehlo.rsqrt %188 : tensor<8x32x1xbf16>
    %190 = stablehlo.reshape %189 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %191 = stablehlo.broadcast_in_dim %190, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %192 = stablehlo.multiply %183, %191 : tensor<8x32x768xbf16>
    %193 = stablehlo.reshape %arg3 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %194 = stablehlo.reshape %193 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %195 = stablehlo.broadcast_in_dim %194, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %196 = stablehlo.multiply %192, %195 : tensor<8x32x768xbf16>
    %197 = stablehlo.reshape %arg2 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %198 = stablehlo.reshape %197 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %199 = stablehlo.broadcast_in_dim %198, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %200 = stablehlo.add %196, %199 : tensor<8x32x768xbf16>
    %201 = stablehlo.reshape %200 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %202 = stablehlo.reshape %arg1 : (tensor<2x768xbf16>) -> tensor<1x2x768xbf16>
    %203 = stablehlo.reshape %202 : (tensor<1x2x768xbf16>) -> tensor<2x768xbf16>
    %204 = stablehlo.transpose %203, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,2]{0,1}"} : (tensor<2x768xbf16>) -> tensor<768x2xbf16>
    %205 = stablehlo.dot_general %201, %204, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x2xbf16>) -> tensor<256x2xbf16>
    %206 = stablehlo.reshape %205 : (tensor<256x2xbf16>) -> tensor<8x32x2xbf16>
    %207 = stablehlo.broadcast_in_dim %c_3, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32>
    %208 = stablehlo.compare  NE, %16, %12 : (tensor<8x32xi64>, tensor<8x32xi64>) -> tensor<8x32xi1>
    %209 = stablehlo.convert %208 : (tensor<8x32xi1>) -> tensor<8x32xi32>
    %210 = stablehlo.multiply %207, %209 : tensor<8x32xi32>
    %211 = stablehlo.iota dim = 0 : tensor<32xi32>
    %212 = stablehlo.broadcast_in_dim %211, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32>
    %213:2 = stablehlo.reduce(%210 init: %c_4), (%212 init: %c_0) across dimensions = [1] : (tensor<8x32xi32>, tensor<8x32xi32>, tensor<i32>, tensor<i32>) -> (tensor<8xi32>, tensor<8xi32>)
     reducer(%arg23: tensor<i32>, %arg25: tensor<i32>) (%arg24: tensor<i32>, %arg26: tensor<i32>)  {
      %221 = stablehlo.compare  GE, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %222 = stablehlo.select %221, %arg23, %arg25 : tensor<i1>, tensor<i32>
      %223 = stablehlo.compare  EQ, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %224 = stablehlo.minimum %arg24, %arg26 : tensor<i32>
      %225 = stablehlo.select %221, %arg24, %arg26 : tensor<i1>, tensor<i32>
      %226 = stablehlo.select %223, %224, %225 : tensor<i1>, tensor<i32>
      stablehlo.return %222, %226 : tensor<i32>, tensor<i32>
    }
    %214 = stablehlo.convert %213#1 : (tensor<8xi32>) -> tensor<8xi64>
    %215 = stablehlo.compare  LT, %214, %c_12 : (tensor<8xi64>, tensor<8xi64>) -> tensor<8xi1>
    %216 = stablehlo.add %214, %c_11 : tensor<8xi64>
    %217 = stablehlo.select %215, %216, %214 : tensor<8xi1>, tensor<8xi64>
    %218 = stablehlo.reshape %217 : (tensor<8xi64>) -> tensor<8x1xi64>
    %219 = stablehlo.concatenate %c_13, %218, dim = 1 : (tensor<8x1xi64>, tensor<8x1xi64>) -> tensor<8x2xi64>
    %220 = "stablehlo.gather"(%206, %219) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1, 2>}> : (tensor<8x32x2xbf16>, tensor<8x2xi64>) -> tensor<8x2xbf16>
    return %220 : tensor<8x2xbf16>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.630 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"}, %arg1: tensor<2x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___score_weight"}, %arg2: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_final_layer_norm_bias"}, %arg3: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_final_layer_norm_weight"}, %arg4: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_fc2_bias"}, %arg5: tensor<768x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_fc2_weight"}, %arg6: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_fc1_bias"}, %arg7: tensor<3072x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_fc1_weight"}, %arg8: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_bias"}, %arg9: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_weight"}, %arg10: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_bias"}, %arg11: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_weight"}, %arg12: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_bias"}, %arg13: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_weight"}, %arg14: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_bias"}, %arg15: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_weight"}, %arg16: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_1"}, %arg17: tensor<2050x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_embed_positions_weight"}, %arg18: tensor<50272x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_embed_tokens_weight"}, %arg19: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_bias"}, %arg20: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_weight"}, %arg21: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_bias"}, %arg22: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_weight"}) -> tensor<8x2xbf16> {
    %c = stablehlo.constant dense<0> : tensor<i64>
    %c_0 = stablehlo.constant dense<0> : tensor<i32>
    %c_1 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64>
    %cst = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c_3 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi32>
    %c_4 = stablehlo.constant dense<-2147483648> : tensor<i32>
    %c_5 = stablehlo.constant dense<2> : tensor<i64>
    %cst_6 = stablehlo.constant dense<1.250000e-01> : tensor<bf16>
    %c_7 = stablehlo.constant dense<1> : tensor<i64>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %cst_9 = stablehlo.constant dense<1.304630e-03> : tensor<bf16>
    %cst_10 = stablehlo.constant dense<1.001360e-05> : tensor<bf16>
    %c_11 = stablehlo.constant dense<32> : tensor<8xi64>
    %c_12 = stablehlo.constant dense<0> : tensor<8xi64>
    %c_13 = stablehlo.constant dense<[[0], [1], [2], [3], [4], [5], [6], [7]]> : tensor<8x1xi64>
    %cst_14 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<256x3072xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<256x1xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<256xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %4 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %5 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %7 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<32x32xi64>
    %8 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<8x32x768xbf16>
    %9 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x32x1xbf16>
    %10 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<8x32xbf16>
    %11 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<8x32xi64>
    %12 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<8x32xi64>
    %13 = stablehlo.reshape %arg18 : (tensor<50272x768xbf16>) -> tensor<1x50272x768xbf16>
    %14 = stablehlo.reshape %13 : (tensor<1x50272x768xbf16>) -> tensor<50272x768xbf16>
    %15 = stablehlo.reshape %arg0 : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %16 = stablehlo.reshape %15 : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %17 = stablehlo.reshape %15 : (tensor<1x8x32xi64>) -> tensor<256xi64>
    %18 = stablehlo.convert %17 : (tensor<256xi64>) -> tensor<256xui32>
    %19 = "stablehlo.gather"(%14, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<50272x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %20 = stablehlo.reshape %19 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %21 = stablehlo.reshape %arg17 : (tensor<2050x768xbf16>) -> tensor<1x2050x768xbf16>
    %22 = stablehlo.reshape %21 : (tensor<1x2050x768xbf16>) -> tensor<2050x768xbf16>
    %23 = stablehlo.reshape %arg16 : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %24 = stablehlo.reshape %23 : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %25 = "stablehlo.reduce_window"(%24, %c) <{padding = dense<[[0, 0], [31, 0]]> : tensor<2x2xi64>, window_dimensions = array<i64: 1, 32>}> ({
    ^bb0(%arg23: tensor<i64>, %arg24: tensor<i64>):
      %221 = stablehlo.add %arg23, %arg24 : tensor<i64>
      stablehlo.return %221 : tensor<i64>
    }) : (tensor<8x32xi64>, tensor<i64>) -> tensor<8x32xi64>
    %26 = stablehlo.multiply %25, %24 : tensor<8x32xi64>
    %27 = stablehlo.subtract %26, %12 : tensor<8x32xi64>
    %28 = stablehlo.add %27, %11 : tensor<8x32xi64>
    %29 = stablehlo.reshape %28 : (tensor<8x32xi64>) -> tensor<256xi64>
    %30 = stablehlo.convert %29 : (tensor<256xi64>) -> tensor<256xui32>
    %31 = "stablehlo.gather"(%22, %30) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<2050x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %32 = stablehlo.reshape %31 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %33 = stablehlo.add %20, %32 : tensor<8x32x768xbf16>
    %34 = stablehlo.reduce(%33 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %35 = stablehlo.multiply %34, %10 : tensor<8x32xbf16>
    %36 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %37 = stablehlo.subtract %33, %36 : tensor<8x32x768xbf16>
    %38 = stablehlo.multiply %37, %37 : tensor<8x32x768xbf16>
    %39 = stablehlo.reduce(%38 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %40 = stablehlo.multiply %39, %10 : tensor<8x32xbf16>
    %41 = stablehlo.reshape %40 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %42 = stablehlo.add %41, %9 : tensor<8x32x1xbf16>
    %43 = stablehlo.rsqrt %42 : tensor<8x32x1xbf16>
    %44 = stablehlo.reshape %43 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %46 = stablehlo.multiply %37, %45 : tensor<8x32x768xbf16>
    %47 = stablehlo.reshape %arg15 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %48 = stablehlo.reshape %47 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %49 = stablehlo.broadcast_in_dim %48, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %50 = stablehlo.multiply %46, %49 : tensor<8x32x768xbf16>
    %51 = stablehlo.reshape %arg14 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %53 = stablehlo.broadcast_in_dim %52, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %54 = stablehlo.add %50, %53 : tensor<8x32x768xbf16>
    %55 = stablehlo.reshape %54 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %56 = stablehlo.reshape %arg22 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %57 = stablehlo.reshape %56 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %58 = stablehlo.transpose %57, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %59 = stablehlo.dot_general %55, %58, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %60 = stablehlo.reshape %59 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %61 = stablehlo.reshape %arg21 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %63 = stablehlo.broadcast_in_dim %62, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %64 = stablehlo.add %60, %63 : tensor<8x32x768xbf16>
    %65 = stablehlo.multiply %64, %8 : tensor<8x32x768xbf16>
    %66 = stablehlo.reshape %65 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %67 = stablehlo.transpose %66, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %68 = stablehlo.reshape %67 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %69 = stablehlo.reshape %arg20 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %70 = stablehlo.reshape %69 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %71 = stablehlo.transpose %70, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %72 = stablehlo.dot_general %55, %71, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %73 = stablehlo.reshape %72 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %74 = stablehlo.reshape %arg19 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %76 = stablehlo.broadcast_in_dim %75, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %77 = stablehlo.add %73, %76 : tensor<8x32x768xbf16>
    %78 = stablehlo.reshape %77 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 3, 1] : (tensor<8x32x12x64xbf16>) -> tensor<8x12x64x32xbf16>
    %80 = stablehlo.reshape %79 : (tensor<8x12x64x32xbf16>) -> tensor<96x64x32xbf16>
    %81 = stablehlo.dot_general %68, %80, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x64xbf16>, tensor<96x64x32xbf16>) -> tensor<96x32x32xbf16>
    %82 = stablehlo.reshape %81 : (tensor<96x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %83 = stablehlo.broadcast_in_dim %c_1, dims = [1] : (tensor<32xi64>) -> tensor<32x32xi64>
    %84 = stablehlo.broadcast_in_dim %c_1, dims = [0] : (tensor<32xi64>) -> tensor<32x32xi64>
    %85 = stablehlo.subtract %83, %84 : tensor<32x32xi64>
    %86 = stablehlo.compare  GE, %85, %7 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %87 = stablehlo.select %86, %6, %5 : tensor<32x32xi1>, tensor<32x32xbf16>
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %89 = stablehlo.convert %88 : (tensor<32x32xi1>) -> tensor<32x32xbf16>
    %90 = stablehlo.multiply %87, %89 : tensor<32x32xbf16>
    %91 = stablehlo.reshape %90 : (tensor<32x32xbf16>) -> tensor<1x32x32xbf16>
    %92 = stablehlo.broadcast_in_dim %91, dims = [1, 2, 3] : (tensor<1x32x32xbf16>) -> tensor<8x1x32x32xbf16>
    %93 = stablehlo.reshape %23 : (tensor<1x8x32xi64>) -> tensor<8x1x1x32xi64>
    %94 = stablehlo.convert %93 : (tensor<8x1x1x32xi64>) -> tensor<8x1x1x32xbf16>
    %95 = stablehlo.reshape %94 : (tensor<8x1x1x32xbf16>) -> tensor<8x1x32xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] : (tensor<8x1x32xbf16>) -> tensor<8x1x32x32xbf16>
    %97 = stablehlo.add %92, %96 : tensor<8x1x32x32xbf16>
    %98 = stablehlo.compare  EQ, %97, %4 : (tensor<8x1x32x32xbf16>, tensor<8x1x32x32xbf16>) -> tensor<8x1x32x32xi1>
    %99 = stablehlo.select %98, %3, %92 : tensor<8x1x32x32xi1>, tensor<8x1x32x32xbf16>
    %100 = stablehlo.reshape %99 : (tensor<8x1x32x32xbf16>) -> tensor<8x32x32xbf16>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] : (tensor<8x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %102 = stablehlo.add %82, %101 : tensor<8x12x32x32xbf16>
    %103 = stablehlo.convert %102 : (tensor<8x12x32x32xbf16>) -> tensor<8x12x32x32xf32>
    %104 = stablehlo.reduce(%103 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %106 = stablehlo.subtract %103, %105 : tensor<8x12x32x32xf32>
    %107 = stablehlo.exponential %106 : tensor<8x12x32x32xf32>
    %108 = stablehlo.reduce(%107 init: %cst_2) applies stablehlo.add across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %110 = stablehlo.divide %107, %109 : tensor<8x12x32x32xf32>
    %111 = stablehlo.convert %110 : (tensor<8x12x32x32xf32>) -> tensor<8x12x32x32xbf16>
    %112 = stablehlo.reshape %111 : (tensor<8x12x32x32xbf16>) -> tensor<96x32x32xbf16>
    %113 = stablehlo.reshape %arg13 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %114 = stablehlo.reshape %113 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %115 = stablehlo.transpose %114, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %116 = stablehlo.dot_general %55, %115, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %117 = stablehlo.reshape %116 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %118 = stablehlo.reshape %arg12 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %121 = stablehlo.add %117, %120 : tensor<8x32x768xbf16>
    %122 = stablehlo.reshape %121 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %123 = stablehlo.transpose %122, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %124 = stablehlo.reshape %123 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %125 = stablehlo.dot_general %112, %124, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x32xbf16>, tensor<96x32x64xbf16>) -> tensor<96x32x64xbf16>
    %126 = stablehlo.reshape %125 : (tensor<96x32x64xbf16>) -> tensor<8x12x32x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,32,12,64]{3,1,2,0}"} : (tensor<8x12x32x64xbf16>) -> tensor<8x32x12x64xbf16>
    %128 = stablehlo.reshape %127 : (tensor<8x32x12x64xbf16>) -> tensor<256x768xbf16>
    %129 = stablehlo.reshape %arg11 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %131 = stablehlo.transpose %130, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %132 = stablehlo.dot_general %128, %131, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %133 = stablehlo.reshape %132 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %134 = stablehlo.reshape %arg10 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %135 = stablehlo.reshape %134 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %136 = stablehlo.broadcast_in_dim %135, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %137 = stablehlo.add %133, %136 : tensor<8x32x768xbf16>
    %138 = stablehlo.add %33, %137 : tensor<8x32x768xbf16>
    %139 = stablehlo.reshape %138 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %140 = stablehlo.reduce(%139 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %141 = stablehlo.multiply %140, %2 : tensor<256xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %143 = stablehlo.subtract %139, %142 : tensor<256x768xbf16>
    %144 = stablehlo.multiply %143, %143 : tensor<256x768xbf16>
    %145 = stablehlo.reduce(%144 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %146 = stablehlo.multiply %145, %2 : tensor<256xbf16>
    %147 = stablehlo.reshape %146 : (tensor<256xbf16>) -> tensor<256x1xbf16>
    %148 = stablehlo.add %147, %1 : tensor<256x1xbf16>
    %149 = stablehlo.rsqrt %148 : tensor<256x1xbf16>
    %150 = stablehlo.reshape %149 : (tensor<256x1xbf16>) -> tensor<256xbf16>
    %151 = stablehlo.broadcast_in_dim %150, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %152 = stablehlo.multiply %143, %151 : tensor<256x768xbf16>
    %153 = stablehlo.reshape %arg9 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %155 = stablehlo.broadcast_in_dim %154, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %156 = stablehlo.multiply %152, %155 : tensor<256x768xbf16>
    %157 = stablehlo.reshape %arg8 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %158 = stablehlo.reshape %157 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %159 = stablehlo.broadcast_in_dim %158, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %160 = stablehlo.add %156, %159 : tensor<256x768xbf16>
    %161 = stablehlo.reshape %arg7 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16>
    %162 = stablehlo.reshape %161 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16>
    %163 = stablehlo.transpose %162, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16>
    %164 = stablehlo.dot_general %160, %163, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x3072xbf16>) -> tensor<256x3072xbf16>
    %165 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %166 = stablehlo.reshape %165 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %167 = stablehlo.broadcast_in_dim %166, dims = [1] : (tensor<3072xbf16>) -> tensor<256x3072xbf16>
    %168 = stablehlo.add %164, %167 : tensor<256x3072xbf16>
    %169 = stablehlo.maximum %168, %0 : tensor<256x3072xbf16>
    %170 = stablehlo.reshape %arg5 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16>
    %172 = stablehlo.transpose %171, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16>
    %173 = stablehlo.dot_general %169, %172, contracting_dims = [1] x [0] : (tensor<256x3072xbf16>, tensor<3072x768xbf16>) -> tensor<256x768xbf16>
    %174 = stablehlo.reshape %arg4 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %175 = stablehlo.reshape %174 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %176 = stablehlo.broadcast_in_dim %175, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %177 = stablehlo.add %173, %176 : tensor<256x768xbf16>
    %178 = stablehlo.add %139, %177 : tensor<256x768xbf16>
    %179 = stablehlo.reshape %178 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %180 = stablehlo.reduce(%179 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %181 = stablehlo.multiply %180, %10 : tensor<8x32xbf16>
    %182 = stablehlo.broadcast_in_dim %181, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %183 = stablehlo.subtract %179, %182 : tensor<8x32x768xbf16>
    %184 = stablehlo.multiply %183, %183 : tensor<8x32x768xbf16>
    %185 = stablehlo.reduce(%184 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %186 = stablehlo.multiply %185, %10 : tensor<8x32xbf16>
    %187 = stablehlo.reshape %186 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %188 = stablehlo.add %187, %9 : tensor<8x32x1xbf16>
    %189 = stablehlo.rsqrt %188 : tensor<8x32x1xbf16>
    %190 = stablehlo.reshape %189 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %191 = stablehlo.broadcast_in_dim %190, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %192 = stablehlo.multiply %183, %191 : tensor<8x32x768xbf16>
    %193 = stablehlo.reshape %arg3 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %194 = stablehlo.reshape %193 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %195 = stablehlo.broadcast_in_dim %194, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %196 = stablehlo.multiply %192, %195 : tensor<8x32x768xbf16>
    %197 = stablehlo.reshape %arg2 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %198 = stablehlo.reshape %197 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %199 = stablehlo.broadcast_in_dim %198, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %200 = stablehlo.add %196, %199 : tensor<8x32x768xbf16>
    %201 = stablehlo.reshape %200 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %202 = stablehlo.reshape %arg1 : (tensor<2x768xbf16>) -> tensor<1x2x768xbf16>
    %203 = stablehlo.reshape %202 : (tensor<1x2x768xbf16>) -> tensor<2x768xbf16>
    %204 = stablehlo.transpose %203, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,2]{0,1}"} : (tensor<2x768xbf16>) -> tensor<768x2xbf16>
    %205 = stablehlo.dot_general %201, %204, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x2xbf16>) -> tensor<256x2xbf16>
    %206 = stablehlo.reshape %205 : (tensor<256x2xbf16>) -> tensor<8x32x2xbf16>
    %207 = stablehlo.broadcast_in_dim %c_3, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32>
    %208 = stablehlo.compare  NE, %16, %12 : (tensor<8x32xi64>, tensor<8x32xi64>) -> tensor<8x32xi1>
    %209 = stablehlo.convert %208 : (tensor<8x32xi1>) -> tensor<8x32xi32>
    %210 = stablehlo.multiply %207, %209 : tensor<8x32xi32>
    %211 = stablehlo.iota dim = 0 : tensor<32xi32>
    %212 = stablehlo.broadcast_in_dim %211, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32>
    %213:2 = stablehlo.reduce(%210 init: %c_4), (%212 init: %c_0) across dimensions = [1] : (tensor<8x32xi32>, tensor<8x32xi32>, tensor<i32>, tensor<i32>) -> (tensor<8xi32>, tensor<8xi32>)
     reducer(%arg23: tensor<i32>, %arg25: tensor<i32>) (%arg24: tensor<i32>, %arg26: tensor<i32>)  {
      %221 = stablehlo.compare  GE, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %222 = stablehlo.select %221, %arg23, %arg25 : tensor<i1>, tensor<i32>
      %223 = stablehlo.compare  EQ, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %224 = stablehlo.minimum %arg24, %arg26 : tensor<i32>
      %225 = stablehlo.select %221, %arg24, %arg26 : tensor<i1>, tensor<i32>
      %226 = stablehlo.select %223, %224, %225 : tensor<i1>, tensor<i32>
      stablehlo.return %222, %226 : tensor<i32>, tensor<i32>
    }
    %214 = stablehlo.convert %213#1 : (tensor<8xi32>) -> tensor<8xi64>
    %215 = stablehlo.compare  LT, %214, %c_12 : (tensor<8xi64>, tensor<8xi64>) -> tensor<8xi1>
    %216 = stablehlo.add %214, %c_11 : tensor<8xi64>
    %217 = stablehlo.select %215, %216, %214 : tensor<8xi1>, tensor<8xi64>
    %218 = stablehlo.reshape %217 : (tensor<8xi64>) -> tensor<8x1xi64>
    %219 = stablehlo.concatenate %c_13, %218, dim = 1 : (tensor<8x1xi64>, tensor<8x1xi64>) -> tensor<8x2xi64>
    %220 = "stablehlo.gather"(%206, %219) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1, 2>}> : (tensor<8x32x2xbf16>, tensor<8x2xi64>) -> tensor<8x2xbf16>
    return %220 : tensor<8x2xbf16>
  }
}


// -----// IR Dump Before TTPopulateArgumentTypes (tt-populate-argument-types) ('builtin.module' operation: @SyncTensorsGraph.630) //----- //
module @SyncTensorsGraph.630 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"}, %arg1: tensor<2x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___score_weight"}, %arg2: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_final_layer_norm_bias"}, %arg3: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_final_layer_norm_weight"}, %arg4: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_fc2_bias"}, %arg5: tensor<768x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_fc2_weight"}, %arg6: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_fc1_bias"}, %arg7: tensor<3072x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_fc1_weight"}, %arg8: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_bias"}, %arg9: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_weight"}, %arg10: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_bias"}, %arg11: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_weight"}, %arg12: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_bias"}, %arg13: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_weight"}, %arg14: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_bias"}, %arg15: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_weight"}, %arg16: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_1"}, %arg17: tensor<2050x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_embed_positions_weight"}, %arg18: tensor<50272x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_embed_tokens_weight"}, %arg19: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_bias"}, %arg20: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_weight"}, %arg21: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_bias"}, %arg22: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_weight"}) -> tensor<8x2xbf16> {
    %c = stablehlo.constant dense<0> : tensor<i64>
    %c_0 = stablehlo.constant dense<0> : tensor<i32>
    %c_1 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64>
    %cst = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c_3 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi32>
    %c_4 = stablehlo.constant dense<-2147483648> : tensor<i32>
    %c_5 = stablehlo.constant dense<2> : tensor<i64>
    %cst_6 = stablehlo.constant dense<1.250000e-01> : tensor<bf16>
    %c_7 = stablehlo.constant dense<1> : tensor<i64>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %cst_9 = stablehlo.constant dense<1.304630e-03> : tensor<bf16>
    %cst_10 = stablehlo.constant dense<1.001360e-05> : tensor<bf16>
    %c_11 = stablehlo.constant dense<32> : tensor<8xi64>
    %c_12 = stablehlo.constant dense<0> : tensor<8xi64>
    %c_13 = stablehlo.constant dense<[[0], [1], [2], [3], [4], [5], [6], [7]]> : tensor<8x1xi64>
    %cst_14 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<256x3072xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<256x1xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<256xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %4 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %5 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %7 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<32x32xi64>
    %8 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<8x32x768xbf16>
    %9 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x32x1xbf16>
    %10 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<8x32xbf16>
    %11 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<8x32xi64>
    %12 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<8x32xi64>
    %13 = stablehlo.reshape %arg18 : (tensor<50272x768xbf16>) -> tensor<1x50272x768xbf16>
    %14 = stablehlo.reshape %13 : (tensor<1x50272x768xbf16>) -> tensor<50272x768xbf16>
    %15 = stablehlo.reshape %arg0 : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %16 = stablehlo.reshape %15 : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %17 = stablehlo.reshape %15 : (tensor<1x8x32xi64>) -> tensor<256xi64>
    %18 = stablehlo.convert %17 : (tensor<256xi64>) -> tensor<256xui32>
    %19 = "stablehlo.gather"(%14, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<50272x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %20 = stablehlo.reshape %19 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %21 = stablehlo.reshape %arg17 : (tensor<2050x768xbf16>) -> tensor<1x2050x768xbf16>
    %22 = stablehlo.reshape %21 : (tensor<1x2050x768xbf16>) -> tensor<2050x768xbf16>
    %23 = stablehlo.reshape %arg16 : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %24 = stablehlo.reshape %23 : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %25 = "stablehlo.reduce_window"(%24, %c) <{padding = dense<[[0, 0], [31, 0]]> : tensor<2x2xi64>, window_dimensions = array<i64: 1, 32>}> ({
    ^bb0(%arg23: tensor<i64>, %arg24: tensor<i64>):
      %221 = stablehlo.add %arg23, %arg24 : tensor<i64>
      stablehlo.return %221 : tensor<i64>
    }) : (tensor<8x32xi64>, tensor<i64>) -> tensor<8x32xi64>
    %26 = stablehlo.multiply %25, %24 : tensor<8x32xi64>
    %27 = stablehlo.subtract %26, %12 : tensor<8x32xi64>
    %28 = stablehlo.add %27, %11 : tensor<8x32xi64>
    %29 = stablehlo.reshape %28 : (tensor<8x32xi64>) -> tensor<256xi64>
    %30 = stablehlo.convert %29 : (tensor<256xi64>) -> tensor<256xui32>
    %31 = "stablehlo.gather"(%22, %30) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<2050x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %32 = stablehlo.reshape %31 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %33 = stablehlo.add %20, %32 : tensor<8x32x768xbf16>
    %34 = stablehlo.reduce(%33 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %35 = stablehlo.multiply %34, %10 : tensor<8x32xbf16>
    %36 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %37 = stablehlo.subtract %33, %36 : tensor<8x32x768xbf16>
    %38 = stablehlo.multiply %37, %37 : tensor<8x32x768xbf16>
    %39 = stablehlo.reduce(%38 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %40 = stablehlo.multiply %39, %10 : tensor<8x32xbf16>
    %41 = stablehlo.reshape %40 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %42 = stablehlo.add %41, %9 : tensor<8x32x1xbf16>
    %43 = stablehlo.rsqrt %42 : tensor<8x32x1xbf16>
    %44 = stablehlo.reshape %43 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %46 = stablehlo.multiply %37, %45 : tensor<8x32x768xbf16>
    %47 = stablehlo.reshape %arg15 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %48 = stablehlo.reshape %47 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %49 = stablehlo.broadcast_in_dim %48, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %50 = stablehlo.multiply %46, %49 : tensor<8x32x768xbf16>
    %51 = stablehlo.reshape %arg14 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %53 = stablehlo.broadcast_in_dim %52, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %54 = stablehlo.add %50, %53 : tensor<8x32x768xbf16>
    %55 = stablehlo.reshape %54 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %56 = stablehlo.reshape %arg22 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %57 = stablehlo.reshape %56 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %58 = stablehlo.transpose %57, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %59 = stablehlo.dot_general %55, %58, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %60 = stablehlo.reshape %59 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %61 = stablehlo.reshape %arg21 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %63 = stablehlo.broadcast_in_dim %62, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %64 = stablehlo.add %60, %63 : tensor<8x32x768xbf16>
    %65 = stablehlo.multiply %64, %8 : tensor<8x32x768xbf16>
    %66 = stablehlo.reshape %65 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %67 = stablehlo.transpose %66, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %68 = stablehlo.reshape %67 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %69 = stablehlo.reshape %arg20 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %70 = stablehlo.reshape %69 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %71 = stablehlo.transpose %70, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %72 = stablehlo.dot_general %55, %71, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %73 = stablehlo.reshape %72 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %74 = stablehlo.reshape %arg19 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %76 = stablehlo.broadcast_in_dim %75, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %77 = stablehlo.add %73, %76 : tensor<8x32x768xbf16>
    %78 = stablehlo.reshape %77 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 3, 1] : (tensor<8x32x12x64xbf16>) -> tensor<8x12x64x32xbf16>
    %80 = stablehlo.reshape %79 : (tensor<8x12x64x32xbf16>) -> tensor<96x64x32xbf16>
    %81 = stablehlo.dot_general %68, %80, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x64xbf16>, tensor<96x64x32xbf16>) -> tensor<96x32x32xbf16>
    %82 = stablehlo.reshape %81 : (tensor<96x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %83 = stablehlo.broadcast_in_dim %c_1, dims = [1] : (tensor<32xi64>) -> tensor<32x32xi64>
    %84 = stablehlo.broadcast_in_dim %c_1, dims = [0] : (tensor<32xi64>) -> tensor<32x32xi64>
    %85 = stablehlo.subtract %83, %84 : tensor<32x32xi64>
    %86 = stablehlo.compare  GE, %85, %7 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %87 = stablehlo.select %86, %6, %5 : tensor<32x32xi1>, tensor<32x32xbf16>
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %89 = stablehlo.convert %88 : (tensor<32x32xi1>) -> tensor<32x32xbf16>
    %90 = stablehlo.multiply %87, %89 : tensor<32x32xbf16>
    %91 = stablehlo.reshape %90 : (tensor<32x32xbf16>) -> tensor<1x32x32xbf16>
    %92 = stablehlo.broadcast_in_dim %91, dims = [1, 2, 3] : (tensor<1x32x32xbf16>) -> tensor<8x1x32x32xbf16>
    %93 = stablehlo.reshape %23 : (tensor<1x8x32xi64>) -> tensor<8x1x1x32xi64>
    %94 = stablehlo.convert %93 : (tensor<8x1x1x32xi64>) -> tensor<8x1x1x32xbf16>
    %95 = stablehlo.reshape %94 : (tensor<8x1x1x32xbf16>) -> tensor<8x1x32xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] : (tensor<8x1x32xbf16>) -> tensor<8x1x32x32xbf16>
    %97 = stablehlo.add %92, %96 : tensor<8x1x32x32xbf16>
    %98 = stablehlo.compare  EQ, %97, %4 : (tensor<8x1x32x32xbf16>, tensor<8x1x32x32xbf16>) -> tensor<8x1x32x32xi1>
    %99 = stablehlo.select %98, %3, %92 : tensor<8x1x32x32xi1>, tensor<8x1x32x32xbf16>
    %100 = stablehlo.reshape %99 : (tensor<8x1x32x32xbf16>) -> tensor<8x32x32xbf16>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] : (tensor<8x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %102 = stablehlo.add %82, %101 : tensor<8x12x32x32xbf16>
    %103 = stablehlo.convert %102 : (tensor<8x12x32x32xbf16>) -> tensor<8x12x32x32xf32>
    %104 = stablehlo.reduce(%103 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %106 = stablehlo.subtract %103, %105 : tensor<8x12x32x32xf32>
    %107 = stablehlo.exponential %106 : tensor<8x12x32x32xf32>
    %108 = stablehlo.reduce(%107 init: %cst_2) applies stablehlo.add across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %110 = stablehlo.divide %107, %109 : tensor<8x12x32x32xf32>
    %111 = stablehlo.convert %110 : (tensor<8x12x32x32xf32>) -> tensor<8x12x32x32xbf16>
    %112 = stablehlo.reshape %111 : (tensor<8x12x32x32xbf16>) -> tensor<96x32x32xbf16>
    %113 = stablehlo.reshape %arg13 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %114 = stablehlo.reshape %113 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %115 = stablehlo.transpose %114, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %116 = stablehlo.dot_general %55, %115, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %117 = stablehlo.reshape %116 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %118 = stablehlo.reshape %arg12 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %121 = stablehlo.add %117, %120 : tensor<8x32x768xbf16>
    %122 = stablehlo.reshape %121 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %123 = stablehlo.transpose %122, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %124 = stablehlo.reshape %123 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %125 = stablehlo.dot_general %112, %124, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x32xbf16>, tensor<96x32x64xbf16>) -> tensor<96x32x64xbf16>
    %126 = stablehlo.reshape %125 : (tensor<96x32x64xbf16>) -> tensor<8x12x32x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,32,12,64]{3,1,2,0}"} : (tensor<8x12x32x64xbf16>) -> tensor<8x32x12x64xbf16>
    %128 = stablehlo.reshape %127 : (tensor<8x32x12x64xbf16>) -> tensor<256x768xbf16>
    %129 = stablehlo.reshape %arg11 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %131 = stablehlo.transpose %130, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %132 = stablehlo.dot_general %128, %131, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %133 = stablehlo.reshape %132 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %134 = stablehlo.reshape %arg10 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %135 = stablehlo.reshape %134 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %136 = stablehlo.broadcast_in_dim %135, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %137 = stablehlo.add %133, %136 : tensor<8x32x768xbf16>
    %138 = stablehlo.add %33, %137 : tensor<8x32x768xbf16>
    %139 = stablehlo.reshape %138 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %140 = stablehlo.reduce(%139 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %141 = stablehlo.multiply %140, %2 : tensor<256xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %143 = stablehlo.subtract %139, %142 : tensor<256x768xbf16>
    %144 = stablehlo.multiply %143, %143 : tensor<256x768xbf16>
    %145 = stablehlo.reduce(%144 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %146 = stablehlo.multiply %145, %2 : tensor<256xbf16>
    %147 = stablehlo.reshape %146 : (tensor<256xbf16>) -> tensor<256x1xbf16>
    %148 = stablehlo.add %147, %1 : tensor<256x1xbf16>
    %149 = stablehlo.rsqrt %148 : tensor<256x1xbf16>
    %150 = stablehlo.reshape %149 : (tensor<256x1xbf16>) -> tensor<256xbf16>
    %151 = stablehlo.broadcast_in_dim %150, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %152 = stablehlo.multiply %143, %151 : tensor<256x768xbf16>
    %153 = stablehlo.reshape %arg9 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %155 = stablehlo.broadcast_in_dim %154, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %156 = stablehlo.multiply %152, %155 : tensor<256x768xbf16>
    %157 = stablehlo.reshape %arg8 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %158 = stablehlo.reshape %157 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %159 = stablehlo.broadcast_in_dim %158, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %160 = stablehlo.add %156, %159 : tensor<256x768xbf16>
    %161 = stablehlo.reshape %arg7 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16>
    %162 = stablehlo.reshape %161 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16>
    %163 = stablehlo.transpose %162, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16>
    %164 = stablehlo.dot_general %160, %163, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x3072xbf16>) -> tensor<256x3072xbf16>
    %165 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %166 = stablehlo.reshape %165 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %167 = stablehlo.broadcast_in_dim %166, dims = [1] : (tensor<3072xbf16>) -> tensor<256x3072xbf16>
    %168 = stablehlo.add %164, %167 : tensor<256x3072xbf16>
    %169 = stablehlo.maximum %168, %0 : tensor<256x3072xbf16>
    %170 = stablehlo.reshape %arg5 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16>
    %172 = stablehlo.transpose %171, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16>
    %173 = stablehlo.dot_general %169, %172, contracting_dims = [1] x [0] : (tensor<256x3072xbf16>, tensor<3072x768xbf16>) -> tensor<256x768xbf16>
    %174 = stablehlo.reshape %arg4 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %175 = stablehlo.reshape %174 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %176 = stablehlo.broadcast_in_dim %175, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %177 = stablehlo.add %173, %176 : tensor<256x768xbf16>
    %178 = stablehlo.add %139, %177 : tensor<256x768xbf16>
    %179 = stablehlo.reshape %178 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %180 = stablehlo.reduce(%179 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %181 = stablehlo.multiply %180, %10 : tensor<8x32xbf16>
    %182 = stablehlo.broadcast_in_dim %181, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %183 = stablehlo.subtract %179, %182 : tensor<8x32x768xbf16>
    %184 = stablehlo.multiply %183, %183 : tensor<8x32x768xbf16>
    %185 = stablehlo.reduce(%184 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %186 = stablehlo.multiply %185, %10 : tensor<8x32xbf16>
    %187 = stablehlo.reshape %186 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %188 = stablehlo.add %187, %9 : tensor<8x32x1xbf16>
    %189 = stablehlo.rsqrt %188 : tensor<8x32x1xbf16>
    %190 = stablehlo.reshape %189 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %191 = stablehlo.broadcast_in_dim %190, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %192 = stablehlo.multiply %183, %191 : tensor<8x32x768xbf16>
    %193 = stablehlo.reshape %arg3 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %194 = stablehlo.reshape %193 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %195 = stablehlo.broadcast_in_dim %194, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %196 = stablehlo.multiply %192, %195 : tensor<8x32x768xbf16>
    %197 = stablehlo.reshape %arg2 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %198 = stablehlo.reshape %197 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %199 = stablehlo.broadcast_in_dim %198, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %200 = stablehlo.add %196, %199 : tensor<8x32x768xbf16>
    %201 = stablehlo.reshape %200 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %202 = stablehlo.reshape %arg1 : (tensor<2x768xbf16>) -> tensor<1x2x768xbf16>
    %203 = stablehlo.reshape %202 : (tensor<1x2x768xbf16>) -> tensor<2x768xbf16>
    %204 = stablehlo.transpose %203, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,2]{0,1}"} : (tensor<2x768xbf16>) -> tensor<768x2xbf16>
    %205 = stablehlo.dot_general %201, %204, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x2xbf16>) -> tensor<256x2xbf16>
    %206 = stablehlo.reshape %205 : (tensor<256x2xbf16>) -> tensor<8x32x2xbf16>
    %207 = stablehlo.broadcast_in_dim %c_3, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32>
    %208 = stablehlo.compare  NE, %16, %12 : (tensor<8x32xi64>, tensor<8x32xi64>) -> tensor<8x32xi1>
    %209 = stablehlo.convert %208 : (tensor<8x32xi1>) -> tensor<8x32xi32>
    %210 = stablehlo.multiply %207, %209 : tensor<8x32xi32>
    %211 = stablehlo.iota dim = 0 : tensor<32xi32>
    %212 = stablehlo.broadcast_in_dim %211, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32>
    %213:2 = stablehlo.reduce(%210 init: %c_4), (%212 init: %c_0) across dimensions = [1] : (tensor<8x32xi32>, tensor<8x32xi32>, tensor<i32>, tensor<i32>) -> (tensor<8xi32>, tensor<8xi32>)
     reducer(%arg23: tensor<i32>, %arg25: tensor<i32>) (%arg24: tensor<i32>, %arg26: tensor<i32>)  {
      %221 = stablehlo.compare  GE, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %222 = stablehlo.select %221, %arg23, %arg25 : tensor<i1>, tensor<i32>
      %223 = stablehlo.compare  EQ, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %224 = stablehlo.minimum %arg24, %arg26 : tensor<i32>
      %225 = stablehlo.select %221, %arg24, %arg26 : tensor<i1>, tensor<i32>
      %226 = stablehlo.select %223, %224, %225 : tensor<i1>, tensor<i32>
      stablehlo.return %222, %226 : tensor<i32>, tensor<i32>
    }
    %214 = stablehlo.convert %213#1 : (tensor<8xi32>) -> tensor<8xi64>
    %215 = stablehlo.compare  LT, %214, %c_12 : (tensor<8xi64>, tensor<8xi64>) -> tensor<8xi1>
    %216 = stablehlo.add %214, %c_11 : tensor<8xi64>
    %217 = stablehlo.select %215, %216, %214 : tensor<8xi1>, tensor<8xi64>
    %218 = stablehlo.reshape %217 : (tensor<8xi64>) -> tensor<8x1xi64>
    %219 = stablehlo.concatenate %c_13, %218, dim = 1 : (tensor<8x1xi64>, tensor<8x1xi64>) -> tensor<8x2xi64>
    %220 = "stablehlo.gather"(%206, %219) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1, 2>}> : (tensor<8x32x2xbf16>, tensor<8x2xi64>) -> tensor<8x2xbf16>
    return %220 : tensor<8x2xbf16>
  }
}


// -----// IR Dump Before ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.630) //----- //
module @SyncTensorsGraph.630 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_0"}, %arg1: tensor<2x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___score_weight"}, %arg2: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_final_layer_norm_bias"}, %arg3: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_final_layer_norm_weight"}, %arg4: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_fc2_bias"}, %arg5: tensor<768x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_fc2_weight"}, %arg6: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_fc1_bias"}, %arg7: tensor<3072x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_fc1_weight"}, %arg8: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_bias"}, %arg9: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_weight"}, %arg10: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_bias"}, %arg11: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_weight"}, %arg12: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_bias"}, %arg13: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_weight"}, %arg14: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_bias"}, %arg15: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_weight"}, %arg16: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "args_1"}, %arg17: tensor<2050x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_embed_positions_weight"}, %arg18: tensor<50272x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_embed_tokens_weight"}, %arg19: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_bias"}, %arg20: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_weight"}, %arg21: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_bias"}, %arg22: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_weight"}) -> tensor<8x2xbf16> {
    %c = stablehlo.constant dense<0> : tensor<i64>
    %c_0 = stablehlo.constant dense<0> : tensor<i32>
    %c_1 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64>
    %cst = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c_3 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi32>
    %c_4 = stablehlo.constant dense<-2147483648> : tensor<i32>
    %c_5 = stablehlo.constant dense<2> : tensor<i64>
    %cst_6 = stablehlo.constant dense<1.250000e-01> : tensor<bf16>
    %c_7 = stablehlo.constant dense<1> : tensor<i64>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %cst_9 = stablehlo.constant dense<1.304630e-03> : tensor<bf16>
    %cst_10 = stablehlo.constant dense<1.001360e-05> : tensor<bf16>
    %c_11 = stablehlo.constant dense<32> : tensor<8xi64>
    %c_12 = stablehlo.constant dense<0> : tensor<8xi64>
    %c_13 = stablehlo.constant dense<[[0], [1], [2], [3], [4], [5], [6], [7]]> : tensor<8x1xi64>
    %cst_14 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<256x3072xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<256x1xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<256xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %4 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %5 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %7 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<32x32xi64>
    %8 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<8x32x768xbf16>
    %9 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x32x1xbf16>
    %10 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<8x32xbf16>
    %11 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<8x32xi64>
    %12 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<8x32xi64>
    %13 = stablehlo.reshape %arg18 : (tensor<50272x768xbf16>) -> tensor<1x50272x768xbf16>
    %14 = stablehlo.reshape %13 : (tensor<1x50272x768xbf16>) -> tensor<50272x768xbf16>
    %15 = stablehlo.reshape %arg0 : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %16 = stablehlo.reshape %15 : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %17 = stablehlo.reshape %15 : (tensor<1x8x32xi64>) -> tensor<256xi64>
    %18 = stablehlo.convert %17 : (tensor<256xi64>) -> tensor<256xui32>
    %19 = "stablehlo.gather"(%14, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<50272x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %20 = stablehlo.reshape %19 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %21 = stablehlo.reshape %arg17 : (tensor<2050x768xbf16>) -> tensor<1x2050x768xbf16>
    %22 = stablehlo.reshape %21 : (tensor<1x2050x768xbf16>) -> tensor<2050x768xbf16>
    %23 = stablehlo.reshape %arg16 : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %24 = stablehlo.reshape %23 : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %25 = "stablehlo.reduce_window"(%24, %c) <{padding = dense<[[0, 0], [31, 0]]> : tensor<2x2xi64>, window_dimensions = array<i64: 1, 32>}> ({
    ^bb0(%arg23: tensor<i64>, %arg24: tensor<i64>):
      %221 = stablehlo.add %arg23, %arg24 : tensor<i64>
      stablehlo.return %221 : tensor<i64>
    }) : (tensor<8x32xi64>, tensor<i64>) -> tensor<8x32xi64>
    %26 = stablehlo.multiply %25, %24 : tensor<8x32xi64>
    %27 = stablehlo.subtract %26, %12 : tensor<8x32xi64>
    %28 = stablehlo.add %27, %11 : tensor<8x32xi64>
    %29 = stablehlo.reshape %28 : (tensor<8x32xi64>) -> tensor<256xi64>
    %30 = stablehlo.convert %29 : (tensor<256xi64>) -> tensor<256xui32>
    %31 = "stablehlo.gather"(%22, %30) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<2050x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %32 = stablehlo.reshape %31 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %33 = stablehlo.add %20, %32 : tensor<8x32x768xbf16>
    %34 = stablehlo.reduce(%33 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %35 = stablehlo.multiply %34, %10 : tensor<8x32xbf16>
    %36 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %37 = stablehlo.subtract %33, %36 : tensor<8x32x768xbf16>
    %38 = stablehlo.multiply %37, %37 : tensor<8x32x768xbf16>
    %39 = stablehlo.reduce(%38 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %40 = stablehlo.multiply %39, %10 : tensor<8x32xbf16>
    %41 = stablehlo.reshape %40 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %42 = stablehlo.add %41, %9 : tensor<8x32x1xbf16>
    %43 = stablehlo.rsqrt %42 : tensor<8x32x1xbf16>
    %44 = stablehlo.reshape %43 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %46 = stablehlo.multiply %37, %45 : tensor<8x32x768xbf16>
    %47 = stablehlo.reshape %arg15 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %48 = stablehlo.reshape %47 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %49 = stablehlo.broadcast_in_dim %48, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %50 = stablehlo.multiply %46, %49 : tensor<8x32x768xbf16>
    %51 = stablehlo.reshape %arg14 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %53 = stablehlo.broadcast_in_dim %52, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %54 = stablehlo.add %50, %53 : tensor<8x32x768xbf16>
    %55 = stablehlo.reshape %54 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %56 = stablehlo.reshape %arg22 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %57 = stablehlo.reshape %56 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %58 = stablehlo.transpose %57, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %59 = stablehlo.dot_general %55, %58, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %60 = stablehlo.reshape %59 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %61 = stablehlo.reshape %arg21 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %63 = stablehlo.broadcast_in_dim %62, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %64 = stablehlo.add %60, %63 : tensor<8x32x768xbf16>
    %65 = stablehlo.multiply %64, %8 : tensor<8x32x768xbf16>
    %66 = stablehlo.reshape %65 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %67 = stablehlo.transpose %66, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %68 = stablehlo.reshape %67 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %69 = stablehlo.reshape %arg20 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %70 = stablehlo.reshape %69 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %71 = stablehlo.transpose %70, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %72 = stablehlo.dot_general %55, %71, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %73 = stablehlo.reshape %72 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %74 = stablehlo.reshape %arg19 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %76 = stablehlo.broadcast_in_dim %75, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %77 = stablehlo.add %73, %76 : tensor<8x32x768xbf16>
    %78 = stablehlo.reshape %77 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 3, 1] : (tensor<8x32x12x64xbf16>) -> tensor<8x12x64x32xbf16>
    %80 = stablehlo.reshape %79 : (tensor<8x12x64x32xbf16>) -> tensor<96x64x32xbf16>
    %81 = stablehlo.dot_general %68, %80, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x64xbf16>, tensor<96x64x32xbf16>) -> tensor<96x32x32xbf16>
    %82 = stablehlo.reshape %81 : (tensor<96x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %83 = stablehlo.broadcast_in_dim %c_1, dims = [1] : (tensor<32xi64>) -> tensor<32x32xi64>
    %84 = stablehlo.broadcast_in_dim %c_1, dims = [0] : (tensor<32xi64>) -> tensor<32x32xi64>
    %85 = stablehlo.subtract %83, %84 : tensor<32x32xi64>
    %86 = stablehlo.compare  GE, %85, %7 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %87 = stablehlo.select %86, %6, %5 : tensor<32x32xi1>, tensor<32x32xbf16>
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %89 = stablehlo.convert %88 : (tensor<32x32xi1>) -> tensor<32x32xbf16>
    %90 = stablehlo.multiply %87, %89 : tensor<32x32xbf16>
    %91 = stablehlo.reshape %90 : (tensor<32x32xbf16>) -> tensor<1x32x32xbf16>
    %92 = stablehlo.broadcast_in_dim %91, dims = [1, 2, 3] : (tensor<1x32x32xbf16>) -> tensor<8x1x32x32xbf16>
    %93 = stablehlo.reshape %23 : (tensor<1x8x32xi64>) -> tensor<8x1x1x32xi64>
    %94 = stablehlo.convert %93 : (tensor<8x1x1x32xi64>) -> tensor<8x1x1x32xbf16>
    %95 = stablehlo.reshape %94 : (tensor<8x1x1x32xbf16>) -> tensor<8x1x32xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] : (tensor<8x1x32xbf16>) -> tensor<8x1x32x32xbf16>
    %97 = stablehlo.add %92, %96 : tensor<8x1x32x32xbf16>
    %98 = stablehlo.compare  EQ, %97, %4 : (tensor<8x1x32x32xbf16>, tensor<8x1x32x32xbf16>) -> tensor<8x1x32x32xi1>
    %99 = stablehlo.select %98, %3, %92 : tensor<8x1x32x32xi1>, tensor<8x1x32x32xbf16>
    %100 = stablehlo.reshape %99 : (tensor<8x1x32x32xbf16>) -> tensor<8x32x32xbf16>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] : (tensor<8x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %102 = stablehlo.add %82, %101 : tensor<8x12x32x32xbf16>
    %103 = stablehlo.convert %102 : (tensor<8x12x32x32xbf16>) -> tensor<8x12x32x32xf32>
    %104 = stablehlo.reduce(%103 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %106 = stablehlo.subtract %103, %105 : tensor<8x12x32x32xf32>
    %107 = stablehlo.exponential %106 : tensor<8x12x32x32xf32>
    %108 = stablehlo.reduce(%107 init: %cst_2) applies stablehlo.add across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %110 = stablehlo.divide %107, %109 : tensor<8x12x32x32xf32>
    %111 = stablehlo.convert %110 : (tensor<8x12x32x32xf32>) -> tensor<8x12x32x32xbf16>
    %112 = stablehlo.reshape %111 : (tensor<8x12x32x32xbf16>) -> tensor<96x32x32xbf16>
    %113 = stablehlo.reshape %arg13 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %114 = stablehlo.reshape %113 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %115 = stablehlo.transpose %114, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %116 = stablehlo.dot_general %55, %115, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %117 = stablehlo.reshape %116 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %118 = stablehlo.reshape %arg12 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %121 = stablehlo.add %117, %120 : tensor<8x32x768xbf16>
    %122 = stablehlo.reshape %121 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %123 = stablehlo.transpose %122, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %124 = stablehlo.reshape %123 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %125 = stablehlo.dot_general %112, %124, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x32xbf16>, tensor<96x32x64xbf16>) -> tensor<96x32x64xbf16>
    %126 = stablehlo.reshape %125 : (tensor<96x32x64xbf16>) -> tensor<8x12x32x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,32,12,64]{3,1,2,0}"} : (tensor<8x12x32x64xbf16>) -> tensor<8x32x12x64xbf16>
    %128 = stablehlo.reshape %127 : (tensor<8x32x12x64xbf16>) -> tensor<256x768xbf16>
    %129 = stablehlo.reshape %arg11 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %131 = stablehlo.transpose %130, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %132 = stablehlo.dot_general %128, %131, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %133 = stablehlo.reshape %132 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %134 = stablehlo.reshape %arg10 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %135 = stablehlo.reshape %134 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %136 = stablehlo.broadcast_in_dim %135, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %137 = stablehlo.add %133, %136 : tensor<8x32x768xbf16>
    %138 = stablehlo.add %33, %137 : tensor<8x32x768xbf16>
    %139 = stablehlo.reshape %138 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %140 = stablehlo.reduce(%139 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %141 = stablehlo.multiply %140, %2 : tensor<256xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %143 = stablehlo.subtract %139, %142 : tensor<256x768xbf16>
    %144 = stablehlo.multiply %143, %143 : tensor<256x768xbf16>
    %145 = stablehlo.reduce(%144 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %146 = stablehlo.multiply %145, %2 : tensor<256xbf16>
    %147 = stablehlo.reshape %146 : (tensor<256xbf16>) -> tensor<256x1xbf16>
    %148 = stablehlo.add %147, %1 : tensor<256x1xbf16>
    %149 = stablehlo.rsqrt %148 : tensor<256x1xbf16>
    %150 = stablehlo.reshape %149 : (tensor<256x1xbf16>) -> tensor<256xbf16>
    %151 = stablehlo.broadcast_in_dim %150, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %152 = stablehlo.multiply %143, %151 : tensor<256x768xbf16>
    %153 = stablehlo.reshape %arg9 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %155 = stablehlo.broadcast_in_dim %154, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %156 = stablehlo.multiply %152, %155 : tensor<256x768xbf16>
    %157 = stablehlo.reshape %arg8 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %158 = stablehlo.reshape %157 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %159 = stablehlo.broadcast_in_dim %158, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %160 = stablehlo.add %156, %159 : tensor<256x768xbf16>
    %161 = stablehlo.reshape %arg7 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16>
    %162 = stablehlo.reshape %161 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16>
    %163 = stablehlo.transpose %162, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16>
    %164 = stablehlo.dot_general %160, %163, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x3072xbf16>) -> tensor<256x3072xbf16>
    %165 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %166 = stablehlo.reshape %165 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %167 = stablehlo.broadcast_in_dim %166, dims = [1] : (tensor<3072xbf16>) -> tensor<256x3072xbf16>
    %168 = stablehlo.add %164, %167 : tensor<256x3072xbf16>
    %169 = stablehlo.maximum %168, %0 : tensor<256x3072xbf16>
    %170 = stablehlo.reshape %arg5 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16>
    %172 = stablehlo.transpose %171, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16>
    %173 = stablehlo.dot_general %169, %172, contracting_dims = [1] x [0] : (tensor<256x3072xbf16>, tensor<3072x768xbf16>) -> tensor<256x768xbf16>
    %174 = stablehlo.reshape %arg4 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %175 = stablehlo.reshape %174 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %176 = stablehlo.broadcast_in_dim %175, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %177 = stablehlo.add %173, %176 : tensor<256x768xbf16>
    %178 = stablehlo.add %139, %177 : tensor<256x768xbf16>
    %179 = stablehlo.reshape %178 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %180 = stablehlo.reduce(%179 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %181 = stablehlo.multiply %180, %10 : tensor<8x32xbf16>
    %182 = stablehlo.broadcast_in_dim %181, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %183 = stablehlo.subtract %179, %182 : tensor<8x32x768xbf16>
    %184 = stablehlo.multiply %183, %183 : tensor<8x32x768xbf16>
    %185 = stablehlo.reduce(%184 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %186 = stablehlo.multiply %185, %10 : tensor<8x32xbf16>
    %187 = stablehlo.reshape %186 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %188 = stablehlo.add %187, %9 : tensor<8x32x1xbf16>
    %189 = stablehlo.rsqrt %188 : tensor<8x32x1xbf16>
    %190 = stablehlo.reshape %189 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %191 = stablehlo.broadcast_in_dim %190, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %192 = stablehlo.multiply %183, %191 : tensor<8x32x768xbf16>
    %193 = stablehlo.reshape %arg3 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %194 = stablehlo.reshape %193 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %195 = stablehlo.broadcast_in_dim %194, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %196 = stablehlo.multiply %192, %195 : tensor<8x32x768xbf16>
    %197 = stablehlo.reshape %arg2 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %198 = stablehlo.reshape %197 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %199 = stablehlo.broadcast_in_dim %198, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %200 = stablehlo.add %196, %199 : tensor<8x32x768xbf16>
    %201 = stablehlo.reshape %200 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %202 = stablehlo.reshape %arg1 : (tensor<2x768xbf16>) -> tensor<1x2x768xbf16>
    %203 = stablehlo.reshape %202 : (tensor<1x2x768xbf16>) -> tensor<2x768xbf16>
    %204 = stablehlo.transpose %203, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,2]{0,1}"} : (tensor<2x768xbf16>) -> tensor<768x2xbf16>
    %205 = stablehlo.dot_general %201, %204, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x2xbf16>) -> tensor<256x2xbf16>
    %206 = stablehlo.reshape %205 : (tensor<256x2xbf16>) -> tensor<8x32x2xbf16>
    %207 = stablehlo.broadcast_in_dim %c_3, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32>
    %208 = stablehlo.compare  NE, %16, %12 : (tensor<8x32xi64>, tensor<8x32xi64>) -> tensor<8x32xi1>
    %209 = stablehlo.convert %208 : (tensor<8x32xi1>) -> tensor<8x32xi32>
    %210 = stablehlo.multiply %207, %209 : tensor<8x32xi32>
    %211 = stablehlo.iota dim = 0 : tensor<32xi32>
    %212 = stablehlo.broadcast_in_dim %211, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32>
    %213:2 = stablehlo.reduce(%210 init: %c_4), (%212 init: %c_0) across dimensions = [1] : (tensor<8x32xi32>, tensor<8x32xi32>, tensor<i32>, tensor<i32>) -> (tensor<8xi32>, tensor<8xi32>)
     reducer(%arg23: tensor<i32>, %arg25: tensor<i32>) (%arg24: tensor<i32>, %arg26: tensor<i32>)  {
      %221 = stablehlo.compare  GE, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %222 = stablehlo.select %221, %arg23, %arg25 : tensor<i1>, tensor<i32>
      %223 = stablehlo.compare  EQ, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %224 = stablehlo.minimum %arg24, %arg26 : tensor<i32>
      %225 = stablehlo.select %221, %arg24, %arg26 : tensor<i1>, tensor<i32>
      %226 = stablehlo.select %223, %224, %225 : tensor<i1>, tensor<i32>
      stablehlo.return %222, %226 : tensor<i32>, tensor<i32>
    }
    %214 = stablehlo.convert %213#1 : (tensor<8xi32>) -> tensor<8xi64>
    %215 = stablehlo.compare  LT, %214, %c_12 : (tensor<8xi64>, tensor<8xi64>) -> tensor<8xi1>
    %216 = stablehlo.add %214, %c_11 : tensor<8xi64>
    %217 = stablehlo.select %215, %216, %214 : tensor<8xi1>, tensor<8xi64>
    %218 = stablehlo.reshape %217 : (tensor<8xi64>) -> tensor<8x1xi64>
    %219 = stablehlo.concatenate %c_13, %218, dim = 1 : (tensor<8x1xi64>, tensor<8x1xi64>) -> tensor<8x2xi64>
    %220 = "stablehlo.gather"(%206, %219) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1, 2>}> : (tensor<8x32x2xbf16>, tensor<8x2xi64>) -> tensor<8x2xbf16>
    return %220 : tensor<8x2xbf16>
  }
}


// -----// IR Dump After ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @SyncTensorsGraph.630) //----- //
module @SyncTensorsGraph.630 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<2x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___score_weight"}, %arg2: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_final_layer_norm_bias"}, %arg3: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_final_layer_norm_weight"}, %arg4: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc2_bias"}, %arg5: tensor<768x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc2_weight"}, %arg6: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc1_bias"}, %arg7: tensor<3072x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc1_weight"}, %arg8: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_bias"}, %arg9: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_weight"}, %arg10: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_bias"}, %arg11: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_weight"}, %arg12: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_bias"}, %arg13: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_weight"}, %arg14: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_bias"}, %arg15: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_weight"}, %arg16: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg17: tensor<2050x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_embed_positions_weight"}, %arg18: tensor<50272x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_embed_tokens_weight"}, %arg19: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_bias"}, %arg20: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_weight"}, %arg21: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_bias"}, %arg22: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_weight"}) -> (tensor<8x2xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %c = stablehlo.constant dense<0> : tensor<i64>
    %c_0 = stablehlo.constant dense<0> : tensor<i32>
    %c_1 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64>
    %cst = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c_3 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi32>
    %c_4 = stablehlo.constant dense<-2147483648> : tensor<i32>
    %c_5 = stablehlo.constant dense<2> : tensor<i64>
    %cst_6 = stablehlo.constant dense<1.250000e-01> : tensor<bf16>
    %c_7 = stablehlo.constant dense<1> : tensor<i64>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %cst_9 = stablehlo.constant dense<1.304630e-03> : tensor<bf16>
    %cst_10 = stablehlo.constant dense<1.001360e-05> : tensor<bf16>
    %c_11 = stablehlo.constant dense<32> : tensor<8xi64>
    %c_12 = stablehlo.constant dense<0> : tensor<8xi64>
    %c_13 = stablehlo.constant dense<[[0], [1], [2], [3], [4], [5], [6], [7]]> : tensor<8x1xi64>
    %cst_14 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<256x3072xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<256x1xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<256xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %4 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %5 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %7 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<32x32xi64>
    %8 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<8x32x768xbf16>
    %9 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x32x1xbf16>
    %10 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<8x32xbf16>
    %11 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<8x32xi64>
    %12 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<8x32xi64>
    %13 = stablehlo.reshape %arg18 : (tensor<50272x768xbf16>) -> tensor<1x50272x768xbf16>
    %14 = stablehlo.reshape %13 : (tensor<1x50272x768xbf16>) -> tensor<50272x768xbf16>
    %15 = stablehlo.reshape %arg0 : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %16 = stablehlo.reshape %15 : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %17 = stablehlo.reshape %15 : (tensor<1x8x32xi64>) -> tensor<256xi64>
    %18 = stablehlo.convert %17 : (tensor<256xi64>) -> tensor<256xui32>
    %19 = "stablehlo.gather"(%14, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<50272x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %20 = stablehlo.reshape %19 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %21 = stablehlo.reshape %arg17 : (tensor<2050x768xbf16>) -> tensor<1x2050x768xbf16>
    %22 = stablehlo.reshape %21 : (tensor<1x2050x768xbf16>) -> tensor<2050x768xbf16>
    %23 = stablehlo.reshape %arg16 : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %24 = stablehlo.reshape %23 : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %25 = "stablehlo.reduce_window"(%24, %c) <{padding = dense<[[0, 0], [31, 0]]> : tensor<2x2xi64>, window_dimensions = array<i64: 1, 32>}> ({
    ^bb0(%arg23: tensor<i64>, %arg24: tensor<i64>):
      %221 = stablehlo.add %arg23, %arg24 : tensor<i64>
      stablehlo.return %221 : tensor<i64>
    }) : (tensor<8x32xi64>, tensor<i64>) -> tensor<8x32xi64>
    %26 = stablehlo.multiply %25, %24 : tensor<8x32xi64>
    %27 = stablehlo.subtract %26, %12 : tensor<8x32xi64>
    %28 = stablehlo.add %27, %11 : tensor<8x32xi64>
    %29 = stablehlo.reshape %28 : (tensor<8x32xi64>) -> tensor<256xi64>
    %30 = stablehlo.convert %29 : (tensor<256xi64>) -> tensor<256xui32>
    %31 = "stablehlo.gather"(%22, %30) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<2050x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %32 = stablehlo.reshape %31 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %33 = stablehlo.add %20, %32 : tensor<8x32x768xbf16>
    %34 = stablehlo.reduce(%33 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %35 = stablehlo.multiply %34, %10 : tensor<8x32xbf16>
    %36 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %37 = stablehlo.subtract %33, %36 : tensor<8x32x768xbf16>
    %38 = stablehlo.multiply %37, %37 : tensor<8x32x768xbf16>
    %39 = stablehlo.reduce(%38 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %40 = stablehlo.multiply %39, %10 : tensor<8x32xbf16>
    %41 = stablehlo.reshape %40 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %42 = stablehlo.add %41, %9 : tensor<8x32x1xbf16>
    %43 = stablehlo.rsqrt %42 : tensor<8x32x1xbf16>
    %44 = stablehlo.reshape %43 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %46 = stablehlo.multiply %37, %45 : tensor<8x32x768xbf16>
    %47 = stablehlo.reshape %arg15 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %48 = stablehlo.reshape %47 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %49 = stablehlo.broadcast_in_dim %48, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %50 = stablehlo.multiply %46, %49 : tensor<8x32x768xbf16>
    %51 = stablehlo.reshape %arg14 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %53 = stablehlo.broadcast_in_dim %52, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %54 = stablehlo.add %50, %53 : tensor<8x32x768xbf16>
    %55 = stablehlo.reshape %54 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %56 = stablehlo.reshape %arg22 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %57 = stablehlo.reshape %56 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %58 = stablehlo.transpose %57, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %59 = stablehlo.dot_general %55, %58, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %60 = stablehlo.reshape %59 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %61 = stablehlo.reshape %arg21 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %63 = stablehlo.broadcast_in_dim %62, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %64 = stablehlo.add %60, %63 : tensor<8x32x768xbf16>
    %65 = stablehlo.multiply %64, %8 : tensor<8x32x768xbf16>
    %66 = stablehlo.reshape %65 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %67 = stablehlo.transpose %66, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %68 = stablehlo.reshape %67 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %69 = stablehlo.reshape %arg20 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %70 = stablehlo.reshape %69 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %71 = stablehlo.transpose %70, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %72 = stablehlo.dot_general %55, %71, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %73 = stablehlo.reshape %72 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %74 = stablehlo.reshape %arg19 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %76 = stablehlo.broadcast_in_dim %75, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %77 = stablehlo.add %73, %76 : tensor<8x32x768xbf16>
    %78 = stablehlo.reshape %77 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 3, 1] : (tensor<8x32x12x64xbf16>) -> tensor<8x12x64x32xbf16>
    %80 = stablehlo.reshape %79 : (tensor<8x12x64x32xbf16>) -> tensor<96x64x32xbf16>
    %81 = stablehlo.dot_general %68, %80, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x64xbf16>, tensor<96x64x32xbf16>) -> tensor<96x32x32xbf16>
    %82 = stablehlo.reshape %81 : (tensor<96x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %83 = stablehlo.broadcast_in_dim %c_1, dims = [1] : (tensor<32xi64>) -> tensor<32x32xi64>
    %84 = stablehlo.broadcast_in_dim %c_1, dims = [0] : (tensor<32xi64>) -> tensor<32x32xi64>
    %85 = stablehlo.subtract %83, %84 : tensor<32x32xi64>
    %86 = stablehlo.compare  GE, %85, %7 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %87 = stablehlo.select %86, %6, %5 : tensor<32x32xi1>, tensor<32x32xbf16>
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %89 = stablehlo.convert %88 : (tensor<32x32xi1>) -> tensor<32x32xbf16>
    %90 = stablehlo.multiply %87, %89 : tensor<32x32xbf16>
    %91 = stablehlo.reshape %90 : (tensor<32x32xbf16>) -> tensor<1x32x32xbf16>
    %92 = stablehlo.broadcast_in_dim %91, dims = [1, 2, 3] : (tensor<1x32x32xbf16>) -> tensor<8x1x32x32xbf16>
    %93 = stablehlo.reshape %23 : (tensor<1x8x32xi64>) -> tensor<8x1x1x32xi64>
    %94 = stablehlo.convert %93 : (tensor<8x1x1x32xi64>) -> tensor<8x1x1x32xbf16>
    %95 = stablehlo.reshape %94 : (tensor<8x1x1x32xbf16>) -> tensor<8x1x32xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] : (tensor<8x1x32xbf16>) -> tensor<8x1x32x32xbf16>
    %97 = stablehlo.add %92, %96 : tensor<8x1x32x32xbf16>
    %98 = stablehlo.compare  EQ, %97, %4 : (tensor<8x1x32x32xbf16>, tensor<8x1x32x32xbf16>) -> tensor<8x1x32x32xi1>
    %99 = stablehlo.select %98, %3, %92 : tensor<8x1x32x32xi1>, tensor<8x1x32x32xbf16>
    %100 = stablehlo.reshape %99 : (tensor<8x1x32x32xbf16>) -> tensor<8x32x32xbf16>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] : (tensor<8x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %102 = stablehlo.add %82, %101 : tensor<8x12x32x32xbf16>
    %103 = stablehlo.convert %102 : (tensor<8x12x32x32xbf16>) -> tensor<8x12x32x32xf32>
    %104 = stablehlo.reduce(%103 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %106 = stablehlo.subtract %103, %105 : tensor<8x12x32x32xf32>
    %107 = stablehlo.exponential %106 : tensor<8x12x32x32xf32>
    %108 = stablehlo.reduce(%107 init: %cst_2) applies stablehlo.add across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %110 = stablehlo.divide %107, %109 : tensor<8x12x32x32xf32>
    %111 = stablehlo.convert %110 : (tensor<8x12x32x32xf32>) -> tensor<8x12x32x32xbf16>
    %112 = stablehlo.reshape %111 : (tensor<8x12x32x32xbf16>) -> tensor<96x32x32xbf16>
    %113 = stablehlo.reshape %arg13 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %114 = stablehlo.reshape %113 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %115 = stablehlo.transpose %114, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %116 = stablehlo.dot_general %55, %115, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %117 = stablehlo.reshape %116 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %118 = stablehlo.reshape %arg12 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %121 = stablehlo.add %117, %120 : tensor<8x32x768xbf16>
    %122 = stablehlo.reshape %121 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %123 = stablehlo.transpose %122, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %124 = stablehlo.reshape %123 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %125 = stablehlo.dot_general %112, %124, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x32xbf16>, tensor<96x32x64xbf16>) -> tensor<96x32x64xbf16>
    %126 = stablehlo.reshape %125 : (tensor<96x32x64xbf16>) -> tensor<8x12x32x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,32,12,64]{3,1,2,0}"} : (tensor<8x12x32x64xbf16>) -> tensor<8x32x12x64xbf16>
    %128 = stablehlo.reshape %127 : (tensor<8x32x12x64xbf16>) -> tensor<256x768xbf16>
    %129 = stablehlo.reshape %arg11 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %131 = stablehlo.transpose %130, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %132 = stablehlo.dot_general %128, %131, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %133 = stablehlo.reshape %132 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %134 = stablehlo.reshape %arg10 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %135 = stablehlo.reshape %134 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %136 = stablehlo.broadcast_in_dim %135, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %137 = stablehlo.add %133, %136 : tensor<8x32x768xbf16>
    %138 = stablehlo.add %33, %137 : tensor<8x32x768xbf16>
    %139 = stablehlo.reshape %138 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %140 = stablehlo.reduce(%139 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %141 = stablehlo.multiply %140, %2 : tensor<256xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %143 = stablehlo.subtract %139, %142 : tensor<256x768xbf16>
    %144 = stablehlo.multiply %143, %143 : tensor<256x768xbf16>
    %145 = stablehlo.reduce(%144 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %146 = stablehlo.multiply %145, %2 : tensor<256xbf16>
    %147 = stablehlo.reshape %146 : (tensor<256xbf16>) -> tensor<256x1xbf16>
    %148 = stablehlo.add %147, %1 : tensor<256x1xbf16>
    %149 = stablehlo.rsqrt %148 : tensor<256x1xbf16>
    %150 = stablehlo.reshape %149 : (tensor<256x1xbf16>) -> tensor<256xbf16>
    %151 = stablehlo.broadcast_in_dim %150, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %152 = stablehlo.multiply %143, %151 : tensor<256x768xbf16>
    %153 = stablehlo.reshape %arg9 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %155 = stablehlo.broadcast_in_dim %154, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %156 = stablehlo.multiply %152, %155 : tensor<256x768xbf16>
    %157 = stablehlo.reshape %arg8 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %158 = stablehlo.reshape %157 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %159 = stablehlo.broadcast_in_dim %158, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %160 = stablehlo.add %156, %159 : tensor<256x768xbf16>
    %161 = stablehlo.reshape %arg7 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16>
    %162 = stablehlo.reshape %161 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16>
    %163 = stablehlo.transpose %162, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16>
    %164 = stablehlo.dot_general %160, %163, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x3072xbf16>) -> tensor<256x3072xbf16>
    %165 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %166 = stablehlo.reshape %165 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %167 = stablehlo.broadcast_in_dim %166, dims = [1] : (tensor<3072xbf16>) -> tensor<256x3072xbf16>
    %168 = stablehlo.add %164, %167 : tensor<256x3072xbf16>
    %169 = stablehlo.maximum %168, %0 : tensor<256x3072xbf16>
    %170 = stablehlo.reshape %arg5 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16>
    %172 = stablehlo.transpose %171, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16>
    %173 = stablehlo.dot_general %169, %172, contracting_dims = [1] x [0] : (tensor<256x3072xbf16>, tensor<3072x768xbf16>) -> tensor<256x768xbf16>
    %174 = stablehlo.reshape %arg4 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %175 = stablehlo.reshape %174 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %176 = stablehlo.broadcast_in_dim %175, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %177 = stablehlo.add %173, %176 : tensor<256x768xbf16>
    %178 = stablehlo.add %139, %177 : tensor<256x768xbf16>
    %179 = stablehlo.reshape %178 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %180 = stablehlo.reduce(%179 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %181 = stablehlo.multiply %180, %10 : tensor<8x32xbf16>
    %182 = stablehlo.broadcast_in_dim %181, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %183 = stablehlo.subtract %179, %182 : tensor<8x32x768xbf16>
    %184 = stablehlo.multiply %183, %183 : tensor<8x32x768xbf16>
    %185 = stablehlo.reduce(%184 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %186 = stablehlo.multiply %185, %10 : tensor<8x32xbf16>
    %187 = stablehlo.reshape %186 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %188 = stablehlo.add %187, %9 : tensor<8x32x1xbf16>
    %189 = stablehlo.rsqrt %188 : tensor<8x32x1xbf16>
    %190 = stablehlo.reshape %189 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %191 = stablehlo.broadcast_in_dim %190, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %192 = stablehlo.multiply %183, %191 : tensor<8x32x768xbf16>
    %193 = stablehlo.reshape %arg3 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %194 = stablehlo.reshape %193 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %195 = stablehlo.broadcast_in_dim %194, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %196 = stablehlo.multiply %192, %195 : tensor<8x32x768xbf16>
    %197 = stablehlo.reshape %arg2 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %198 = stablehlo.reshape %197 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %199 = stablehlo.broadcast_in_dim %198, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %200 = stablehlo.add %196, %199 : tensor<8x32x768xbf16>
    %201 = stablehlo.reshape %200 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %202 = stablehlo.reshape %arg1 : (tensor<2x768xbf16>) -> tensor<1x2x768xbf16>
    %203 = stablehlo.reshape %202 : (tensor<1x2x768xbf16>) -> tensor<2x768xbf16>
    %204 = stablehlo.transpose %203, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,2]{0,1}"} : (tensor<2x768xbf16>) -> tensor<768x2xbf16>
    %205 = stablehlo.dot_general %201, %204, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x2xbf16>) -> tensor<256x2xbf16>
    %206 = stablehlo.reshape %205 : (tensor<256x2xbf16>) -> tensor<8x32x2xbf16>
    %207 = stablehlo.broadcast_in_dim %c_3, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32>
    %208 = stablehlo.compare  NE, %16, %12 : (tensor<8x32xi64>, tensor<8x32xi64>) -> tensor<8x32xi1>
    %209 = stablehlo.convert %208 : (tensor<8x32xi1>) -> tensor<8x32xi32>
    %210 = stablehlo.multiply %207, %209 : tensor<8x32xi32>
    %211 = stablehlo.iota dim = 0 : tensor<32xi32>
    %212 = stablehlo.broadcast_in_dim %211, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32>
    %213:2 = stablehlo.reduce(%210 init: %c_4), (%212 init: %c_0) across dimensions = [1] : (tensor<8x32xi32>, tensor<8x32xi32>, tensor<i32>, tensor<i32>) -> (tensor<8xi32>, tensor<8xi32>)
     reducer(%arg23: tensor<i32>, %arg25: tensor<i32>) (%arg24: tensor<i32>, %arg26: tensor<i32>)  {
      %221 = stablehlo.compare  GE, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %222 = stablehlo.select %221, %arg23, %arg25 : tensor<i1>, tensor<i32>
      %223 = stablehlo.compare  EQ, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %224 = stablehlo.minimum %arg24, %arg26 : tensor<i32>
      %225 = stablehlo.select %221, %arg24, %arg26 : tensor<i1>, tensor<i32>
      %226 = stablehlo.select %223, %224, %225 : tensor<i1>, tensor<i32>
      stablehlo.return %222, %226 : tensor<i32>, tensor<i32>
    }
    %214 = stablehlo.convert %213#1 : (tensor<8xi32>) -> tensor<8xi64>
    %215 = stablehlo.compare  LT, %214, %c_12 : (tensor<8xi64>, tensor<8xi64>) -> tensor<8xi1>
    %216 = stablehlo.add %214, %c_11 : tensor<8xi64>
    %217 = stablehlo.select %215, %216, %214 : tensor<8xi1>, tensor<8xi64>
    %218 = stablehlo.reshape %217 : (tensor<8xi64>) -> tensor<8x1xi64>
    %219 = stablehlo.concatenate %c_13, %218, dim = 1 : (tensor<8x1xi64>, tensor<8x1xi64>) -> tensor<8x2xi64>
    %220 = "stablehlo.gather"(%206, %219) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1, 2>}> : (tensor<8x32x2xbf16>, tensor<8x2xi64>) -> tensor<8x2xbf16>
    return %220 : tensor<8x2xbf16>
  }
}


// -----// IR Dump Before ConvertXlaSdyToSdyPass (convert-xla-sdy-to-sdy) ('builtin.module' operation: @SyncTensorsGraph.630) //----- //
module @SyncTensorsGraph.630 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<2x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___score_weight"}, %arg2: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_final_layer_norm_bias"}, %arg3: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_final_layer_norm_weight"}, %arg4: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc2_bias"}, %arg5: tensor<768x3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc2_weight"}, %arg6: tensor<3072xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc1_bias"}, %arg7: tensor<3072x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc1_weight"}, %arg8: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_bias"}, %arg9: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_weight"}, %arg10: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_bias"}, %arg11: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_weight"}, %arg12: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_bias"}, %arg13: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_weight"}, %arg14: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_bias"}, %arg15: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_weight"}, %arg16: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg17: tensor<2050x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_embed_positions_weight"}, %arg18: tensor<50272x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_embed_tokens_weight"}, %arg19: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_bias"}, %arg20: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_weight"}, %arg21: tensor<768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_bias"}, %arg22: tensor<768x768xbf16> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{}, {}]>"}, mhlo.sharding = "{replicated}", ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_weight"}) -> (tensor<8x2xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %c = stablehlo.constant dense<0> : tensor<i64>
    %c_0 = stablehlo.constant dense<0> : tensor<i32>
    %c_1 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64>
    %cst = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c_3 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi32>
    %c_4 = stablehlo.constant dense<-2147483648> : tensor<i32>
    %c_5 = stablehlo.constant dense<2> : tensor<i64>
    %cst_6 = stablehlo.constant dense<1.250000e-01> : tensor<bf16>
    %c_7 = stablehlo.constant dense<1> : tensor<i64>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %cst_9 = stablehlo.constant dense<1.304630e-03> : tensor<bf16>
    %cst_10 = stablehlo.constant dense<1.001360e-05> : tensor<bf16>
    %c_11 = stablehlo.constant dense<32> : tensor<8xi64>
    %c_12 = stablehlo.constant dense<0> : tensor<8xi64>
    %c_13 = stablehlo.constant dense<[[0], [1], [2], [3], [4], [5], [6], [7]]> : tensor<8x1xi64>
    %cst_14 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<256x3072xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<256x1xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<256xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %4 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %5 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %7 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<32x32xi64>
    %8 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<8x32x768xbf16>
    %9 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x32x1xbf16>
    %10 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<8x32xbf16>
    %11 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<8x32xi64>
    %12 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<8x32xi64>
    %13 = stablehlo.reshape %arg18 : (tensor<50272x768xbf16>) -> tensor<1x50272x768xbf16>
    %14 = stablehlo.reshape %13 : (tensor<1x50272x768xbf16>) -> tensor<50272x768xbf16>
    %15 = stablehlo.reshape %arg0 : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %16 = stablehlo.reshape %15 : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %17 = stablehlo.reshape %15 : (tensor<1x8x32xi64>) -> tensor<256xi64>
    %18 = stablehlo.convert %17 : (tensor<256xi64>) -> tensor<256xui32>
    %19 = "stablehlo.gather"(%14, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<50272x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %20 = stablehlo.reshape %19 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %21 = stablehlo.reshape %arg17 : (tensor<2050x768xbf16>) -> tensor<1x2050x768xbf16>
    %22 = stablehlo.reshape %21 : (tensor<1x2050x768xbf16>) -> tensor<2050x768xbf16>
    %23 = stablehlo.reshape %arg16 : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %24 = stablehlo.reshape %23 : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %25 = "stablehlo.reduce_window"(%24, %c) <{padding = dense<[[0, 0], [31, 0]]> : tensor<2x2xi64>, window_dimensions = array<i64: 1, 32>}> ({
    ^bb0(%arg23: tensor<i64>, %arg24: tensor<i64>):
      %221 = stablehlo.add %arg23, %arg24 : tensor<i64>
      stablehlo.return %221 : tensor<i64>
    }) : (tensor<8x32xi64>, tensor<i64>) -> tensor<8x32xi64>
    %26 = stablehlo.multiply %25, %24 : tensor<8x32xi64>
    %27 = stablehlo.subtract %26, %12 : tensor<8x32xi64>
    %28 = stablehlo.add %27, %11 : tensor<8x32xi64>
    %29 = stablehlo.reshape %28 : (tensor<8x32xi64>) -> tensor<256xi64>
    %30 = stablehlo.convert %29 : (tensor<256xi64>) -> tensor<256xui32>
    %31 = "stablehlo.gather"(%22, %30) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<2050x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %32 = stablehlo.reshape %31 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %33 = stablehlo.add %20, %32 : tensor<8x32x768xbf16>
    %34 = stablehlo.reduce(%33 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %35 = stablehlo.multiply %34, %10 : tensor<8x32xbf16>
    %36 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %37 = stablehlo.subtract %33, %36 : tensor<8x32x768xbf16>
    %38 = stablehlo.multiply %37, %37 : tensor<8x32x768xbf16>
    %39 = stablehlo.reduce(%38 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %40 = stablehlo.multiply %39, %10 : tensor<8x32xbf16>
    %41 = stablehlo.reshape %40 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %42 = stablehlo.add %41, %9 : tensor<8x32x1xbf16>
    %43 = stablehlo.rsqrt %42 : tensor<8x32x1xbf16>
    %44 = stablehlo.reshape %43 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %46 = stablehlo.multiply %37, %45 : tensor<8x32x768xbf16>
    %47 = stablehlo.reshape %arg15 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %48 = stablehlo.reshape %47 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %49 = stablehlo.broadcast_in_dim %48, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %50 = stablehlo.multiply %46, %49 : tensor<8x32x768xbf16>
    %51 = stablehlo.reshape %arg14 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %53 = stablehlo.broadcast_in_dim %52, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %54 = stablehlo.add %50, %53 : tensor<8x32x768xbf16>
    %55 = stablehlo.reshape %54 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %56 = stablehlo.reshape %arg22 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %57 = stablehlo.reshape %56 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %58 = stablehlo.transpose %57, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %59 = stablehlo.dot_general %55, %58, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %60 = stablehlo.reshape %59 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %61 = stablehlo.reshape %arg21 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %63 = stablehlo.broadcast_in_dim %62, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %64 = stablehlo.add %60, %63 : tensor<8x32x768xbf16>
    %65 = stablehlo.multiply %64, %8 : tensor<8x32x768xbf16>
    %66 = stablehlo.reshape %65 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %67 = stablehlo.transpose %66, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %68 = stablehlo.reshape %67 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %69 = stablehlo.reshape %arg20 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %70 = stablehlo.reshape %69 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %71 = stablehlo.transpose %70, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %72 = stablehlo.dot_general %55, %71, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %73 = stablehlo.reshape %72 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %74 = stablehlo.reshape %arg19 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %76 = stablehlo.broadcast_in_dim %75, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %77 = stablehlo.add %73, %76 : tensor<8x32x768xbf16>
    %78 = stablehlo.reshape %77 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 3, 1] : (tensor<8x32x12x64xbf16>) -> tensor<8x12x64x32xbf16>
    %80 = stablehlo.reshape %79 : (tensor<8x12x64x32xbf16>) -> tensor<96x64x32xbf16>
    %81 = stablehlo.dot_general %68, %80, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x64xbf16>, tensor<96x64x32xbf16>) -> tensor<96x32x32xbf16>
    %82 = stablehlo.reshape %81 : (tensor<96x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %83 = stablehlo.broadcast_in_dim %c_1, dims = [1] : (tensor<32xi64>) -> tensor<32x32xi64>
    %84 = stablehlo.broadcast_in_dim %c_1, dims = [0] : (tensor<32xi64>) -> tensor<32x32xi64>
    %85 = stablehlo.subtract %83, %84 : tensor<32x32xi64>
    %86 = stablehlo.compare  GE, %85, %7 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %87 = stablehlo.select %86, %6, %5 : tensor<32x32xi1>, tensor<32x32xbf16>
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %89 = stablehlo.convert %88 : (tensor<32x32xi1>) -> tensor<32x32xbf16>
    %90 = stablehlo.multiply %87, %89 : tensor<32x32xbf16>
    %91 = stablehlo.reshape %90 : (tensor<32x32xbf16>) -> tensor<1x32x32xbf16>
    %92 = stablehlo.broadcast_in_dim %91, dims = [1, 2, 3] : (tensor<1x32x32xbf16>) -> tensor<8x1x32x32xbf16>
    %93 = stablehlo.reshape %23 : (tensor<1x8x32xi64>) -> tensor<8x1x1x32xi64>
    %94 = stablehlo.convert %93 : (tensor<8x1x1x32xi64>) -> tensor<8x1x1x32xbf16>
    %95 = stablehlo.reshape %94 : (tensor<8x1x1x32xbf16>) -> tensor<8x1x32xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] : (tensor<8x1x32xbf16>) -> tensor<8x1x32x32xbf16>
    %97 = stablehlo.add %92, %96 : tensor<8x1x32x32xbf16>
    %98 = stablehlo.compare  EQ, %97, %4 : (tensor<8x1x32x32xbf16>, tensor<8x1x32x32xbf16>) -> tensor<8x1x32x32xi1>
    %99 = stablehlo.select %98, %3, %92 : tensor<8x1x32x32xi1>, tensor<8x1x32x32xbf16>
    %100 = stablehlo.reshape %99 : (tensor<8x1x32x32xbf16>) -> tensor<8x32x32xbf16>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] : (tensor<8x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %102 = stablehlo.add %82, %101 : tensor<8x12x32x32xbf16>
    %103 = stablehlo.convert %102 : (tensor<8x12x32x32xbf16>) -> tensor<8x12x32x32xf32>
    %104 = stablehlo.reduce(%103 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %106 = stablehlo.subtract %103, %105 : tensor<8x12x32x32xf32>
    %107 = stablehlo.exponential %106 : tensor<8x12x32x32xf32>
    %108 = stablehlo.reduce(%107 init: %cst_2) applies stablehlo.add across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %110 = stablehlo.divide %107, %109 : tensor<8x12x32x32xf32>
    %111 = stablehlo.convert %110 : (tensor<8x12x32x32xf32>) -> tensor<8x12x32x32xbf16>
    %112 = stablehlo.reshape %111 : (tensor<8x12x32x32xbf16>) -> tensor<96x32x32xbf16>
    %113 = stablehlo.reshape %arg13 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %114 = stablehlo.reshape %113 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %115 = stablehlo.transpose %114, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %116 = stablehlo.dot_general %55, %115, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %117 = stablehlo.reshape %116 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %118 = stablehlo.reshape %arg12 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %121 = stablehlo.add %117, %120 : tensor<8x32x768xbf16>
    %122 = stablehlo.reshape %121 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %123 = stablehlo.transpose %122, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %124 = stablehlo.reshape %123 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %125 = stablehlo.dot_general %112, %124, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x32xbf16>, tensor<96x32x64xbf16>) -> tensor<96x32x64xbf16>
    %126 = stablehlo.reshape %125 : (tensor<96x32x64xbf16>) -> tensor<8x12x32x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,32,12,64]{3,1,2,0}"} : (tensor<8x12x32x64xbf16>) -> tensor<8x32x12x64xbf16>
    %128 = stablehlo.reshape %127 : (tensor<8x32x12x64xbf16>) -> tensor<256x768xbf16>
    %129 = stablehlo.reshape %arg11 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %131 = stablehlo.transpose %130, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %132 = stablehlo.dot_general %128, %131, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %133 = stablehlo.reshape %132 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %134 = stablehlo.reshape %arg10 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %135 = stablehlo.reshape %134 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %136 = stablehlo.broadcast_in_dim %135, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %137 = stablehlo.add %133, %136 : tensor<8x32x768xbf16>
    %138 = stablehlo.add %33, %137 : tensor<8x32x768xbf16>
    %139 = stablehlo.reshape %138 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %140 = stablehlo.reduce(%139 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %141 = stablehlo.multiply %140, %2 : tensor<256xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %143 = stablehlo.subtract %139, %142 : tensor<256x768xbf16>
    %144 = stablehlo.multiply %143, %143 : tensor<256x768xbf16>
    %145 = stablehlo.reduce(%144 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %146 = stablehlo.multiply %145, %2 : tensor<256xbf16>
    %147 = stablehlo.reshape %146 : (tensor<256xbf16>) -> tensor<256x1xbf16>
    %148 = stablehlo.add %147, %1 : tensor<256x1xbf16>
    %149 = stablehlo.rsqrt %148 : tensor<256x1xbf16>
    %150 = stablehlo.reshape %149 : (tensor<256x1xbf16>) -> tensor<256xbf16>
    %151 = stablehlo.broadcast_in_dim %150, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %152 = stablehlo.multiply %143, %151 : tensor<256x768xbf16>
    %153 = stablehlo.reshape %arg9 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %155 = stablehlo.broadcast_in_dim %154, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %156 = stablehlo.multiply %152, %155 : tensor<256x768xbf16>
    %157 = stablehlo.reshape %arg8 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %158 = stablehlo.reshape %157 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %159 = stablehlo.broadcast_in_dim %158, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %160 = stablehlo.add %156, %159 : tensor<256x768xbf16>
    %161 = stablehlo.reshape %arg7 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16>
    %162 = stablehlo.reshape %161 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16>
    %163 = stablehlo.transpose %162, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16>
    %164 = stablehlo.dot_general %160, %163, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x3072xbf16>) -> tensor<256x3072xbf16>
    %165 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %166 = stablehlo.reshape %165 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %167 = stablehlo.broadcast_in_dim %166, dims = [1] : (tensor<3072xbf16>) -> tensor<256x3072xbf16>
    %168 = stablehlo.add %164, %167 : tensor<256x3072xbf16>
    %169 = stablehlo.maximum %168, %0 : tensor<256x3072xbf16>
    %170 = stablehlo.reshape %arg5 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16>
    %172 = stablehlo.transpose %171, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16>
    %173 = stablehlo.dot_general %169, %172, contracting_dims = [1] x [0] : (tensor<256x3072xbf16>, tensor<3072x768xbf16>) -> tensor<256x768xbf16>
    %174 = stablehlo.reshape %arg4 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %175 = stablehlo.reshape %174 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %176 = stablehlo.broadcast_in_dim %175, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %177 = stablehlo.add %173, %176 : tensor<256x768xbf16>
    %178 = stablehlo.add %139, %177 : tensor<256x768xbf16>
    %179 = stablehlo.reshape %178 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %180 = stablehlo.reduce(%179 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %181 = stablehlo.multiply %180, %10 : tensor<8x32xbf16>
    %182 = stablehlo.broadcast_in_dim %181, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %183 = stablehlo.subtract %179, %182 : tensor<8x32x768xbf16>
    %184 = stablehlo.multiply %183, %183 : tensor<8x32x768xbf16>
    %185 = stablehlo.reduce(%184 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %186 = stablehlo.multiply %185, %10 : tensor<8x32xbf16>
    %187 = stablehlo.reshape %186 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %188 = stablehlo.add %187, %9 : tensor<8x32x1xbf16>
    %189 = stablehlo.rsqrt %188 : tensor<8x32x1xbf16>
    %190 = stablehlo.reshape %189 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %191 = stablehlo.broadcast_in_dim %190, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %192 = stablehlo.multiply %183, %191 : tensor<8x32x768xbf16>
    %193 = stablehlo.reshape %arg3 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %194 = stablehlo.reshape %193 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %195 = stablehlo.broadcast_in_dim %194, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %196 = stablehlo.multiply %192, %195 : tensor<8x32x768xbf16>
    %197 = stablehlo.reshape %arg2 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %198 = stablehlo.reshape %197 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %199 = stablehlo.broadcast_in_dim %198, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %200 = stablehlo.add %196, %199 : tensor<8x32x768xbf16>
    %201 = stablehlo.reshape %200 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %202 = stablehlo.reshape %arg1 : (tensor<2x768xbf16>) -> tensor<1x2x768xbf16>
    %203 = stablehlo.reshape %202 : (tensor<1x2x768xbf16>) -> tensor<2x768xbf16>
    %204 = stablehlo.transpose %203, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,2]{0,1}"} : (tensor<2x768xbf16>) -> tensor<768x2xbf16>
    %205 = stablehlo.dot_general %201, %204, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x2xbf16>) -> tensor<256x2xbf16>
    %206 = stablehlo.reshape %205 : (tensor<256x2xbf16>) -> tensor<8x32x2xbf16>
    %207 = stablehlo.broadcast_in_dim %c_3, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32>
    %208 = stablehlo.compare  NE, %16, %12 : (tensor<8x32xi64>, tensor<8x32xi64>) -> tensor<8x32xi1>
    %209 = stablehlo.convert %208 : (tensor<8x32xi1>) -> tensor<8x32xi32>
    %210 = stablehlo.multiply %207, %209 : tensor<8x32xi32>
    %211 = stablehlo.iota dim = 0 : tensor<32xi32>
    %212 = stablehlo.broadcast_in_dim %211, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32>
    %213:2 = stablehlo.reduce(%210 init: %c_4), (%212 init: %c_0) across dimensions = [1] : (tensor<8x32xi32>, tensor<8x32xi32>, tensor<i32>, tensor<i32>) -> (tensor<8xi32>, tensor<8xi32>)
     reducer(%arg23: tensor<i32>, %arg25: tensor<i32>) (%arg24: tensor<i32>, %arg26: tensor<i32>)  {
      %221 = stablehlo.compare  GE, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %222 = stablehlo.select %221, %arg23, %arg25 : tensor<i1>, tensor<i32>
      %223 = stablehlo.compare  EQ, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %224 = stablehlo.minimum %arg24, %arg26 : tensor<i32>
      %225 = stablehlo.select %221, %arg24, %arg26 : tensor<i1>, tensor<i32>
      %226 = stablehlo.select %223, %224, %225 : tensor<i1>, tensor<i32>
      stablehlo.return %222, %226 : tensor<i32>, tensor<i32>
    }
    %214 = stablehlo.convert %213#1 : (tensor<8xi32>) -> tensor<8xi64>
    %215 = stablehlo.compare  LT, %214, %c_12 : (tensor<8xi64>, tensor<8xi64>) -> tensor<8xi1>
    %216 = stablehlo.add %214, %c_11 : tensor<8xi64>
    %217 = stablehlo.select %215, %216, %214 : tensor<8xi1>, tensor<8xi64>
    %218 = stablehlo.reshape %217 : (tensor<8xi64>) -> tensor<8x1xi64>
    %219 = stablehlo.concatenate %c_13, %218, dim = 1 : (tensor<8x1xi64>, tensor<8x1xi64>) -> tensor<8x2xi64>
    %220 = "stablehlo.gather"(%206, %219) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1, 2>}> : (tensor<8x32x2xbf16>, tensor<8x2xi64>) -> tensor<8x2xbf16>
    return %220 : tensor<8x2xbf16>
  }
}


// -----// IR Dump After ConvertXlaSdyToSdyPass (convert-xla-sdy-to-sdy) ('builtin.module' operation: @SyncTensorsGraph.630) //----- //
module @SyncTensorsGraph.630 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<2x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___score_weight"}, %arg2: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_final_layer_norm_bias"}, %arg3: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_final_layer_norm_weight"}, %arg4: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc2_bias"}, %arg5: tensor<768x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc2_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc1_bias"}, %arg7: tensor<3072x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc1_weight"}, %arg8: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_bias"}, %arg9: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_weight"}, %arg10: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_bias"}, %arg11: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_weight"}, %arg12: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_bias"}, %arg13: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_weight"}, %arg14: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_bias"}, %arg15: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_weight"}, %arg16: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg17: tensor<2050x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_embed_positions_weight"}, %arg18: tensor<50272x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_embed_tokens_weight"}, %arg19: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_bias"}, %arg20: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_weight"}, %arg21: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_bias"}, %arg22: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_weight"}) -> (tensor<8x2xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %c = stablehlo.constant dense<0> : tensor<i64>
    %c_0 = stablehlo.constant dense<0> : tensor<i32>
    %c_1 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64>
    %cst = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c_3 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi32>
    %c_4 = stablehlo.constant dense<-2147483648> : tensor<i32>
    %c_5 = stablehlo.constant dense<2> : tensor<i64>
    %cst_6 = stablehlo.constant dense<1.250000e-01> : tensor<bf16>
    %c_7 = stablehlo.constant dense<1> : tensor<i64>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %cst_9 = stablehlo.constant dense<1.304630e-03> : tensor<bf16>
    %cst_10 = stablehlo.constant dense<1.001360e-05> : tensor<bf16>
    %c_11 = stablehlo.constant dense<32> : tensor<8xi64>
    %c_12 = stablehlo.constant dense<0> : tensor<8xi64>
    %c_13 = stablehlo.constant dense<[[0], [1], [2], [3], [4], [5], [6], [7]]> : tensor<8x1xi64>
    %cst_14 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<256x3072xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<256x1xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<256xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %4 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %5 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %7 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<32x32xi64>
    %8 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<8x32x768xbf16>
    %9 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x32x1xbf16>
    %10 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<8x32xbf16>
    %11 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<8x32xi64>
    %12 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<8x32xi64>
    %13 = stablehlo.reshape %arg18 : (tensor<50272x768xbf16>) -> tensor<1x50272x768xbf16>
    %14 = stablehlo.reshape %13 : (tensor<1x50272x768xbf16>) -> tensor<50272x768xbf16>
    %15 = stablehlo.reshape %arg0 : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %16 = stablehlo.reshape %15 : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %17 = stablehlo.reshape %15 : (tensor<1x8x32xi64>) -> tensor<256xi64>
    %18 = stablehlo.convert %17 : (tensor<256xi64>) -> tensor<256xui32>
    %19 = "stablehlo.gather"(%14, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<50272x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %20 = stablehlo.reshape %19 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %21 = stablehlo.reshape %arg17 : (tensor<2050x768xbf16>) -> tensor<1x2050x768xbf16>
    %22 = stablehlo.reshape %21 : (tensor<1x2050x768xbf16>) -> tensor<2050x768xbf16>
    %23 = stablehlo.reshape %arg16 : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %24 = stablehlo.reshape %23 : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %25 = "stablehlo.reduce_window"(%24, %c) <{padding = dense<[[0, 0], [31, 0]]> : tensor<2x2xi64>, window_dimensions = array<i64: 1, 32>}> ({
    ^bb0(%arg23: tensor<i64>, %arg24: tensor<i64>):
      %221 = stablehlo.add %arg23, %arg24 : tensor<i64>
      stablehlo.return %221 : tensor<i64>
    }) : (tensor<8x32xi64>, tensor<i64>) -> tensor<8x32xi64>
    %26 = stablehlo.multiply %25, %24 : tensor<8x32xi64>
    %27 = stablehlo.subtract %26, %12 : tensor<8x32xi64>
    %28 = stablehlo.add %27, %11 : tensor<8x32xi64>
    %29 = stablehlo.reshape %28 : (tensor<8x32xi64>) -> tensor<256xi64>
    %30 = stablehlo.convert %29 : (tensor<256xi64>) -> tensor<256xui32>
    %31 = "stablehlo.gather"(%22, %30) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<2050x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %32 = stablehlo.reshape %31 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %33 = stablehlo.add %20, %32 : tensor<8x32x768xbf16>
    %34 = stablehlo.reduce(%33 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %35 = stablehlo.multiply %34, %10 : tensor<8x32xbf16>
    %36 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %37 = stablehlo.subtract %33, %36 : tensor<8x32x768xbf16>
    %38 = stablehlo.multiply %37, %37 : tensor<8x32x768xbf16>
    %39 = stablehlo.reduce(%38 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %40 = stablehlo.multiply %39, %10 : tensor<8x32xbf16>
    %41 = stablehlo.reshape %40 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %42 = stablehlo.add %41, %9 : tensor<8x32x1xbf16>
    %43 = stablehlo.rsqrt %42 : tensor<8x32x1xbf16>
    %44 = stablehlo.reshape %43 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %46 = stablehlo.multiply %37, %45 : tensor<8x32x768xbf16>
    %47 = stablehlo.reshape %arg15 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %48 = stablehlo.reshape %47 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %49 = stablehlo.broadcast_in_dim %48, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %50 = stablehlo.multiply %46, %49 : tensor<8x32x768xbf16>
    %51 = stablehlo.reshape %arg14 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %53 = stablehlo.broadcast_in_dim %52, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %54 = stablehlo.add %50, %53 : tensor<8x32x768xbf16>
    %55 = stablehlo.reshape %54 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %56 = stablehlo.reshape %arg22 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %57 = stablehlo.reshape %56 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %58 = stablehlo.transpose %57, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %59 = stablehlo.dot_general %55, %58, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %60 = stablehlo.reshape %59 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %61 = stablehlo.reshape %arg21 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %63 = stablehlo.broadcast_in_dim %62, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %64 = stablehlo.add %60, %63 : tensor<8x32x768xbf16>
    %65 = stablehlo.multiply %64, %8 : tensor<8x32x768xbf16>
    %66 = stablehlo.reshape %65 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %67 = stablehlo.transpose %66, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %68 = stablehlo.reshape %67 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %69 = stablehlo.reshape %arg20 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %70 = stablehlo.reshape %69 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %71 = stablehlo.transpose %70, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %72 = stablehlo.dot_general %55, %71, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %73 = stablehlo.reshape %72 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %74 = stablehlo.reshape %arg19 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %76 = stablehlo.broadcast_in_dim %75, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %77 = stablehlo.add %73, %76 : tensor<8x32x768xbf16>
    %78 = stablehlo.reshape %77 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 3, 1] : (tensor<8x32x12x64xbf16>) -> tensor<8x12x64x32xbf16>
    %80 = stablehlo.reshape %79 : (tensor<8x12x64x32xbf16>) -> tensor<96x64x32xbf16>
    %81 = stablehlo.dot_general %68, %80, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x64xbf16>, tensor<96x64x32xbf16>) -> tensor<96x32x32xbf16>
    %82 = stablehlo.reshape %81 : (tensor<96x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %83 = stablehlo.broadcast_in_dim %c_1, dims = [1] : (tensor<32xi64>) -> tensor<32x32xi64>
    %84 = stablehlo.broadcast_in_dim %c_1, dims = [0] : (tensor<32xi64>) -> tensor<32x32xi64>
    %85 = stablehlo.subtract %83, %84 : tensor<32x32xi64>
    %86 = stablehlo.compare  GE, %85, %7 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %87 = stablehlo.select %86, %6, %5 : tensor<32x32xi1>, tensor<32x32xbf16>
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %89 = stablehlo.convert %88 : (tensor<32x32xi1>) -> tensor<32x32xbf16>
    %90 = stablehlo.multiply %87, %89 : tensor<32x32xbf16>
    %91 = stablehlo.reshape %90 : (tensor<32x32xbf16>) -> tensor<1x32x32xbf16>
    %92 = stablehlo.broadcast_in_dim %91, dims = [1, 2, 3] : (tensor<1x32x32xbf16>) -> tensor<8x1x32x32xbf16>
    %93 = stablehlo.reshape %23 : (tensor<1x8x32xi64>) -> tensor<8x1x1x32xi64>
    %94 = stablehlo.convert %93 : (tensor<8x1x1x32xi64>) -> tensor<8x1x1x32xbf16>
    %95 = stablehlo.reshape %94 : (tensor<8x1x1x32xbf16>) -> tensor<8x1x32xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] : (tensor<8x1x32xbf16>) -> tensor<8x1x32x32xbf16>
    %97 = stablehlo.add %92, %96 : tensor<8x1x32x32xbf16>
    %98 = stablehlo.compare  EQ, %97, %4 : (tensor<8x1x32x32xbf16>, tensor<8x1x32x32xbf16>) -> tensor<8x1x32x32xi1>
    %99 = stablehlo.select %98, %3, %92 : tensor<8x1x32x32xi1>, tensor<8x1x32x32xbf16>
    %100 = stablehlo.reshape %99 : (tensor<8x1x32x32xbf16>) -> tensor<8x32x32xbf16>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] : (tensor<8x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %102 = stablehlo.add %82, %101 : tensor<8x12x32x32xbf16>
    %103 = stablehlo.convert %102 : (tensor<8x12x32x32xbf16>) -> tensor<8x12x32x32xf32>
    %104 = stablehlo.reduce(%103 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %106 = stablehlo.subtract %103, %105 : tensor<8x12x32x32xf32>
    %107 = stablehlo.exponential %106 : tensor<8x12x32x32xf32>
    %108 = stablehlo.reduce(%107 init: %cst_2) applies stablehlo.add across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %110 = stablehlo.divide %107, %109 : tensor<8x12x32x32xf32>
    %111 = stablehlo.convert %110 : (tensor<8x12x32x32xf32>) -> tensor<8x12x32x32xbf16>
    %112 = stablehlo.reshape %111 : (tensor<8x12x32x32xbf16>) -> tensor<96x32x32xbf16>
    %113 = stablehlo.reshape %arg13 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %114 = stablehlo.reshape %113 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %115 = stablehlo.transpose %114, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %116 = stablehlo.dot_general %55, %115, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %117 = stablehlo.reshape %116 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %118 = stablehlo.reshape %arg12 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %121 = stablehlo.add %117, %120 : tensor<8x32x768xbf16>
    %122 = stablehlo.reshape %121 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %123 = stablehlo.transpose %122, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %124 = stablehlo.reshape %123 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %125 = stablehlo.dot_general %112, %124, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x32xbf16>, tensor<96x32x64xbf16>) -> tensor<96x32x64xbf16>
    %126 = stablehlo.reshape %125 : (tensor<96x32x64xbf16>) -> tensor<8x12x32x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,32,12,64]{3,1,2,0}"} : (tensor<8x12x32x64xbf16>) -> tensor<8x32x12x64xbf16>
    %128 = stablehlo.reshape %127 : (tensor<8x32x12x64xbf16>) -> tensor<256x768xbf16>
    %129 = stablehlo.reshape %arg11 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %131 = stablehlo.transpose %130, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %132 = stablehlo.dot_general %128, %131, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %133 = stablehlo.reshape %132 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %134 = stablehlo.reshape %arg10 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %135 = stablehlo.reshape %134 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %136 = stablehlo.broadcast_in_dim %135, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %137 = stablehlo.add %133, %136 : tensor<8x32x768xbf16>
    %138 = stablehlo.add %33, %137 : tensor<8x32x768xbf16>
    %139 = stablehlo.reshape %138 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %140 = stablehlo.reduce(%139 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %141 = stablehlo.multiply %140, %2 : tensor<256xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %143 = stablehlo.subtract %139, %142 : tensor<256x768xbf16>
    %144 = stablehlo.multiply %143, %143 : tensor<256x768xbf16>
    %145 = stablehlo.reduce(%144 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %146 = stablehlo.multiply %145, %2 : tensor<256xbf16>
    %147 = stablehlo.reshape %146 : (tensor<256xbf16>) -> tensor<256x1xbf16>
    %148 = stablehlo.add %147, %1 : tensor<256x1xbf16>
    %149 = stablehlo.rsqrt %148 : tensor<256x1xbf16>
    %150 = stablehlo.reshape %149 : (tensor<256x1xbf16>) -> tensor<256xbf16>
    %151 = stablehlo.broadcast_in_dim %150, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %152 = stablehlo.multiply %143, %151 : tensor<256x768xbf16>
    %153 = stablehlo.reshape %arg9 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %155 = stablehlo.broadcast_in_dim %154, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %156 = stablehlo.multiply %152, %155 : tensor<256x768xbf16>
    %157 = stablehlo.reshape %arg8 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %158 = stablehlo.reshape %157 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %159 = stablehlo.broadcast_in_dim %158, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %160 = stablehlo.add %156, %159 : tensor<256x768xbf16>
    %161 = stablehlo.reshape %arg7 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16>
    %162 = stablehlo.reshape %161 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16>
    %163 = stablehlo.transpose %162, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16>
    %164 = stablehlo.dot_general %160, %163, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x3072xbf16>) -> tensor<256x3072xbf16>
    %165 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %166 = stablehlo.reshape %165 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %167 = stablehlo.broadcast_in_dim %166, dims = [1] : (tensor<3072xbf16>) -> tensor<256x3072xbf16>
    %168 = stablehlo.add %164, %167 : tensor<256x3072xbf16>
    %169 = stablehlo.maximum %168, %0 : tensor<256x3072xbf16>
    %170 = stablehlo.reshape %arg5 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16>
    %172 = stablehlo.transpose %171, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16>
    %173 = stablehlo.dot_general %169, %172, contracting_dims = [1] x [0] : (tensor<256x3072xbf16>, tensor<3072x768xbf16>) -> tensor<256x768xbf16>
    %174 = stablehlo.reshape %arg4 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %175 = stablehlo.reshape %174 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %176 = stablehlo.broadcast_in_dim %175, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %177 = stablehlo.add %173, %176 : tensor<256x768xbf16>
    %178 = stablehlo.add %139, %177 : tensor<256x768xbf16>
    %179 = stablehlo.reshape %178 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %180 = stablehlo.reduce(%179 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %181 = stablehlo.multiply %180, %10 : tensor<8x32xbf16>
    %182 = stablehlo.broadcast_in_dim %181, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %183 = stablehlo.subtract %179, %182 : tensor<8x32x768xbf16>
    %184 = stablehlo.multiply %183, %183 : tensor<8x32x768xbf16>
    %185 = stablehlo.reduce(%184 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %186 = stablehlo.multiply %185, %10 : tensor<8x32xbf16>
    %187 = stablehlo.reshape %186 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %188 = stablehlo.add %187, %9 : tensor<8x32x1xbf16>
    %189 = stablehlo.rsqrt %188 : tensor<8x32x1xbf16>
    %190 = stablehlo.reshape %189 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %191 = stablehlo.broadcast_in_dim %190, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %192 = stablehlo.multiply %183, %191 : tensor<8x32x768xbf16>
    %193 = stablehlo.reshape %arg3 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %194 = stablehlo.reshape %193 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %195 = stablehlo.broadcast_in_dim %194, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %196 = stablehlo.multiply %192, %195 : tensor<8x32x768xbf16>
    %197 = stablehlo.reshape %arg2 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %198 = stablehlo.reshape %197 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %199 = stablehlo.broadcast_in_dim %198, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %200 = stablehlo.add %196, %199 : tensor<8x32x768xbf16>
    %201 = stablehlo.reshape %200 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %202 = stablehlo.reshape %arg1 : (tensor<2x768xbf16>) -> tensor<1x2x768xbf16>
    %203 = stablehlo.reshape %202 : (tensor<1x2x768xbf16>) -> tensor<2x768xbf16>
    %204 = stablehlo.transpose %203, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,2]{0,1}"} : (tensor<2x768xbf16>) -> tensor<768x2xbf16>
    %205 = stablehlo.dot_general %201, %204, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x2xbf16>) -> tensor<256x2xbf16>
    %206 = stablehlo.reshape %205 : (tensor<256x2xbf16>) -> tensor<8x32x2xbf16>
    %207 = stablehlo.broadcast_in_dim %c_3, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32>
    %208 = stablehlo.compare  NE, %16, %12 : (tensor<8x32xi64>, tensor<8x32xi64>) -> tensor<8x32xi1>
    %209 = stablehlo.convert %208 : (tensor<8x32xi1>) -> tensor<8x32xi32>
    %210 = stablehlo.multiply %207, %209 : tensor<8x32xi32>
    %211 = stablehlo.iota dim = 0 : tensor<32xi32>
    %212 = stablehlo.broadcast_in_dim %211, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32>
    %213:2 = stablehlo.reduce(%210 init: %c_4), (%212 init: %c_0) across dimensions = [1] : (tensor<8x32xi32>, tensor<8x32xi32>, tensor<i32>, tensor<i32>) -> (tensor<8xi32>, tensor<8xi32>)
     reducer(%arg23: tensor<i32>, %arg25: tensor<i32>) (%arg24: tensor<i32>, %arg26: tensor<i32>)  {
      %221 = stablehlo.compare  GE, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %222 = stablehlo.select %221, %arg23, %arg25 : tensor<i1>, tensor<i32>
      %223 = stablehlo.compare  EQ, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %224 = stablehlo.minimum %arg24, %arg26 : tensor<i32>
      %225 = stablehlo.select %221, %arg24, %arg26 : tensor<i1>, tensor<i32>
      %226 = stablehlo.select %223, %224, %225 : tensor<i1>, tensor<i32>
      stablehlo.return %222, %226 : tensor<i32>, tensor<i32>
    }
    %214 = stablehlo.convert %213#1 : (tensor<8xi32>) -> tensor<8xi64>
    %215 = stablehlo.compare  LT, %214, %c_12 : (tensor<8xi64>, tensor<8xi64>) -> tensor<8xi1>
    %216 = stablehlo.add %214, %c_11 : tensor<8xi64>
    %217 = stablehlo.select %215, %216, %214 : tensor<8xi1>, tensor<8xi64>
    %218 = stablehlo.reshape %217 : (tensor<8xi64>) -> tensor<8x1xi64>
    %219 = stablehlo.concatenate %c_13, %218, dim = 1 : (tensor<8x1xi64>, tensor<8x1xi64>) -> tensor<8x2xi64>
    %220 = "stablehlo.gather"(%206, %219) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1, 2>}> : (tensor<8x32x2xbf16>, tensor<8x2xi64>) -> tensor<8x2xbf16>
    return %220 : tensor<8x2xbf16>
  }
}


// -----// IR Dump Before AnalyzeMeshPass (analyze-mesh) ('builtin.module' operation: @SyncTensorsGraph.630) //----- //
module @SyncTensorsGraph.630 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<2x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___score_weight"}, %arg2: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_final_layer_norm_bias"}, %arg3: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_final_layer_norm_weight"}, %arg4: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc2_bias"}, %arg5: tensor<768x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc2_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc1_bias"}, %arg7: tensor<3072x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc1_weight"}, %arg8: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_bias"}, %arg9: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_weight"}, %arg10: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_bias"}, %arg11: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_weight"}, %arg12: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_bias"}, %arg13: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_weight"}, %arg14: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_bias"}, %arg15: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_weight"}, %arg16: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg17: tensor<2050x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_embed_positions_weight"}, %arg18: tensor<50272x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_embed_tokens_weight"}, %arg19: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_bias"}, %arg20: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_weight"}, %arg21: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_bias"}, %arg22: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_weight"}) -> (tensor<8x2xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %c = stablehlo.constant dense<0> : tensor<i64>
    %c_0 = stablehlo.constant dense<0> : tensor<i32>
    %c_1 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64>
    %cst = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c_3 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi32>
    %c_4 = stablehlo.constant dense<-2147483648> : tensor<i32>
    %c_5 = stablehlo.constant dense<2> : tensor<i64>
    %cst_6 = stablehlo.constant dense<1.250000e-01> : tensor<bf16>
    %c_7 = stablehlo.constant dense<1> : tensor<i64>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %cst_9 = stablehlo.constant dense<1.304630e-03> : tensor<bf16>
    %cst_10 = stablehlo.constant dense<1.001360e-05> : tensor<bf16>
    %c_11 = stablehlo.constant dense<32> : tensor<8xi64>
    %c_12 = stablehlo.constant dense<0> : tensor<8xi64>
    %c_13 = stablehlo.constant dense<[[0], [1], [2], [3], [4], [5], [6], [7]]> : tensor<8x1xi64>
    %cst_14 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<256x3072xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<256x1xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<256xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %4 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %5 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %7 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<32x32xi64>
    %8 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<8x32x768xbf16>
    %9 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x32x1xbf16>
    %10 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<8x32xbf16>
    %11 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<8x32xi64>
    %12 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<8x32xi64>
    %13 = stablehlo.reshape %arg18 : (tensor<50272x768xbf16>) -> tensor<1x50272x768xbf16>
    %14 = stablehlo.reshape %13 : (tensor<1x50272x768xbf16>) -> tensor<50272x768xbf16>
    %15 = stablehlo.reshape %arg0 : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %16 = stablehlo.reshape %15 : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %17 = stablehlo.reshape %15 : (tensor<1x8x32xi64>) -> tensor<256xi64>
    %18 = stablehlo.convert %17 : (tensor<256xi64>) -> tensor<256xui32>
    %19 = "stablehlo.gather"(%14, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<50272x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %20 = stablehlo.reshape %19 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %21 = stablehlo.reshape %arg17 : (tensor<2050x768xbf16>) -> tensor<1x2050x768xbf16>
    %22 = stablehlo.reshape %21 : (tensor<1x2050x768xbf16>) -> tensor<2050x768xbf16>
    %23 = stablehlo.reshape %arg16 : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %24 = stablehlo.reshape %23 : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %25 = "stablehlo.reduce_window"(%24, %c) <{padding = dense<[[0, 0], [31, 0]]> : tensor<2x2xi64>, window_dimensions = array<i64: 1, 32>}> ({
    ^bb0(%arg23: tensor<i64>, %arg24: tensor<i64>):
      %221 = stablehlo.add %arg23, %arg24 : tensor<i64>
      stablehlo.return %221 : tensor<i64>
    }) : (tensor<8x32xi64>, tensor<i64>) -> tensor<8x32xi64>
    %26 = stablehlo.multiply %25, %24 : tensor<8x32xi64>
    %27 = stablehlo.subtract %26, %12 : tensor<8x32xi64>
    %28 = stablehlo.add %27, %11 : tensor<8x32xi64>
    %29 = stablehlo.reshape %28 : (tensor<8x32xi64>) -> tensor<256xi64>
    %30 = stablehlo.convert %29 : (tensor<256xi64>) -> tensor<256xui32>
    %31 = "stablehlo.gather"(%22, %30) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<2050x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %32 = stablehlo.reshape %31 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %33 = stablehlo.add %20, %32 : tensor<8x32x768xbf16>
    %34 = stablehlo.reduce(%33 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %35 = stablehlo.multiply %34, %10 : tensor<8x32xbf16>
    %36 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %37 = stablehlo.subtract %33, %36 : tensor<8x32x768xbf16>
    %38 = stablehlo.multiply %37, %37 : tensor<8x32x768xbf16>
    %39 = stablehlo.reduce(%38 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %40 = stablehlo.multiply %39, %10 : tensor<8x32xbf16>
    %41 = stablehlo.reshape %40 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %42 = stablehlo.add %41, %9 : tensor<8x32x1xbf16>
    %43 = stablehlo.rsqrt %42 : tensor<8x32x1xbf16>
    %44 = stablehlo.reshape %43 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %46 = stablehlo.multiply %37, %45 : tensor<8x32x768xbf16>
    %47 = stablehlo.reshape %arg15 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %48 = stablehlo.reshape %47 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %49 = stablehlo.broadcast_in_dim %48, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %50 = stablehlo.multiply %46, %49 : tensor<8x32x768xbf16>
    %51 = stablehlo.reshape %arg14 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %53 = stablehlo.broadcast_in_dim %52, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %54 = stablehlo.add %50, %53 : tensor<8x32x768xbf16>
    %55 = stablehlo.reshape %54 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %56 = stablehlo.reshape %arg22 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %57 = stablehlo.reshape %56 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %58 = stablehlo.transpose %57, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %59 = stablehlo.dot_general %55, %58, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %60 = stablehlo.reshape %59 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %61 = stablehlo.reshape %arg21 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %63 = stablehlo.broadcast_in_dim %62, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %64 = stablehlo.add %60, %63 : tensor<8x32x768xbf16>
    %65 = stablehlo.multiply %64, %8 : tensor<8x32x768xbf16>
    %66 = stablehlo.reshape %65 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %67 = stablehlo.transpose %66, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %68 = stablehlo.reshape %67 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %69 = stablehlo.reshape %arg20 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %70 = stablehlo.reshape %69 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %71 = stablehlo.transpose %70, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %72 = stablehlo.dot_general %55, %71, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %73 = stablehlo.reshape %72 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %74 = stablehlo.reshape %arg19 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %76 = stablehlo.broadcast_in_dim %75, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %77 = stablehlo.add %73, %76 : tensor<8x32x768xbf16>
    %78 = stablehlo.reshape %77 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 3, 1] : (tensor<8x32x12x64xbf16>) -> tensor<8x12x64x32xbf16>
    %80 = stablehlo.reshape %79 : (tensor<8x12x64x32xbf16>) -> tensor<96x64x32xbf16>
    %81 = stablehlo.dot_general %68, %80, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x64xbf16>, tensor<96x64x32xbf16>) -> tensor<96x32x32xbf16>
    %82 = stablehlo.reshape %81 : (tensor<96x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %83 = stablehlo.broadcast_in_dim %c_1, dims = [1] : (tensor<32xi64>) -> tensor<32x32xi64>
    %84 = stablehlo.broadcast_in_dim %c_1, dims = [0] : (tensor<32xi64>) -> tensor<32x32xi64>
    %85 = stablehlo.subtract %83, %84 : tensor<32x32xi64>
    %86 = stablehlo.compare  GE, %85, %7 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %87 = stablehlo.select %86, %6, %5 : tensor<32x32xi1>, tensor<32x32xbf16>
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %89 = stablehlo.convert %88 : (tensor<32x32xi1>) -> tensor<32x32xbf16>
    %90 = stablehlo.multiply %87, %89 : tensor<32x32xbf16>
    %91 = stablehlo.reshape %90 : (tensor<32x32xbf16>) -> tensor<1x32x32xbf16>
    %92 = stablehlo.broadcast_in_dim %91, dims = [1, 2, 3] : (tensor<1x32x32xbf16>) -> tensor<8x1x32x32xbf16>
    %93 = stablehlo.reshape %23 : (tensor<1x8x32xi64>) -> tensor<8x1x1x32xi64>
    %94 = stablehlo.convert %93 : (tensor<8x1x1x32xi64>) -> tensor<8x1x1x32xbf16>
    %95 = stablehlo.reshape %94 : (tensor<8x1x1x32xbf16>) -> tensor<8x1x32xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] : (tensor<8x1x32xbf16>) -> tensor<8x1x32x32xbf16>
    %97 = stablehlo.add %92, %96 : tensor<8x1x32x32xbf16>
    %98 = stablehlo.compare  EQ, %97, %4 : (tensor<8x1x32x32xbf16>, tensor<8x1x32x32xbf16>) -> tensor<8x1x32x32xi1>
    %99 = stablehlo.select %98, %3, %92 : tensor<8x1x32x32xi1>, tensor<8x1x32x32xbf16>
    %100 = stablehlo.reshape %99 : (tensor<8x1x32x32xbf16>) -> tensor<8x32x32xbf16>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] : (tensor<8x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %102 = stablehlo.add %82, %101 : tensor<8x12x32x32xbf16>
    %103 = stablehlo.convert %102 : (tensor<8x12x32x32xbf16>) -> tensor<8x12x32x32xf32>
    %104 = stablehlo.reduce(%103 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %106 = stablehlo.subtract %103, %105 : tensor<8x12x32x32xf32>
    %107 = stablehlo.exponential %106 : tensor<8x12x32x32xf32>
    %108 = stablehlo.reduce(%107 init: %cst_2) applies stablehlo.add across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %110 = stablehlo.divide %107, %109 : tensor<8x12x32x32xf32>
    %111 = stablehlo.convert %110 : (tensor<8x12x32x32xf32>) -> tensor<8x12x32x32xbf16>
    %112 = stablehlo.reshape %111 : (tensor<8x12x32x32xbf16>) -> tensor<96x32x32xbf16>
    %113 = stablehlo.reshape %arg13 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %114 = stablehlo.reshape %113 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %115 = stablehlo.transpose %114, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %116 = stablehlo.dot_general %55, %115, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %117 = stablehlo.reshape %116 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %118 = stablehlo.reshape %arg12 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %121 = stablehlo.add %117, %120 : tensor<8x32x768xbf16>
    %122 = stablehlo.reshape %121 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %123 = stablehlo.transpose %122, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %124 = stablehlo.reshape %123 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %125 = stablehlo.dot_general %112, %124, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x32xbf16>, tensor<96x32x64xbf16>) -> tensor<96x32x64xbf16>
    %126 = stablehlo.reshape %125 : (tensor<96x32x64xbf16>) -> tensor<8x12x32x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,32,12,64]{3,1,2,0}"} : (tensor<8x12x32x64xbf16>) -> tensor<8x32x12x64xbf16>
    %128 = stablehlo.reshape %127 : (tensor<8x32x12x64xbf16>) -> tensor<256x768xbf16>
    %129 = stablehlo.reshape %arg11 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %131 = stablehlo.transpose %130, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %132 = stablehlo.dot_general %128, %131, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %133 = stablehlo.reshape %132 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %134 = stablehlo.reshape %arg10 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %135 = stablehlo.reshape %134 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %136 = stablehlo.broadcast_in_dim %135, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %137 = stablehlo.add %133, %136 : tensor<8x32x768xbf16>
    %138 = stablehlo.add %33, %137 : tensor<8x32x768xbf16>
    %139 = stablehlo.reshape %138 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %140 = stablehlo.reduce(%139 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %141 = stablehlo.multiply %140, %2 : tensor<256xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %143 = stablehlo.subtract %139, %142 : tensor<256x768xbf16>
    %144 = stablehlo.multiply %143, %143 : tensor<256x768xbf16>
    %145 = stablehlo.reduce(%144 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %146 = stablehlo.multiply %145, %2 : tensor<256xbf16>
    %147 = stablehlo.reshape %146 : (tensor<256xbf16>) -> tensor<256x1xbf16>
    %148 = stablehlo.add %147, %1 : tensor<256x1xbf16>
    %149 = stablehlo.rsqrt %148 : tensor<256x1xbf16>
    %150 = stablehlo.reshape %149 : (tensor<256x1xbf16>) -> tensor<256xbf16>
    %151 = stablehlo.broadcast_in_dim %150, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %152 = stablehlo.multiply %143, %151 : tensor<256x768xbf16>
    %153 = stablehlo.reshape %arg9 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %155 = stablehlo.broadcast_in_dim %154, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %156 = stablehlo.multiply %152, %155 : tensor<256x768xbf16>
    %157 = stablehlo.reshape %arg8 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %158 = stablehlo.reshape %157 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %159 = stablehlo.broadcast_in_dim %158, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %160 = stablehlo.add %156, %159 : tensor<256x768xbf16>
    %161 = stablehlo.reshape %arg7 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16>
    %162 = stablehlo.reshape %161 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16>
    %163 = stablehlo.transpose %162, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16>
    %164 = stablehlo.dot_general %160, %163, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x3072xbf16>) -> tensor<256x3072xbf16>
    %165 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %166 = stablehlo.reshape %165 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %167 = stablehlo.broadcast_in_dim %166, dims = [1] : (tensor<3072xbf16>) -> tensor<256x3072xbf16>
    %168 = stablehlo.add %164, %167 : tensor<256x3072xbf16>
    %169 = stablehlo.maximum %168, %0 : tensor<256x3072xbf16>
    %170 = stablehlo.reshape %arg5 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16>
    %172 = stablehlo.transpose %171, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16>
    %173 = stablehlo.dot_general %169, %172, contracting_dims = [1] x [0] : (tensor<256x3072xbf16>, tensor<3072x768xbf16>) -> tensor<256x768xbf16>
    %174 = stablehlo.reshape %arg4 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %175 = stablehlo.reshape %174 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %176 = stablehlo.broadcast_in_dim %175, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %177 = stablehlo.add %173, %176 : tensor<256x768xbf16>
    %178 = stablehlo.add %139, %177 : tensor<256x768xbf16>
    %179 = stablehlo.reshape %178 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %180 = stablehlo.reduce(%179 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %181 = stablehlo.multiply %180, %10 : tensor<8x32xbf16>
    %182 = stablehlo.broadcast_in_dim %181, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %183 = stablehlo.subtract %179, %182 : tensor<8x32x768xbf16>
    %184 = stablehlo.multiply %183, %183 : tensor<8x32x768xbf16>
    %185 = stablehlo.reduce(%184 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %186 = stablehlo.multiply %185, %10 : tensor<8x32xbf16>
    %187 = stablehlo.reshape %186 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %188 = stablehlo.add %187, %9 : tensor<8x32x1xbf16>
    %189 = stablehlo.rsqrt %188 : tensor<8x32x1xbf16>
    %190 = stablehlo.reshape %189 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %191 = stablehlo.broadcast_in_dim %190, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %192 = stablehlo.multiply %183, %191 : tensor<8x32x768xbf16>
    %193 = stablehlo.reshape %arg3 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %194 = stablehlo.reshape %193 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %195 = stablehlo.broadcast_in_dim %194, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %196 = stablehlo.multiply %192, %195 : tensor<8x32x768xbf16>
    %197 = stablehlo.reshape %arg2 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %198 = stablehlo.reshape %197 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %199 = stablehlo.broadcast_in_dim %198, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %200 = stablehlo.add %196, %199 : tensor<8x32x768xbf16>
    %201 = stablehlo.reshape %200 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %202 = stablehlo.reshape %arg1 : (tensor<2x768xbf16>) -> tensor<1x2x768xbf16>
    %203 = stablehlo.reshape %202 : (tensor<1x2x768xbf16>) -> tensor<2x768xbf16>
    %204 = stablehlo.transpose %203, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,2]{0,1}"} : (tensor<2x768xbf16>) -> tensor<768x2xbf16>
    %205 = stablehlo.dot_general %201, %204, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x2xbf16>) -> tensor<256x2xbf16>
    %206 = stablehlo.reshape %205 : (tensor<256x2xbf16>) -> tensor<8x32x2xbf16>
    %207 = stablehlo.broadcast_in_dim %c_3, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32>
    %208 = stablehlo.compare  NE, %16, %12 : (tensor<8x32xi64>, tensor<8x32xi64>) -> tensor<8x32xi1>
    %209 = stablehlo.convert %208 : (tensor<8x32xi1>) -> tensor<8x32xi32>
    %210 = stablehlo.multiply %207, %209 : tensor<8x32xi32>
    %211 = stablehlo.iota dim = 0 : tensor<32xi32>
    %212 = stablehlo.broadcast_in_dim %211, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32>
    %213:2 = stablehlo.reduce(%210 init: %c_4), (%212 init: %c_0) across dimensions = [1] : (tensor<8x32xi32>, tensor<8x32xi32>, tensor<i32>, tensor<i32>) -> (tensor<8xi32>, tensor<8xi32>)
     reducer(%arg23: tensor<i32>, %arg25: tensor<i32>) (%arg24: tensor<i32>, %arg26: tensor<i32>)  {
      %221 = stablehlo.compare  GE, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %222 = stablehlo.select %221, %arg23, %arg25 : tensor<i1>, tensor<i32>
      %223 = stablehlo.compare  EQ, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %224 = stablehlo.minimum %arg24, %arg26 : tensor<i32>
      %225 = stablehlo.select %221, %arg24, %arg26 : tensor<i1>, tensor<i32>
      %226 = stablehlo.select %223, %224, %225 : tensor<i1>, tensor<i32>
      stablehlo.return %222, %226 : tensor<i32>, tensor<i32>
    }
    %214 = stablehlo.convert %213#1 : (tensor<8xi32>) -> tensor<8xi64>
    %215 = stablehlo.compare  LT, %214, %c_12 : (tensor<8xi64>, tensor<8xi64>) -> tensor<8xi1>
    %216 = stablehlo.add %214, %c_11 : tensor<8xi64>
    %217 = stablehlo.select %215, %216, %214 : tensor<8xi1>, tensor<8xi64>
    %218 = stablehlo.reshape %217 : (tensor<8xi64>) -> tensor<8x1xi64>
    %219 = stablehlo.concatenate %c_13, %218, dim = 1 : (tensor<8x1xi64>, tensor<8x1xi64>) -> tensor<8x2xi64>
    %220 = "stablehlo.gather"(%206, %219) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1, 2>}> : (tensor<8x32x2xbf16>, tensor<8x2xi64>) -> tensor<8x2xbf16>
    return %220 : tensor<8x2xbf16>
  }
}


// -----// IR Dump Before ApplyShardingConstraintsPass (sdy-apply-sharding-constraints) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.630 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<2x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___score_weight"}, %arg2: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_final_layer_norm_bias"}, %arg3: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_final_layer_norm_weight"}, %arg4: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc2_bias"}, %arg5: tensor<768x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc2_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc1_bias"}, %arg7: tensor<3072x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc1_weight"}, %arg8: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_bias"}, %arg9: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_weight"}, %arg10: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_bias"}, %arg11: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_weight"}, %arg12: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_bias"}, %arg13: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_weight"}, %arg14: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_bias"}, %arg15: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_weight"}, %arg16: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg17: tensor<2050x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_embed_positions_weight"}, %arg18: tensor<50272x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_embed_tokens_weight"}, %arg19: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_bias"}, %arg20: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_weight"}, %arg21: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_bias"}, %arg22: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_weight"}) -> (tensor<8x2xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %c = stablehlo.constant dense<0> : tensor<i64>
    %c_0 = stablehlo.constant dense<0> : tensor<i32>
    %c_1 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64>
    %cst = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c_3 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi32>
    %c_4 = stablehlo.constant dense<-2147483648> : tensor<i32>
    %c_5 = stablehlo.constant dense<2> : tensor<i64>
    %cst_6 = stablehlo.constant dense<1.250000e-01> : tensor<bf16>
    %c_7 = stablehlo.constant dense<1> : tensor<i64>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %cst_9 = stablehlo.constant dense<1.304630e-03> : tensor<bf16>
    %cst_10 = stablehlo.constant dense<1.001360e-05> : tensor<bf16>
    %c_11 = stablehlo.constant dense<32> : tensor<8xi64>
    %c_12 = stablehlo.constant dense<0> : tensor<8xi64>
    %c_13 = stablehlo.constant dense<[[0], [1], [2], [3], [4], [5], [6], [7]]> : tensor<8x1xi64>
    %cst_14 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<256x3072xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<256x1xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<256xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %4 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %5 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %7 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<32x32xi64>
    %8 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<8x32x768xbf16>
    %9 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x32x1xbf16>
    %10 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<8x32xbf16>
    %11 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<8x32xi64>
    %12 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<8x32xi64>
    %13 = stablehlo.reshape %arg18 : (tensor<50272x768xbf16>) -> tensor<1x50272x768xbf16>
    %14 = stablehlo.reshape %13 : (tensor<1x50272x768xbf16>) -> tensor<50272x768xbf16>
    %15 = stablehlo.reshape %arg0 : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %16 = stablehlo.reshape %15 : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %17 = stablehlo.reshape %15 : (tensor<1x8x32xi64>) -> tensor<256xi64>
    %18 = stablehlo.convert %17 : (tensor<256xi64>) -> tensor<256xui32>
    %19 = "stablehlo.gather"(%14, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<50272x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %20 = stablehlo.reshape %19 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %21 = stablehlo.reshape %arg17 : (tensor<2050x768xbf16>) -> tensor<1x2050x768xbf16>
    %22 = stablehlo.reshape %21 : (tensor<1x2050x768xbf16>) -> tensor<2050x768xbf16>
    %23 = stablehlo.reshape %arg16 : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %24 = stablehlo.reshape %23 : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %25 = "stablehlo.reduce_window"(%24, %c) <{padding = dense<[[0, 0], [31, 0]]> : tensor<2x2xi64>, window_dimensions = array<i64: 1, 32>}> ({
    ^bb0(%arg23: tensor<i64>, %arg24: tensor<i64>):
      %221 = stablehlo.add %arg23, %arg24 : tensor<i64>
      stablehlo.return %221 : tensor<i64>
    }) : (tensor<8x32xi64>, tensor<i64>) -> tensor<8x32xi64>
    %26 = stablehlo.multiply %25, %24 : tensor<8x32xi64>
    %27 = stablehlo.subtract %26, %12 : tensor<8x32xi64>
    %28 = stablehlo.add %27, %11 : tensor<8x32xi64>
    %29 = stablehlo.reshape %28 : (tensor<8x32xi64>) -> tensor<256xi64>
    %30 = stablehlo.convert %29 : (tensor<256xi64>) -> tensor<256xui32>
    %31 = "stablehlo.gather"(%22, %30) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<2050x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %32 = stablehlo.reshape %31 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %33 = stablehlo.add %20, %32 : tensor<8x32x768xbf16>
    %34 = stablehlo.reduce(%33 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %35 = stablehlo.multiply %34, %10 : tensor<8x32xbf16>
    %36 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %37 = stablehlo.subtract %33, %36 : tensor<8x32x768xbf16>
    %38 = stablehlo.multiply %37, %37 : tensor<8x32x768xbf16>
    %39 = stablehlo.reduce(%38 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %40 = stablehlo.multiply %39, %10 : tensor<8x32xbf16>
    %41 = stablehlo.reshape %40 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %42 = stablehlo.add %41, %9 : tensor<8x32x1xbf16>
    %43 = stablehlo.rsqrt %42 : tensor<8x32x1xbf16>
    %44 = stablehlo.reshape %43 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %46 = stablehlo.multiply %37, %45 : tensor<8x32x768xbf16>
    %47 = stablehlo.reshape %arg15 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %48 = stablehlo.reshape %47 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %49 = stablehlo.broadcast_in_dim %48, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %50 = stablehlo.multiply %46, %49 : tensor<8x32x768xbf16>
    %51 = stablehlo.reshape %arg14 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %53 = stablehlo.broadcast_in_dim %52, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %54 = stablehlo.add %50, %53 : tensor<8x32x768xbf16>
    %55 = stablehlo.reshape %54 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %56 = stablehlo.reshape %arg22 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %57 = stablehlo.reshape %56 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %58 = stablehlo.transpose %57, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %59 = stablehlo.dot_general %55, %58, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %60 = stablehlo.reshape %59 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %61 = stablehlo.reshape %arg21 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %63 = stablehlo.broadcast_in_dim %62, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %64 = stablehlo.add %60, %63 : tensor<8x32x768xbf16>
    %65 = stablehlo.multiply %64, %8 : tensor<8x32x768xbf16>
    %66 = stablehlo.reshape %65 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %67 = stablehlo.transpose %66, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %68 = stablehlo.reshape %67 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %69 = stablehlo.reshape %arg20 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %70 = stablehlo.reshape %69 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %71 = stablehlo.transpose %70, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %72 = stablehlo.dot_general %55, %71, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %73 = stablehlo.reshape %72 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %74 = stablehlo.reshape %arg19 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %76 = stablehlo.broadcast_in_dim %75, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %77 = stablehlo.add %73, %76 : tensor<8x32x768xbf16>
    %78 = stablehlo.reshape %77 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 3, 1] : (tensor<8x32x12x64xbf16>) -> tensor<8x12x64x32xbf16>
    %80 = stablehlo.reshape %79 : (tensor<8x12x64x32xbf16>) -> tensor<96x64x32xbf16>
    %81 = stablehlo.dot_general %68, %80, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x64xbf16>, tensor<96x64x32xbf16>) -> tensor<96x32x32xbf16>
    %82 = stablehlo.reshape %81 : (tensor<96x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %83 = stablehlo.broadcast_in_dim %c_1, dims = [1] : (tensor<32xi64>) -> tensor<32x32xi64>
    %84 = stablehlo.broadcast_in_dim %c_1, dims = [0] : (tensor<32xi64>) -> tensor<32x32xi64>
    %85 = stablehlo.subtract %83, %84 : tensor<32x32xi64>
    %86 = stablehlo.compare  GE, %85, %7 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %87 = stablehlo.select %86, %6, %5 : tensor<32x32xi1>, tensor<32x32xbf16>
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %89 = stablehlo.convert %88 : (tensor<32x32xi1>) -> tensor<32x32xbf16>
    %90 = stablehlo.multiply %87, %89 : tensor<32x32xbf16>
    %91 = stablehlo.reshape %90 : (tensor<32x32xbf16>) -> tensor<1x32x32xbf16>
    %92 = stablehlo.broadcast_in_dim %91, dims = [1, 2, 3] : (tensor<1x32x32xbf16>) -> tensor<8x1x32x32xbf16>
    %93 = stablehlo.reshape %23 : (tensor<1x8x32xi64>) -> tensor<8x1x1x32xi64>
    %94 = stablehlo.convert %93 : (tensor<8x1x1x32xi64>) -> tensor<8x1x1x32xbf16>
    %95 = stablehlo.reshape %94 : (tensor<8x1x1x32xbf16>) -> tensor<8x1x32xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] : (tensor<8x1x32xbf16>) -> tensor<8x1x32x32xbf16>
    %97 = stablehlo.add %92, %96 : tensor<8x1x32x32xbf16>
    %98 = stablehlo.compare  EQ, %97, %4 : (tensor<8x1x32x32xbf16>, tensor<8x1x32x32xbf16>) -> tensor<8x1x32x32xi1>
    %99 = stablehlo.select %98, %3, %92 : tensor<8x1x32x32xi1>, tensor<8x1x32x32xbf16>
    %100 = stablehlo.reshape %99 : (tensor<8x1x32x32xbf16>) -> tensor<8x32x32xbf16>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] : (tensor<8x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %102 = stablehlo.add %82, %101 : tensor<8x12x32x32xbf16>
    %103 = stablehlo.convert %102 : (tensor<8x12x32x32xbf16>) -> tensor<8x12x32x32xf32>
    %104 = stablehlo.reduce(%103 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %106 = stablehlo.subtract %103, %105 : tensor<8x12x32x32xf32>
    %107 = stablehlo.exponential %106 : tensor<8x12x32x32xf32>
    %108 = stablehlo.reduce(%107 init: %cst_2) applies stablehlo.add across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %110 = stablehlo.divide %107, %109 : tensor<8x12x32x32xf32>
    %111 = stablehlo.convert %110 : (tensor<8x12x32x32xf32>) -> tensor<8x12x32x32xbf16>
    %112 = stablehlo.reshape %111 : (tensor<8x12x32x32xbf16>) -> tensor<96x32x32xbf16>
    %113 = stablehlo.reshape %arg13 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %114 = stablehlo.reshape %113 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %115 = stablehlo.transpose %114, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %116 = stablehlo.dot_general %55, %115, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %117 = stablehlo.reshape %116 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %118 = stablehlo.reshape %arg12 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %121 = stablehlo.add %117, %120 : tensor<8x32x768xbf16>
    %122 = stablehlo.reshape %121 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %123 = stablehlo.transpose %122, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %124 = stablehlo.reshape %123 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %125 = stablehlo.dot_general %112, %124, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x32xbf16>, tensor<96x32x64xbf16>) -> tensor<96x32x64xbf16>
    %126 = stablehlo.reshape %125 : (tensor<96x32x64xbf16>) -> tensor<8x12x32x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,32,12,64]{3,1,2,0}"} : (tensor<8x12x32x64xbf16>) -> tensor<8x32x12x64xbf16>
    %128 = stablehlo.reshape %127 : (tensor<8x32x12x64xbf16>) -> tensor<256x768xbf16>
    %129 = stablehlo.reshape %arg11 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %131 = stablehlo.transpose %130, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %132 = stablehlo.dot_general %128, %131, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %133 = stablehlo.reshape %132 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %134 = stablehlo.reshape %arg10 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %135 = stablehlo.reshape %134 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %136 = stablehlo.broadcast_in_dim %135, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %137 = stablehlo.add %133, %136 : tensor<8x32x768xbf16>
    %138 = stablehlo.add %33, %137 : tensor<8x32x768xbf16>
    %139 = stablehlo.reshape %138 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %140 = stablehlo.reduce(%139 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %141 = stablehlo.multiply %140, %2 : tensor<256xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %143 = stablehlo.subtract %139, %142 : tensor<256x768xbf16>
    %144 = stablehlo.multiply %143, %143 : tensor<256x768xbf16>
    %145 = stablehlo.reduce(%144 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %146 = stablehlo.multiply %145, %2 : tensor<256xbf16>
    %147 = stablehlo.reshape %146 : (tensor<256xbf16>) -> tensor<256x1xbf16>
    %148 = stablehlo.add %147, %1 : tensor<256x1xbf16>
    %149 = stablehlo.rsqrt %148 : tensor<256x1xbf16>
    %150 = stablehlo.reshape %149 : (tensor<256x1xbf16>) -> tensor<256xbf16>
    %151 = stablehlo.broadcast_in_dim %150, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %152 = stablehlo.multiply %143, %151 : tensor<256x768xbf16>
    %153 = stablehlo.reshape %arg9 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %155 = stablehlo.broadcast_in_dim %154, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %156 = stablehlo.multiply %152, %155 : tensor<256x768xbf16>
    %157 = stablehlo.reshape %arg8 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %158 = stablehlo.reshape %157 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %159 = stablehlo.broadcast_in_dim %158, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %160 = stablehlo.add %156, %159 : tensor<256x768xbf16>
    %161 = stablehlo.reshape %arg7 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16>
    %162 = stablehlo.reshape %161 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16>
    %163 = stablehlo.transpose %162, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16>
    %164 = stablehlo.dot_general %160, %163, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x3072xbf16>) -> tensor<256x3072xbf16>
    %165 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %166 = stablehlo.reshape %165 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %167 = stablehlo.broadcast_in_dim %166, dims = [1] : (tensor<3072xbf16>) -> tensor<256x3072xbf16>
    %168 = stablehlo.add %164, %167 : tensor<256x3072xbf16>
    %169 = stablehlo.maximum %168, %0 : tensor<256x3072xbf16>
    %170 = stablehlo.reshape %arg5 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16>
    %172 = stablehlo.transpose %171, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16>
    %173 = stablehlo.dot_general %169, %172, contracting_dims = [1] x [0] : (tensor<256x3072xbf16>, tensor<3072x768xbf16>) -> tensor<256x768xbf16>
    %174 = stablehlo.reshape %arg4 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %175 = stablehlo.reshape %174 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %176 = stablehlo.broadcast_in_dim %175, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %177 = stablehlo.add %173, %176 : tensor<256x768xbf16>
    %178 = stablehlo.add %139, %177 : tensor<256x768xbf16>
    %179 = stablehlo.reshape %178 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %180 = stablehlo.reduce(%179 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %181 = stablehlo.multiply %180, %10 : tensor<8x32xbf16>
    %182 = stablehlo.broadcast_in_dim %181, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %183 = stablehlo.subtract %179, %182 : tensor<8x32x768xbf16>
    %184 = stablehlo.multiply %183, %183 : tensor<8x32x768xbf16>
    %185 = stablehlo.reduce(%184 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %186 = stablehlo.multiply %185, %10 : tensor<8x32xbf16>
    %187 = stablehlo.reshape %186 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %188 = stablehlo.add %187, %9 : tensor<8x32x1xbf16>
    %189 = stablehlo.rsqrt %188 : tensor<8x32x1xbf16>
    %190 = stablehlo.reshape %189 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %191 = stablehlo.broadcast_in_dim %190, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %192 = stablehlo.multiply %183, %191 : tensor<8x32x768xbf16>
    %193 = stablehlo.reshape %arg3 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %194 = stablehlo.reshape %193 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %195 = stablehlo.broadcast_in_dim %194, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %196 = stablehlo.multiply %192, %195 : tensor<8x32x768xbf16>
    %197 = stablehlo.reshape %arg2 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %198 = stablehlo.reshape %197 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %199 = stablehlo.broadcast_in_dim %198, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %200 = stablehlo.add %196, %199 : tensor<8x32x768xbf16>
    %201 = stablehlo.reshape %200 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %202 = stablehlo.reshape %arg1 : (tensor<2x768xbf16>) -> tensor<1x2x768xbf16>
    %203 = stablehlo.reshape %202 : (tensor<1x2x768xbf16>) -> tensor<2x768xbf16>
    %204 = stablehlo.transpose %203, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,2]{0,1}"} : (tensor<2x768xbf16>) -> tensor<768x2xbf16>
    %205 = stablehlo.dot_general %201, %204, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x2xbf16>) -> tensor<256x2xbf16>
    %206 = stablehlo.reshape %205 : (tensor<256x2xbf16>) -> tensor<8x32x2xbf16>
    %207 = stablehlo.broadcast_in_dim %c_3, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32>
    %208 = stablehlo.compare  NE, %16, %12 : (tensor<8x32xi64>, tensor<8x32xi64>) -> tensor<8x32xi1>
    %209 = stablehlo.convert %208 : (tensor<8x32xi1>) -> tensor<8x32xi32>
    %210 = stablehlo.multiply %207, %209 : tensor<8x32xi32>
    %211 = stablehlo.iota dim = 0 : tensor<32xi32>
    %212 = stablehlo.broadcast_in_dim %211, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32>
    %213:2 = stablehlo.reduce(%210 init: %c_4), (%212 init: %c_0) across dimensions = [1] : (tensor<8x32xi32>, tensor<8x32xi32>, tensor<i32>, tensor<i32>) -> (tensor<8xi32>, tensor<8xi32>)
     reducer(%arg23: tensor<i32>, %arg25: tensor<i32>) (%arg24: tensor<i32>, %arg26: tensor<i32>)  {
      %221 = stablehlo.compare  GE, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %222 = stablehlo.select %221, %arg23, %arg25 : tensor<i1>, tensor<i32>
      %223 = stablehlo.compare  EQ, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %224 = stablehlo.minimum %arg24, %arg26 : tensor<i32>
      %225 = stablehlo.select %221, %arg24, %arg26 : tensor<i1>, tensor<i32>
      %226 = stablehlo.select %223, %224, %225 : tensor<i1>, tensor<i32>
      stablehlo.return %222, %226 : tensor<i32>, tensor<i32>
    }
    %214 = stablehlo.convert %213#1 : (tensor<8xi32>) -> tensor<8xi64>
    %215 = stablehlo.compare  LT, %214, %c_12 : (tensor<8xi64>, tensor<8xi64>) -> tensor<8xi1>
    %216 = stablehlo.add %214, %c_11 : tensor<8xi64>
    %217 = stablehlo.select %215, %216, %214 : tensor<8xi1>, tensor<8xi64>
    %218 = stablehlo.reshape %217 : (tensor<8xi64>) -> tensor<8x1xi64>
    %219 = stablehlo.concatenate %c_13, %218, dim = 1 : (tensor<8x1xi64>, tensor<8x1xi64>) -> tensor<8x2xi64>
    %220 = "stablehlo.gather"(%206, %219) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1, 2>}> : (tensor<8x32x2xbf16>, tensor<8x2xi64>) -> tensor<8x2xbf16>
    return %220 : tensor<8x2xbf16>
  }
}


// -----// IR Dump Before AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @SyncTensorsGraph.630) //----- //
module @SyncTensorsGraph.630 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<2x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___score_weight"}, %arg2: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_final_layer_norm_bias"}, %arg3: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_final_layer_norm_weight"}, %arg4: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc2_bias"}, %arg5: tensor<768x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc2_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc1_bias"}, %arg7: tensor<3072x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc1_weight"}, %arg8: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_bias"}, %arg9: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_weight"}, %arg10: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_bias"}, %arg11: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_weight"}, %arg12: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_bias"}, %arg13: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_weight"}, %arg14: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_bias"}, %arg15: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_weight"}, %arg16: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg17: tensor<2050x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_embed_positions_weight"}, %arg18: tensor<50272x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_embed_tokens_weight"}, %arg19: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_bias"}, %arg20: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_weight"}, %arg21: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_bias"}, %arg22: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_weight"}) -> (tensor<8x2xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %c = stablehlo.constant dense<0> : tensor<i64>
    %c_0 = stablehlo.constant dense<0> : tensor<i32>
    %c_1 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64>
    %cst = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c_3 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi32>
    %c_4 = stablehlo.constant dense<-2147483648> : tensor<i32>
    %c_5 = stablehlo.constant dense<2> : tensor<i64>
    %cst_6 = stablehlo.constant dense<1.250000e-01> : tensor<bf16>
    %c_7 = stablehlo.constant dense<1> : tensor<i64>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %cst_9 = stablehlo.constant dense<1.304630e-03> : tensor<bf16>
    %cst_10 = stablehlo.constant dense<1.001360e-05> : tensor<bf16>
    %c_11 = stablehlo.constant dense<32> : tensor<8xi64>
    %c_12 = stablehlo.constant dense<0> : tensor<8xi64>
    %c_13 = stablehlo.constant dense<[[0], [1], [2], [3], [4], [5], [6], [7]]> : tensor<8x1xi64>
    %cst_14 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<256x3072xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<256x1xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<256xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %4 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %5 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %7 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<32x32xi64>
    %8 = stablehlo.broadcast_in_dim %cst_6, dims = [] : (tensor<bf16>) -> tensor<8x32x768xbf16>
    %9 = stablehlo.broadcast_in_dim %cst_10, dims = [] : (tensor<bf16>) -> tensor<8x32x1xbf16>
    %10 = stablehlo.broadcast_in_dim %cst_9, dims = [] : (tensor<bf16>) -> tensor<8x32xbf16>
    %11 = stablehlo.broadcast_in_dim %c_5, dims = [] : (tensor<i64>) -> tensor<8x32xi64>
    %12 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<8x32xi64>
    %13 = stablehlo.reshape %arg18 : (tensor<50272x768xbf16>) -> tensor<1x50272x768xbf16>
    %14 = stablehlo.reshape %13 : (tensor<1x50272x768xbf16>) -> tensor<50272x768xbf16>
    %15 = stablehlo.reshape %arg0 : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %16 = stablehlo.reshape %15 : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %17 = stablehlo.reshape %15 : (tensor<1x8x32xi64>) -> tensor<256xi64>
    %18 = stablehlo.convert %17 : (tensor<256xi64>) -> tensor<256xui32>
    %19 = "stablehlo.gather"(%14, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<50272x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %20 = stablehlo.reshape %19 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %21 = stablehlo.reshape %arg17 : (tensor<2050x768xbf16>) -> tensor<1x2050x768xbf16>
    %22 = stablehlo.reshape %21 : (tensor<1x2050x768xbf16>) -> tensor<2050x768xbf16>
    %23 = stablehlo.reshape %arg16 : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %24 = stablehlo.reshape %23 : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %25 = "stablehlo.reduce_window"(%24, %c) <{padding = dense<[[0, 0], [31, 0]]> : tensor<2x2xi64>, window_dimensions = array<i64: 1, 32>}> ({
    ^bb0(%arg23: tensor<i64>, %arg24: tensor<i64>):
      %221 = stablehlo.add %arg23, %arg24 : tensor<i64>
      stablehlo.return %221 : tensor<i64>
    }) : (tensor<8x32xi64>, tensor<i64>) -> tensor<8x32xi64>
    %26 = stablehlo.multiply %25, %24 : tensor<8x32xi64>
    %27 = stablehlo.subtract %26, %12 : tensor<8x32xi64>
    %28 = stablehlo.add %27, %11 : tensor<8x32xi64>
    %29 = stablehlo.reshape %28 : (tensor<8x32xi64>) -> tensor<256xi64>
    %30 = stablehlo.convert %29 : (tensor<256xi64>) -> tensor<256xui32>
    %31 = "stablehlo.gather"(%22, %30) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> : (tensor<2050x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %32 = stablehlo.reshape %31 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %33 = stablehlo.add %20, %32 : tensor<8x32x768xbf16>
    %34 = stablehlo.reduce(%33 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %35 = stablehlo.multiply %34, %10 : tensor<8x32xbf16>
    %36 = stablehlo.broadcast_in_dim %35, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %37 = stablehlo.subtract %33, %36 : tensor<8x32x768xbf16>
    %38 = stablehlo.multiply %37, %37 : tensor<8x32x768xbf16>
    %39 = stablehlo.reduce(%38 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %40 = stablehlo.multiply %39, %10 : tensor<8x32xbf16>
    %41 = stablehlo.reshape %40 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %42 = stablehlo.add %41, %9 : tensor<8x32x1xbf16>
    %43 = stablehlo.rsqrt %42 : tensor<8x32x1xbf16>
    %44 = stablehlo.reshape %43 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %46 = stablehlo.multiply %37, %45 : tensor<8x32x768xbf16>
    %47 = stablehlo.reshape %arg15 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %48 = stablehlo.reshape %47 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %49 = stablehlo.broadcast_in_dim %48, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %50 = stablehlo.multiply %46, %49 : tensor<8x32x768xbf16>
    %51 = stablehlo.reshape %arg14 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %53 = stablehlo.broadcast_in_dim %52, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %54 = stablehlo.add %50, %53 : tensor<8x32x768xbf16>
    %55 = stablehlo.reshape %54 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %56 = stablehlo.reshape %arg22 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %57 = stablehlo.reshape %56 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %58 = stablehlo.transpose %57, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %59 = stablehlo.dot_general %55, %58, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %60 = stablehlo.reshape %59 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %61 = stablehlo.reshape %arg21 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %63 = stablehlo.broadcast_in_dim %62, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %64 = stablehlo.add %60, %63 : tensor<8x32x768xbf16>
    %65 = stablehlo.multiply %64, %8 : tensor<8x32x768xbf16>
    %66 = stablehlo.reshape %65 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %67 = stablehlo.transpose %66, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %68 = stablehlo.reshape %67 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %69 = stablehlo.reshape %arg20 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %70 = stablehlo.reshape %69 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %71 = stablehlo.transpose %70, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %72 = stablehlo.dot_general %55, %71, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %73 = stablehlo.reshape %72 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %74 = stablehlo.reshape %arg19 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %76 = stablehlo.broadcast_in_dim %75, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %77 = stablehlo.add %73, %76 : tensor<8x32x768xbf16>
    %78 = stablehlo.reshape %77 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 3, 1] : (tensor<8x32x12x64xbf16>) -> tensor<8x12x64x32xbf16>
    %80 = stablehlo.reshape %79 : (tensor<8x12x64x32xbf16>) -> tensor<96x64x32xbf16>
    %81 = stablehlo.dot_general %68, %80, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x64xbf16>, tensor<96x64x32xbf16>) -> tensor<96x32x32xbf16>
    %82 = stablehlo.reshape %81 : (tensor<96x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %83 = stablehlo.broadcast_in_dim %c_1, dims = [1] : (tensor<32xi64>) -> tensor<32x32xi64>
    %84 = stablehlo.broadcast_in_dim %c_1, dims = [0] : (tensor<32xi64>) -> tensor<32x32xi64>
    %85 = stablehlo.subtract %83, %84 : tensor<32x32xi64>
    %86 = stablehlo.compare  GE, %85, %7 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %87 = stablehlo.select %86, %6, %5 : tensor<32x32xi1>, tensor<32x32xbf16>
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %89 = stablehlo.convert %88 : (tensor<32x32xi1>) -> tensor<32x32xbf16>
    %90 = stablehlo.multiply %87, %89 : tensor<32x32xbf16>
    %91 = stablehlo.reshape %90 : (tensor<32x32xbf16>) -> tensor<1x32x32xbf16>
    %92 = stablehlo.broadcast_in_dim %91, dims = [1, 2, 3] : (tensor<1x32x32xbf16>) -> tensor<8x1x32x32xbf16>
    %93 = stablehlo.reshape %23 : (tensor<1x8x32xi64>) -> tensor<8x1x1x32xi64>
    %94 = stablehlo.convert %93 : (tensor<8x1x1x32xi64>) -> tensor<8x1x1x32xbf16>
    %95 = stablehlo.reshape %94 : (tensor<8x1x1x32xbf16>) -> tensor<8x1x32xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] : (tensor<8x1x32xbf16>) -> tensor<8x1x32x32xbf16>
    %97 = stablehlo.add %92, %96 : tensor<8x1x32x32xbf16>
    %98 = stablehlo.compare  EQ, %97, %4 : (tensor<8x1x32x32xbf16>, tensor<8x1x32x32xbf16>) -> tensor<8x1x32x32xi1>
    %99 = stablehlo.select %98, %3, %92 : tensor<8x1x32x32xi1>, tensor<8x1x32x32xbf16>
    %100 = stablehlo.reshape %99 : (tensor<8x1x32x32xbf16>) -> tensor<8x32x32xbf16>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] : (tensor<8x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %102 = stablehlo.add %82, %101 : tensor<8x12x32x32xbf16>
    %103 = stablehlo.convert %102 : (tensor<8x12x32x32xbf16>) -> tensor<8x12x32x32xf32>
    %104 = stablehlo.reduce(%103 init: %cst) applies stablehlo.maximum across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %106 = stablehlo.subtract %103, %105 : tensor<8x12x32x32xf32>
    %107 = stablehlo.exponential %106 : tensor<8x12x32x32xf32>
    %108 = stablehlo.reduce(%107 init: %cst_2) applies stablehlo.add across dimensions = [3] : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %110 = stablehlo.divide %107, %109 : tensor<8x12x32x32xf32>
    %111 = stablehlo.convert %110 : (tensor<8x12x32x32xf32>) -> tensor<8x12x32x32xbf16>
    %112 = stablehlo.reshape %111 : (tensor<8x12x32x32xbf16>) -> tensor<96x32x32xbf16>
    %113 = stablehlo.reshape %arg13 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %114 = stablehlo.reshape %113 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %115 = stablehlo.transpose %114, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %116 = stablehlo.dot_general %55, %115, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %117 = stablehlo.reshape %116 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %118 = stablehlo.reshape %arg12 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %121 = stablehlo.add %117, %120 : tensor<8x32x768xbf16>
    %122 = stablehlo.reshape %121 : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %123 = stablehlo.transpose %122, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %124 = stablehlo.reshape %123 : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %125 = stablehlo.dot_general %112, %124, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<96x32x32xbf16>, tensor<96x32x64xbf16>) -> tensor<96x32x64xbf16>
    %126 = stablehlo.reshape %125 : (tensor<96x32x64xbf16>) -> tensor<8x12x32x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[8,32,12,64]{3,1,2,0}"} : (tensor<8x12x32x64xbf16>) -> tensor<8x32x12x64xbf16>
    %128 = stablehlo.reshape %127 : (tensor<8x32x12x64xbf16>) -> tensor<256x768xbf16>
    %129 = stablehlo.reshape %arg11 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %131 = stablehlo.transpose %130, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %132 = stablehlo.dot_general %128, %131, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %133 = stablehlo.reshape %132 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %134 = stablehlo.reshape %arg10 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %135 = stablehlo.reshape %134 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %136 = stablehlo.broadcast_in_dim %135, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %137 = stablehlo.add %133, %136 : tensor<8x32x768xbf16>
    %138 = stablehlo.add %33, %137 : tensor<8x32x768xbf16>
    %139 = stablehlo.reshape %138 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %140 = stablehlo.reduce(%139 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %141 = stablehlo.multiply %140, %2 : tensor<256xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %143 = stablehlo.subtract %139, %142 : tensor<256x768xbf16>
    %144 = stablehlo.multiply %143, %143 : tensor<256x768xbf16>
    %145 = stablehlo.reduce(%144 init: %cst_14) applies stablehlo.add across dimensions = [1] : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %146 = stablehlo.multiply %145, %2 : tensor<256xbf16>
    %147 = stablehlo.reshape %146 : (tensor<256xbf16>) -> tensor<256x1xbf16>
    %148 = stablehlo.add %147, %1 : tensor<256x1xbf16>
    %149 = stablehlo.rsqrt %148 : tensor<256x1xbf16>
    %150 = stablehlo.reshape %149 : (tensor<256x1xbf16>) -> tensor<256xbf16>
    %151 = stablehlo.broadcast_in_dim %150, dims = [0] : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %152 = stablehlo.multiply %143, %151 : tensor<256x768xbf16>
    %153 = stablehlo.reshape %arg9 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %155 = stablehlo.broadcast_in_dim %154, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %156 = stablehlo.multiply %152, %155 : tensor<256x768xbf16>
    %157 = stablehlo.reshape %arg8 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %158 = stablehlo.reshape %157 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %159 = stablehlo.broadcast_in_dim %158, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %160 = stablehlo.add %156, %159 : tensor<256x768xbf16>
    %161 = stablehlo.reshape %arg7 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16>
    %162 = stablehlo.reshape %161 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16>
    %163 = stablehlo.transpose %162, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16>
    %164 = stablehlo.dot_general %160, %163, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x3072xbf16>) -> tensor<256x3072xbf16>
    %165 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %166 = stablehlo.reshape %165 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %167 = stablehlo.broadcast_in_dim %166, dims = [1] : (tensor<3072xbf16>) -> tensor<256x3072xbf16>
    %168 = stablehlo.add %164, %167 : tensor<256x3072xbf16>
    %169 = stablehlo.maximum %168, %0 : tensor<256x3072xbf16>
    %170 = stablehlo.reshape %arg5 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16>
    %172 = stablehlo.transpose %171, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16>
    %173 = stablehlo.dot_general %169, %172, contracting_dims = [1] x [0] : (tensor<256x3072xbf16>, tensor<3072x768xbf16>) -> tensor<256x768xbf16>
    %174 = stablehlo.reshape %arg4 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %175 = stablehlo.reshape %174 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %176 = stablehlo.broadcast_in_dim %175, dims = [1] : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %177 = stablehlo.add %173, %176 : tensor<256x768xbf16>
    %178 = stablehlo.add %139, %177 : tensor<256x768xbf16>
    %179 = stablehlo.reshape %178 : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %180 = stablehlo.reduce(%179 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %181 = stablehlo.multiply %180, %10 : tensor<8x32xbf16>
    %182 = stablehlo.broadcast_in_dim %181, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %183 = stablehlo.subtract %179, %182 : tensor<8x32x768xbf16>
    %184 = stablehlo.multiply %183, %183 : tensor<8x32x768xbf16>
    %185 = stablehlo.reduce(%184 init: %cst_14) applies stablehlo.add across dimensions = [2] : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %186 = stablehlo.multiply %185, %10 : tensor<8x32xbf16>
    %187 = stablehlo.reshape %186 : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %188 = stablehlo.add %187, %9 : tensor<8x32x1xbf16>
    %189 = stablehlo.rsqrt %188 : tensor<8x32x1xbf16>
    %190 = stablehlo.reshape %189 : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %191 = stablehlo.broadcast_in_dim %190, dims = [0, 1] : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %192 = stablehlo.multiply %183, %191 : tensor<8x32x768xbf16>
    %193 = stablehlo.reshape %arg3 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %194 = stablehlo.reshape %193 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %195 = stablehlo.broadcast_in_dim %194, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %196 = stablehlo.multiply %192, %195 : tensor<8x32x768xbf16>
    %197 = stablehlo.reshape %arg2 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %198 = stablehlo.reshape %197 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %199 = stablehlo.broadcast_in_dim %198, dims = [2] : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %200 = stablehlo.add %196, %199 : tensor<8x32x768xbf16>
    %201 = stablehlo.reshape %200 : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %202 = stablehlo.reshape %arg1 : (tensor<2x768xbf16>) -> tensor<1x2x768xbf16>
    %203 = stablehlo.reshape %202 : (tensor<1x2x768xbf16>) -> tensor<2x768xbf16>
    %204 = stablehlo.transpose %203, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,2]{0,1}"} : (tensor<2x768xbf16>) -> tensor<768x2xbf16>
    %205 = stablehlo.dot_general %201, %204, contracting_dims = [1] x [0] : (tensor<256x768xbf16>, tensor<768x2xbf16>) -> tensor<256x2xbf16>
    %206 = stablehlo.reshape %205 : (tensor<256x2xbf16>) -> tensor<8x32x2xbf16>
    %207 = stablehlo.broadcast_in_dim %c_3, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32>
    %208 = stablehlo.compare  NE, %16, %12 : (tensor<8x32xi64>, tensor<8x32xi64>) -> tensor<8x32xi1>
    %209 = stablehlo.convert %208 : (tensor<8x32xi1>) -> tensor<8x32xi32>
    %210 = stablehlo.multiply %207, %209 : tensor<8x32xi32>
    %211 = stablehlo.iota dim = 0 : tensor<32xi32>
    %212 = stablehlo.broadcast_in_dim %211, dims = [1] : (tensor<32xi32>) -> tensor<8x32xi32>
    %213:2 = stablehlo.reduce(%210 init: %c_4), (%212 init: %c_0) across dimensions = [1] : (tensor<8x32xi32>, tensor<8x32xi32>, tensor<i32>, tensor<i32>) -> (tensor<8xi32>, tensor<8xi32>)
     reducer(%arg23: tensor<i32>, %arg25: tensor<i32>) (%arg24: tensor<i32>, %arg26: tensor<i32>)  {
      %221 = stablehlo.compare  GE, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %222 = stablehlo.select %221, %arg23, %arg25 : tensor<i1>, tensor<i32>
      %223 = stablehlo.compare  EQ, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %224 = stablehlo.minimum %arg24, %arg26 : tensor<i32>
      %225 = stablehlo.select %221, %arg24, %arg26 : tensor<i1>, tensor<i32>
      %226 = stablehlo.select %223, %224, %225 : tensor<i1>, tensor<i32>
      stablehlo.return %222, %226 : tensor<i32>, tensor<i32>
    }
    %214 = stablehlo.convert %213#1 : (tensor<8xi32>) -> tensor<8xi64>
    %215 = stablehlo.compare  LT, %214, %c_12 : (tensor<8xi64>, tensor<8xi64>) -> tensor<8xi1>
    %216 = stablehlo.add %214, %c_11 : tensor<8xi64>
    %217 = stablehlo.select %215, %216, %214 : tensor<8xi1>, tensor<8xi64>
    %218 = stablehlo.reshape %217 : (tensor<8xi64>) -> tensor<8x1xi64>
    %219 = stablehlo.concatenate %c_13, %218, dim = 1 : (tensor<8x1xi64>, tensor<8x1xi64>) -> tensor<8x2xi64>
    %220 = "stablehlo.gather"(%206, %219) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1, 2>}> : (tensor<8x32x2xbf16>, tensor<8x2xi64>) -> tensor<8x2xbf16>
    return %220 : tensor<8x2xbf16>
  }
}


// -----// IR Dump After AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @SyncTensorsGraph.630) //----- //
module @SyncTensorsGraph.630 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<2x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___score_weight"}, %arg2: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_final_layer_norm_bias"}, %arg3: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_final_layer_norm_weight"}, %arg4: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc2_bias"}, %arg5: tensor<768x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc2_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc1_bias"}, %arg7: tensor<3072x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc1_weight"}, %arg8: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_bias"}, %arg9: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_weight"}, %arg10: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_bias"}, %arg11: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_weight"}, %arg12: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_bias"}, %arg13: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_weight"}, %arg14: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_bias"}, %arg15: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_weight"}, %arg16: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg17: tensor<2050x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_embed_positions_weight"}, %arg18: tensor<50272x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_embed_tokens_weight"}, %arg19: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_bias"}, %arg20: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_weight"}, %arg21: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_bias"}, %arg22: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_weight"}) -> (tensor<8x2xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %c = stablehlo.constant dense<0> : tensor<i64>
    %c_0 = stablehlo.constant dense<0> : tensor<i32>
    %c_1 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64>
    %cst = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c_3 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi32>
    %c_4 = stablehlo.constant dense<-2147483648> : tensor<i32>
    %c_5 = stablehlo.constant dense<2> : tensor<i64>
    %cst_6 = stablehlo.constant dense<1.250000e-01> : tensor<bf16>
    %c_7 = stablehlo.constant dense<1> : tensor<i64>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %cst_9 = stablehlo.constant dense<1.304630e-03> : tensor<bf16>
    %cst_10 = stablehlo.constant dense<1.001360e-05> : tensor<bf16>
    %c_11 = stablehlo.constant {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} dense<32> : tensor<8xi64>
    %c_12 = stablehlo.constant {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} dense<0> : tensor<8xi64>
    %c_13 = stablehlo.constant {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} dense<[[0], [1], [2], [3], [4], [5], [6], [7]]> : tensor<8x1xi64>
    %cst_14 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_14, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<bf16>) -> tensor<256x3072xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_10, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<bf16>) -> tensor<256x1xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_9, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<256xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %4 = stablehlo.broadcast_in_dim %cst_14, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %5 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %7 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<32x32xi64>
    %8 = stablehlo.broadcast_in_dim %cst_6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<8x32x768xbf16>
    %9 = stablehlo.broadcast_in_dim %cst_10, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<8x32x1xbf16>
    %10 = stablehlo.broadcast_in_dim %cst_9, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<bf16>) -> tensor<8x32xbf16>
    %11 = stablehlo.broadcast_in_dim %c_5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<i64>) -> tensor<8x32xi64>
    %12 = stablehlo.broadcast_in_dim %c_7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<i64>) -> tensor<8x32xi64>
    %13 = stablehlo.reshape %arg18 : (tensor<50272x768xbf16>) -> tensor<1x50272x768xbf16>
    %14 = stablehlo.reshape %13 : (tensor<1x50272x768xbf16>) -> tensor<50272x768xbf16>
    %15 = stablehlo.reshape %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %16 = stablehlo.reshape %15 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %17 = stablehlo.reshape %15 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<1x8x32xi64>) -> tensor<256xi64>
    %18 = stablehlo.convert %17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<256xi64>) -> tensor<256xui32>
    %19 = "stablehlo.gather"(%14, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<50272x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %20 = stablehlo.reshape %19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %21 = stablehlo.reshape %arg17 : (tensor<2050x768xbf16>) -> tensor<1x2050x768xbf16>
    %22 = stablehlo.reshape %21 : (tensor<1x2050x768xbf16>) -> tensor<2050x768xbf16>
    %23 = stablehlo.reshape %arg16 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %24 = stablehlo.reshape %23 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %25 = "stablehlo.reduce_window"(%24, %c) <{padding = dense<[[0, 0], [31, 0]]> : tensor<2x2xi64>, window_dimensions = array<i64: 1, 32>}> ({
    ^bb0(%arg23: tensor<i64>, %arg24: tensor<i64>):
      %221 = stablehlo.add %arg23, %arg24 : tensor<i64>
      stablehlo.return %221 : tensor<i64>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi64>, tensor<i64>) -> tensor<8x32xi64>
    %26 = stablehlo.multiply %25, %24 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xi64>
    %27 = stablehlo.subtract %26, %12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xi64>
    %28 = stablehlo.add %27, %11 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xi64>
    %29 = stablehlo.reshape %28 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<8x32xi64>) -> tensor<256xi64>
    %30 = stablehlo.convert %29 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<256xi64>) -> tensor<256xui32>
    %31 = "stablehlo.gather"(%22, %30) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<2050x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %32 = stablehlo.reshape %31 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %33 = stablehlo.add %20, %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %34 = stablehlo.reduce(%33 init: %cst_14) applies stablehlo.add across dimensions = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %35 = stablehlo.multiply %34, %10 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xbf16>
    %36 = stablehlo.broadcast_in_dim %35, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %37 = stablehlo.subtract %33, %36 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %38 = stablehlo.multiply %37, %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %39 = stablehlo.reduce(%38 init: %cst_14) applies stablehlo.add across dimensions = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %40 = stablehlo.multiply %39, %10 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %42 = stablehlo.add %41, %9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x1xbf16>
    %43 = stablehlo.rsqrt %42 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x1xbf16>
    %44 = stablehlo.reshape %43 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %46 = stablehlo.multiply %37, %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %47 = stablehlo.reshape %arg15 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %48 = stablehlo.reshape %47 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %49 = stablehlo.broadcast_in_dim %48, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %50 = stablehlo.multiply %46, %49 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %51 = stablehlo.reshape %arg14 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %53 = stablehlo.broadcast_in_dim %52, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %54 = stablehlo.add %50, %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %55 = stablehlo.reshape %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %56 = stablehlo.reshape %arg22 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %57 = stablehlo.reshape %56 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %58 = stablehlo.transpose %57, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %59 = stablehlo.dot_general %55, %58, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %60 = stablehlo.reshape %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %61 = stablehlo.reshape %arg21 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %63 = stablehlo.broadcast_in_dim %62, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %64 = stablehlo.add %60, %63 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %65 = stablehlo.multiply %64, %8 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %66 = stablehlo.reshape %65 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %67 = stablehlo.transpose %66, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %68 = stablehlo.reshape %67 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %69 = stablehlo.reshape %arg20 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %70 = stablehlo.reshape %69 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %71 = stablehlo.transpose %70, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %72 = stablehlo.dot_general %55, %71, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %73 = stablehlo.reshape %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %74 = stablehlo.reshape %arg19 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %76 = stablehlo.broadcast_in_dim %75, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %77 = stablehlo.add %73, %76 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 3, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x64x32xbf16>
    %80 = stablehlo.reshape %79 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x64x32xbf16>) -> tensor<96x64x32xbf16>
    %81 = stablehlo.dot_general %68, %80, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<96x32x64xbf16>, tensor<96x64x32xbf16>) -> tensor<96x32x32xbf16>
    %82 = stablehlo.reshape %81 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<96x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %83 = stablehlo.broadcast_in_dim %c_1, dims = [1] : (tensor<32xi64>) -> tensor<32x32xi64>
    %84 = stablehlo.broadcast_in_dim %c_1, dims = [0] : (tensor<32xi64>) -> tensor<32x32xi64>
    %85 = stablehlo.subtract %83, %84 : tensor<32x32xi64>
    %86 = stablehlo.compare  GE, %85, %7 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %87 = stablehlo.select %86, %6, %5 : tensor<32x32xi1>, tensor<32x32xbf16>
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %89 = stablehlo.convert %88 : (tensor<32x32xi1>) -> tensor<32x32xbf16>
    %90 = stablehlo.multiply %87, %89 : tensor<32x32xbf16>
    %91 = stablehlo.reshape %90 : (tensor<32x32xbf16>) -> tensor<1x32x32xbf16>
    %92 = stablehlo.broadcast_in_dim %91, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x32x32xbf16>) -> tensor<8x1x32x32xbf16>
    %93 = stablehlo.reshape %23 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x32xi64>) -> tensor<8x1x1x32xi64>
    %94 = stablehlo.convert %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x1x1x32xi64>) -> tensor<8x1x1x32xbf16>
    %95 = stablehlo.reshape %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x1x1x32xbf16>) -> tensor<8x1x32xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x1x32xbf16>) -> tensor<8x1x32x32xbf16>
    %97 = stablehlo.add %92, %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<8x1x32x32xbf16>
    %98 = stablehlo.compare  EQ, %97, %4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x1x32x32xbf16>, tensor<8x1x32x32xbf16>) -> tensor<8x1x32x32xi1>
    %99 = stablehlo.select %98, %3, %92 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<8x1x32x32xi1>, tensor<8x1x32x32xbf16>
    %100 = stablehlo.reshape %99 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x1x32x32xbf16>) -> tensor<8x32x32xbf16>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %102 = stablehlo.add %82, %101 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<8x12x32x32xbf16>
    %103 = stablehlo.convert %102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x12x32x32xbf16>) -> tensor<8x12x32x32xf32>
    %104 = stablehlo.reduce(%103 init: %cst) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %106 = stablehlo.subtract %103, %105 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<8x12x32x32xf32>
    %107 = stablehlo.exponential %106 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<8x12x32x32xf32>
    %108 = stablehlo.reduce(%107 init: %cst_2) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %110 = stablehlo.divide %107, %109 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<8x12x32x32xf32>
    %111 = stablehlo.convert %110 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x12x32x32xf32>) -> tensor<8x12x32x32xbf16>
    %112 = stablehlo.reshape %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x32x32xbf16>) -> tensor<96x32x32xbf16>
    %113 = stablehlo.reshape %arg13 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %114 = stablehlo.reshape %113 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %115 = stablehlo.transpose %114, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %116 = stablehlo.dot_general %55, %115, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %117 = stablehlo.reshape %116 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %118 = stablehlo.reshape %arg12 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %121 = stablehlo.add %117, %120 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %122 = stablehlo.reshape %121 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %123 = stablehlo.transpose %122, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %124 = stablehlo.reshape %123 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %125 = stablehlo.dot_general %112, %124, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<96x32x32xbf16>, tensor<96x32x64xbf16>) -> tensor<96x32x64xbf16>
    %126 = stablehlo.reshape %125 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<96x32x64xbf16>) -> tensor<8x12x32x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, xla_shape = "bf16[8,32,12,64]{3,1,2,0}"} : (tensor<8x12x32x64xbf16>) -> tensor<8x32x12x64xbf16>
    %128 = stablehlo.reshape %127 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x12x64xbf16>) -> tensor<256x768xbf16>
    %129 = stablehlo.reshape %arg11 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %131 = stablehlo.transpose %130, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %132 = stablehlo.dot_general %128, %131, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %133 = stablehlo.reshape %132 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %134 = stablehlo.reshape %arg10 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %135 = stablehlo.reshape %134 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %136 = stablehlo.broadcast_in_dim %135, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %137 = stablehlo.add %133, %136 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %138 = stablehlo.add %33, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %139 = stablehlo.reshape %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %140 = stablehlo.reduce(%139 init: %cst_14) applies stablehlo.add across dimensions = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %141 = stablehlo.multiply %140, %2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : tensor<256xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %143 = stablehlo.subtract %139, %142 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x768xbf16>
    %144 = stablehlo.multiply %143, %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x768xbf16>
    %145 = stablehlo.reduce(%144 init: %cst_14) applies stablehlo.add across dimensions = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %146 = stablehlo.multiply %145, %2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : tensor<256xbf16>
    %147 = stablehlo.reshape %146 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256xbf16>) -> tensor<256x1xbf16>
    %148 = stablehlo.add %147, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x1xbf16>
    %149 = stablehlo.rsqrt %148 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x1xbf16>
    %150 = stablehlo.reshape %149 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<256x1xbf16>) -> tensor<256xbf16>
    %151 = stablehlo.broadcast_in_dim %150, dims = [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %152 = stablehlo.multiply %143, %151 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x768xbf16>
    %153 = stablehlo.reshape %arg9 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %155 = stablehlo.broadcast_in_dim %154, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %156 = stablehlo.multiply %152, %155 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x768xbf16>
    %157 = stablehlo.reshape %arg8 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %158 = stablehlo.reshape %157 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %159 = stablehlo.broadcast_in_dim %158, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %160 = stablehlo.add %156, %159 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x768xbf16>
    %161 = stablehlo.reshape %arg7 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16>
    %162 = stablehlo.reshape %161 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16>
    %163 = stablehlo.transpose %162, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16>
    %164 = stablehlo.dot_general %160, %163, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x3072xbf16>) -> tensor<256x3072xbf16>
    %165 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %166 = stablehlo.reshape %165 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %167 = stablehlo.broadcast_in_dim %166, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<3072xbf16>) -> tensor<256x3072xbf16>
    %168 = stablehlo.add %164, %167 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x3072xbf16>
    %169 = stablehlo.maximum %168, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x3072xbf16>
    %170 = stablehlo.reshape %arg5 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16>
    %172 = stablehlo.transpose %171, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16>
    %173 = stablehlo.dot_general %169, %172, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x3072xbf16>, tensor<3072x768xbf16>) -> tensor<256x768xbf16>
    %174 = stablehlo.reshape %arg4 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %175 = stablehlo.reshape %174 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %176 = stablehlo.broadcast_in_dim %175, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %177 = stablehlo.add %173, %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x768xbf16>
    %178 = stablehlo.add %139, %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x768xbf16>
    %179 = stablehlo.reshape %178 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %180 = stablehlo.reduce(%179 init: %cst_14) applies stablehlo.add across dimensions = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %181 = stablehlo.multiply %180, %10 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xbf16>
    %182 = stablehlo.broadcast_in_dim %181, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %183 = stablehlo.subtract %179, %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %184 = stablehlo.multiply %183, %183 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %185 = stablehlo.reduce(%184 init: %cst_14) applies stablehlo.add across dimensions = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %186 = stablehlo.multiply %185, %10 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xbf16>
    %187 = stablehlo.reshape %186 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %188 = stablehlo.add %187, %9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x1xbf16>
    %189 = stablehlo.rsqrt %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x1xbf16>
    %190 = stablehlo.reshape %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %191 = stablehlo.broadcast_in_dim %190, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %192 = stablehlo.multiply %183, %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %193 = stablehlo.reshape %arg3 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %194 = stablehlo.reshape %193 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %195 = stablehlo.broadcast_in_dim %194, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %196 = stablehlo.multiply %192, %195 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %197 = stablehlo.reshape %arg2 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %198 = stablehlo.reshape %197 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %199 = stablehlo.broadcast_in_dim %198, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %200 = stablehlo.add %196, %199 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %201 = stablehlo.reshape %200 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %202 = stablehlo.reshape %arg1 : (tensor<2x768xbf16>) -> tensor<1x2x768xbf16>
    %203 = stablehlo.reshape %202 : (tensor<1x2x768xbf16>) -> tensor<2x768xbf16>
    %204 = stablehlo.transpose %203, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,2]{0,1}"} : (tensor<2x768xbf16>) -> tensor<768x2xbf16>
    %205 = stablehlo.dot_general %201, %204, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x2xbf16>) -> tensor<256x2xbf16>
    %206 = stablehlo.reshape %205 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x2xbf16>) -> tensor<8x32x2xbf16>
    %207 = stablehlo.broadcast_in_dim %c_3, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<32xi32>) -> tensor<8x32xi32>
    %208 = stablehlo.compare  NE, %16, %12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi64>, tensor<8x32xi64>) -> tensor<8x32xi1>
    %209 = stablehlo.convert %208 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi1>) -> tensor<8x32xi32>
    %210 = stablehlo.multiply %207, %209 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xi32>
    %211 = stablehlo.iota dim = 0 : tensor<32xi32>
    %212 = stablehlo.broadcast_in_dim %211, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<32xi32>) -> tensor<8x32xi32>
    %213:2 = stablehlo.reduce(%210 init: %c_4), (%212 init: %c_0) across dimensions = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}]>]>} : (tensor<8x32xi32>, tensor<8x32xi32>, tensor<i32>, tensor<i32>) -> (tensor<8xi32>, tensor<8xi32>)
     reducer(%arg23: tensor<i32>, %arg25: tensor<i32>) (%arg24: tensor<i32>, %arg26: tensor<i32>)  {
      %221 = stablehlo.compare  GE, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %222 = stablehlo.select %221, %arg23, %arg25 : tensor<i1>, tensor<i32>
      %223 = stablehlo.compare  EQ, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %224 = stablehlo.minimum %arg24, %arg26 : tensor<i32>
      %225 = stablehlo.select %221, %arg24, %arg26 : tensor<i1>, tensor<i32>
      %226 = stablehlo.select %223, %224, %225 : tensor<i1>, tensor<i32>
      stablehlo.return %222, %226 : tensor<i32>, tensor<i32>
    }
    %214 = stablehlo.convert %213#1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<8xi32>) -> tensor<8xi64>
    %215 = stablehlo.compare  LT, %214, %c_12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<8xi64>, tensor<8xi64>) -> tensor<8xi1>
    %216 = stablehlo.add %214, %c_11 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : tensor<8xi64>
    %217 = stablehlo.select %215, %216, %214 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : tensor<8xi1>, tensor<8xi64>
    %218 = stablehlo.reshape %217 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8xi64>) -> tensor<8x1xi64>
    %219 = stablehlo.concatenate %c_13, %218, dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x1xi64>, tensor<8x1xi64>) -> tensor<8x2xi64>
    %220 = "stablehlo.gather"(%206, %219) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1, 2>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x2xbf16>, tensor<8x2xi64>) -> tensor<8x2xbf16>
    return %220 : tensor<8x2xbf16>
  }
}


// -----// IR Dump Before ShardingConstraintToReshardPass (sdy-sharding-constraint-to-reshard) ('func.func' operation: @main) //----- //
module @SyncTensorsGraph.630 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<2x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___score_weight"}, %arg2: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_final_layer_norm_bias"}, %arg3: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_final_layer_norm_weight"}, %arg4: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc2_bias"}, %arg5: tensor<768x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc2_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc1_bias"}, %arg7: tensor<3072x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc1_weight"}, %arg8: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_bias"}, %arg9: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_weight"}, %arg10: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_bias"}, %arg11: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_weight"}, %arg12: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_bias"}, %arg13: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_weight"}, %arg14: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_bias"}, %arg15: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_weight"}, %arg16: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg17: tensor<2050x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_embed_positions_weight"}, %arg18: tensor<50272x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_embed_tokens_weight"}, %arg19: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_bias"}, %arg20: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_weight"}, %arg21: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_bias"}, %arg22: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_weight"}) -> (tensor<8x2xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %c = stablehlo.constant dense<0> : tensor<i64>
    %c_0 = stablehlo.constant dense<0> : tensor<i32>
    %c_1 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64>
    %cst = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c_3 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi32>
    %c_4 = stablehlo.constant dense<-2147483648> : tensor<i32>
    %c_5 = stablehlo.constant dense<2> : tensor<i64>
    %cst_6 = stablehlo.constant dense<1.250000e-01> : tensor<bf16>
    %c_7 = stablehlo.constant dense<1> : tensor<i64>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %cst_9 = stablehlo.constant dense<1.304630e-03> : tensor<bf16>
    %cst_10 = stablehlo.constant dense<1.001360e-05> : tensor<bf16>
    %c_11 = stablehlo.constant {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} dense<32> : tensor<8xi64>
    %c_12 = stablehlo.constant {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} dense<0> : tensor<8xi64>
    %c_13 = stablehlo.constant {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} dense<[[0], [1], [2], [3], [4], [5], [6], [7]]> : tensor<8x1xi64>
    %cst_14 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_14, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<bf16>) -> tensor<256x3072xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_10, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<bf16>) -> tensor<256x1xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_9, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<256xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %4 = stablehlo.broadcast_in_dim %cst_14, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %5 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %7 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<32x32xi64>
    %8 = stablehlo.broadcast_in_dim %cst_6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<8x32x768xbf16>
    %9 = stablehlo.broadcast_in_dim %cst_10, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<8x32x1xbf16>
    %10 = stablehlo.broadcast_in_dim %cst_9, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<bf16>) -> tensor<8x32xbf16>
    %11 = stablehlo.broadcast_in_dim %c_5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<i64>) -> tensor<8x32xi64>
    %12 = stablehlo.broadcast_in_dim %c_7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<i64>) -> tensor<8x32xi64>
    %13 = stablehlo.reshape %arg18 : (tensor<50272x768xbf16>) -> tensor<1x50272x768xbf16>
    %14 = stablehlo.reshape %13 : (tensor<1x50272x768xbf16>) -> tensor<50272x768xbf16>
    %15 = stablehlo.reshape %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %16 = stablehlo.reshape %15 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %17 = stablehlo.reshape %15 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<1x8x32xi64>) -> tensor<256xi64>
    %18 = stablehlo.convert %17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<256xi64>) -> tensor<256xui32>
    %19 = "stablehlo.gather"(%14, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<50272x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %20 = stablehlo.reshape %19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %21 = stablehlo.reshape %arg17 : (tensor<2050x768xbf16>) -> tensor<1x2050x768xbf16>
    %22 = stablehlo.reshape %21 : (tensor<1x2050x768xbf16>) -> tensor<2050x768xbf16>
    %23 = stablehlo.reshape %arg16 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %24 = stablehlo.reshape %23 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %25 = "stablehlo.reduce_window"(%24, %c) <{padding = dense<[[0, 0], [31, 0]]> : tensor<2x2xi64>, window_dimensions = array<i64: 1, 32>}> ({
    ^bb0(%arg23: tensor<i64>, %arg24: tensor<i64>):
      %221 = stablehlo.add %arg23, %arg24 : tensor<i64>
      stablehlo.return %221 : tensor<i64>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi64>, tensor<i64>) -> tensor<8x32xi64>
    %26 = stablehlo.multiply %25, %24 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xi64>
    %27 = stablehlo.subtract %26, %12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xi64>
    %28 = stablehlo.add %27, %11 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xi64>
    %29 = stablehlo.reshape %28 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<8x32xi64>) -> tensor<256xi64>
    %30 = stablehlo.convert %29 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<256xi64>) -> tensor<256xui32>
    %31 = "stablehlo.gather"(%22, %30) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<2050x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %32 = stablehlo.reshape %31 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %33 = stablehlo.add %20, %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %34 = stablehlo.reduce(%33 init: %cst_14) applies stablehlo.add across dimensions = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %35 = stablehlo.multiply %34, %10 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xbf16>
    %36 = stablehlo.broadcast_in_dim %35, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %37 = stablehlo.subtract %33, %36 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %38 = stablehlo.multiply %37, %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %39 = stablehlo.reduce(%38 init: %cst_14) applies stablehlo.add across dimensions = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %40 = stablehlo.multiply %39, %10 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %42 = stablehlo.add %41, %9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x1xbf16>
    %43 = stablehlo.rsqrt %42 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x1xbf16>
    %44 = stablehlo.reshape %43 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %46 = stablehlo.multiply %37, %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %47 = stablehlo.reshape %arg15 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %48 = stablehlo.reshape %47 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %49 = stablehlo.broadcast_in_dim %48, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %50 = stablehlo.multiply %46, %49 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %51 = stablehlo.reshape %arg14 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %53 = stablehlo.broadcast_in_dim %52, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %54 = stablehlo.add %50, %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %55 = stablehlo.reshape %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %56 = stablehlo.reshape %arg22 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %57 = stablehlo.reshape %56 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %58 = stablehlo.transpose %57, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %59 = stablehlo.dot_general %55, %58, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %60 = stablehlo.reshape %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %61 = stablehlo.reshape %arg21 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %63 = stablehlo.broadcast_in_dim %62, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %64 = stablehlo.add %60, %63 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %65 = stablehlo.multiply %64, %8 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %66 = stablehlo.reshape %65 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %67 = stablehlo.transpose %66, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %68 = stablehlo.reshape %67 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %69 = stablehlo.reshape %arg20 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %70 = stablehlo.reshape %69 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %71 = stablehlo.transpose %70, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %72 = stablehlo.dot_general %55, %71, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %73 = stablehlo.reshape %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %74 = stablehlo.reshape %arg19 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %76 = stablehlo.broadcast_in_dim %75, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %77 = stablehlo.add %73, %76 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 3, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x64x32xbf16>
    %80 = stablehlo.reshape %79 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x64x32xbf16>) -> tensor<96x64x32xbf16>
    %81 = stablehlo.dot_general %68, %80, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<96x32x64xbf16>, tensor<96x64x32xbf16>) -> tensor<96x32x32xbf16>
    %82 = stablehlo.reshape %81 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<96x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %83 = stablehlo.broadcast_in_dim %c_1, dims = [1] : (tensor<32xi64>) -> tensor<32x32xi64>
    %84 = stablehlo.broadcast_in_dim %c_1, dims = [0] : (tensor<32xi64>) -> tensor<32x32xi64>
    %85 = stablehlo.subtract %83, %84 : tensor<32x32xi64>
    %86 = stablehlo.compare  GE, %85, %7 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %87 = stablehlo.select %86, %6, %5 : tensor<32x32xi1>, tensor<32x32xbf16>
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %89 = stablehlo.convert %88 : (tensor<32x32xi1>) -> tensor<32x32xbf16>
    %90 = stablehlo.multiply %87, %89 : tensor<32x32xbf16>
    %91 = stablehlo.reshape %90 : (tensor<32x32xbf16>) -> tensor<1x32x32xbf16>
    %92 = stablehlo.broadcast_in_dim %91, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x32x32xbf16>) -> tensor<8x1x32x32xbf16>
    %93 = stablehlo.reshape %23 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x32xi64>) -> tensor<8x1x1x32xi64>
    %94 = stablehlo.convert %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x1x1x32xi64>) -> tensor<8x1x1x32xbf16>
    %95 = stablehlo.reshape %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x1x1x32xbf16>) -> tensor<8x1x32xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x1x32xbf16>) -> tensor<8x1x32x32xbf16>
    %97 = stablehlo.add %92, %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<8x1x32x32xbf16>
    %98 = stablehlo.compare  EQ, %97, %4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x1x32x32xbf16>, tensor<8x1x32x32xbf16>) -> tensor<8x1x32x32xi1>
    %99 = stablehlo.select %98, %3, %92 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<8x1x32x32xi1>, tensor<8x1x32x32xbf16>
    %100 = stablehlo.reshape %99 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x1x32x32xbf16>) -> tensor<8x32x32xbf16>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %102 = stablehlo.add %82, %101 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<8x12x32x32xbf16>
    %103 = stablehlo.convert %102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x12x32x32xbf16>) -> tensor<8x12x32x32xf32>
    %104 = stablehlo.reduce(%103 init: %cst) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %106 = stablehlo.subtract %103, %105 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<8x12x32x32xf32>
    %107 = stablehlo.exponential %106 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<8x12x32x32xf32>
    %108 = stablehlo.reduce(%107 init: %cst_2) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %110 = stablehlo.divide %107, %109 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<8x12x32x32xf32>
    %111 = stablehlo.convert %110 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x12x32x32xf32>) -> tensor<8x12x32x32xbf16>
    %112 = stablehlo.reshape %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x32x32xbf16>) -> tensor<96x32x32xbf16>
    %113 = stablehlo.reshape %arg13 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %114 = stablehlo.reshape %113 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %115 = stablehlo.transpose %114, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %116 = stablehlo.dot_general %55, %115, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %117 = stablehlo.reshape %116 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %118 = stablehlo.reshape %arg12 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %121 = stablehlo.add %117, %120 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %122 = stablehlo.reshape %121 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %123 = stablehlo.transpose %122, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %124 = stablehlo.reshape %123 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %125 = stablehlo.dot_general %112, %124, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<96x32x32xbf16>, tensor<96x32x64xbf16>) -> tensor<96x32x64xbf16>
    %126 = stablehlo.reshape %125 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<96x32x64xbf16>) -> tensor<8x12x32x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, xla_shape = "bf16[8,32,12,64]{3,1,2,0}"} : (tensor<8x12x32x64xbf16>) -> tensor<8x32x12x64xbf16>
    %128 = stablehlo.reshape %127 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x12x64xbf16>) -> tensor<256x768xbf16>
    %129 = stablehlo.reshape %arg11 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %131 = stablehlo.transpose %130, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %132 = stablehlo.dot_general %128, %131, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %133 = stablehlo.reshape %132 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %134 = stablehlo.reshape %arg10 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %135 = stablehlo.reshape %134 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %136 = stablehlo.broadcast_in_dim %135, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %137 = stablehlo.add %133, %136 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %138 = stablehlo.add %33, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %139 = stablehlo.reshape %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %140 = stablehlo.reduce(%139 init: %cst_14) applies stablehlo.add across dimensions = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %141 = stablehlo.multiply %140, %2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : tensor<256xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %143 = stablehlo.subtract %139, %142 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x768xbf16>
    %144 = stablehlo.multiply %143, %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x768xbf16>
    %145 = stablehlo.reduce(%144 init: %cst_14) applies stablehlo.add across dimensions = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %146 = stablehlo.multiply %145, %2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : tensor<256xbf16>
    %147 = stablehlo.reshape %146 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256xbf16>) -> tensor<256x1xbf16>
    %148 = stablehlo.add %147, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x1xbf16>
    %149 = stablehlo.rsqrt %148 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x1xbf16>
    %150 = stablehlo.reshape %149 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<256x1xbf16>) -> tensor<256xbf16>
    %151 = stablehlo.broadcast_in_dim %150, dims = [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %152 = stablehlo.multiply %143, %151 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x768xbf16>
    %153 = stablehlo.reshape %arg9 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %155 = stablehlo.broadcast_in_dim %154, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %156 = stablehlo.multiply %152, %155 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x768xbf16>
    %157 = stablehlo.reshape %arg8 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %158 = stablehlo.reshape %157 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %159 = stablehlo.broadcast_in_dim %158, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %160 = stablehlo.add %156, %159 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x768xbf16>
    %161 = stablehlo.reshape %arg7 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16>
    %162 = stablehlo.reshape %161 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16>
    %163 = stablehlo.transpose %162, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16>
    %164 = stablehlo.dot_general %160, %163, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x3072xbf16>) -> tensor<256x3072xbf16>
    %165 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %166 = stablehlo.reshape %165 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %167 = stablehlo.broadcast_in_dim %166, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<3072xbf16>) -> tensor<256x3072xbf16>
    %168 = stablehlo.add %164, %167 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x3072xbf16>
    %169 = stablehlo.maximum %168, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x3072xbf16>
    %170 = stablehlo.reshape %arg5 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16>
    %172 = stablehlo.transpose %171, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16>
    %173 = stablehlo.dot_general %169, %172, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x3072xbf16>, tensor<3072x768xbf16>) -> tensor<256x768xbf16>
    %174 = stablehlo.reshape %arg4 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %175 = stablehlo.reshape %174 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %176 = stablehlo.broadcast_in_dim %175, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %177 = stablehlo.add %173, %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x768xbf16>
    %178 = stablehlo.add %139, %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x768xbf16>
    %179 = stablehlo.reshape %178 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %180 = stablehlo.reduce(%179 init: %cst_14) applies stablehlo.add across dimensions = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %181 = stablehlo.multiply %180, %10 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xbf16>
    %182 = stablehlo.broadcast_in_dim %181, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %183 = stablehlo.subtract %179, %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %184 = stablehlo.multiply %183, %183 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %185 = stablehlo.reduce(%184 init: %cst_14) applies stablehlo.add across dimensions = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %186 = stablehlo.multiply %185, %10 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xbf16>
    %187 = stablehlo.reshape %186 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %188 = stablehlo.add %187, %9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x1xbf16>
    %189 = stablehlo.rsqrt %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x1xbf16>
    %190 = stablehlo.reshape %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %191 = stablehlo.broadcast_in_dim %190, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %192 = stablehlo.multiply %183, %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %193 = stablehlo.reshape %arg3 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %194 = stablehlo.reshape %193 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %195 = stablehlo.broadcast_in_dim %194, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %196 = stablehlo.multiply %192, %195 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %197 = stablehlo.reshape %arg2 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %198 = stablehlo.reshape %197 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %199 = stablehlo.broadcast_in_dim %198, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %200 = stablehlo.add %196, %199 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %201 = stablehlo.reshape %200 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %202 = stablehlo.reshape %arg1 : (tensor<2x768xbf16>) -> tensor<1x2x768xbf16>
    %203 = stablehlo.reshape %202 : (tensor<1x2x768xbf16>) -> tensor<2x768xbf16>
    %204 = stablehlo.transpose %203, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,2]{0,1}"} : (tensor<2x768xbf16>) -> tensor<768x2xbf16>
    %205 = stablehlo.dot_general %201, %204, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x2xbf16>) -> tensor<256x2xbf16>
    %206 = stablehlo.reshape %205 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x2xbf16>) -> tensor<8x32x2xbf16>
    %207 = stablehlo.broadcast_in_dim %c_3, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<32xi32>) -> tensor<8x32xi32>
    %208 = stablehlo.compare  NE, %16, %12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi64>, tensor<8x32xi64>) -> tensor<8x32xi1>
    %209 = stablehlo.convert %208 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi1>) -> tensor<8x32xi32>
    %210 = stablehlo.multiply %207, %209 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xi32>
    %211 = stablehlo.iota dim = 0 : tensor<32xi32>
    %212 = stablehlo.broadcast_in_dim %211, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<32xi32>) -> tensor<8x32xi32>
    %213:2 = stablehlo.reduce(%210 init: %c_4), (%212 init: %c_0) across dimensions = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}]>]>} : (tensor<8x32xi32>, tensor<8x32xi32>, tensor<i32>, tensor<i32>) -> (tensor<8xi32>, tensor<8xi32>)
     reducer(%arg23: tensor<i32>, %arg25: tensor<i32>) (%arg24: tensor<i32>, %arg26: tensor<i32>)  {
      %221 = stablehlo.compare  GE, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %222 = stablehlo.select %221, %arg23, %arg25 : tensor<i1>, tensor<i32>
      %223 = stablehlo.compare  EQ, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %224 = stablehlo.minimum %arg24, %arg26 : tensor<i32>
      %225 = stablehlo.select %221, %arg24, %arg26 : tensor<i1>, tensor<i32>
      %226 = stablehlo.select %223, %224, %225 : tensor<i1>, tensor<i32>
      stablehlo.return %222, %226 : tensor<i32>, tensor<i32>
    }
    %214 = stablehlo.convert %213#1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<8xi32>) -> tensor<8xi64>
    %215 = stablehlo.compare  LT, %214, %c_12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<8xi64>, tensor<8xi64>) -> tensor<8xi1>
    %216 = stablehlo.add %214, %c_11 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : tensor<8xi64>
    %217 = stablehlo.select %215, %216, %214 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : tensor<8xi1>, tensor<8xi64>
    %218 = stablehlo.reshape %217 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8xi64>) -> tensor<8x1xi64>
    %219 = stablehlo.concatenate %c_13, %218, dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x1xi64>, tensor<8x1xi64>) -> tensor<8x2xi64>
    %220 = "stablehlo.gather"(%206, %219) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1, 2>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x2xbf16>, tensor<8x2xi64>) -> tensor<8x2xbf16>
    return %220 : tensor<8x2xbf16>
  }
}


// -----// IR Dump Before InsertExplicitReshardsPass (insert-explicit-reshards) ('builtin.module' operation: @SyncTensorsGraph.630) //----- //
module @SyncTensorsGraph.630 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, %arg1: tensor<2x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___score_weight"}, %arg2: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_final_layer_norm_bias"}, %arg3: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_final_layer_norm_weight"}, %arg4: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc2_bias"}, %arg5: tensor<768x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc2_weight"}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc1_bias"}, %arg7: tensor<3072x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc1_weight"}, %arg8: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_bias"}, %arg9: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_weight"}, %arg10: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_bias"}, %arg11: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_weight"}, %arg12: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_bias"}, %arg13: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_weight"}, %arg14: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_bias"}, %arg15: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_weight"}, %arg16: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, %arg17: tensor<2050x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_embed_positions_weight"}, %arg18: tensor<50272x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_embed_tokens_weight"}, %arg19: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_bias"}, %arg20: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_weight"}, %arg21: tensor<768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_bias"}, %arg22: tensor<768x768xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_weight"}) -> (tensor<8x2xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %c = stablehlo.constant dense<0> : tensor<i64>
    %c_0 = stablehlo.constant dense<0> : tensor<i32>
    %c_1 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64>
    %cst = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %c_3 = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi32>
    %c_4 = stablehlo.constant dense<-2147483648> : tensor<i32>
    %c_5 = stablehlo.constant dense<2> : tensor<i64>
    %cst_6 = stablehlo.constant dense<1.250000e-01> : tensor<bf16>
    %c_7 = stablehlo.constant dense<1> : tensor<i64>
    %cst_8 = stablehlo.constant dense<-3.389530e+38> : tensor<bf16>
    %cst_9 = stablehlo.constant dense<1.304630e-03> : tensor<bf16>
    %cst_10 = stablehlo.constant dense<1.001360e-05> : tensor<bf16>
    %c_11 = stablehlo.constant {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} dense<32> : tensor<8xi64>
    %c_12 = stablehlo.constant {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} dense<0> : tensor<8xi64>
    %c_13 = stablehlo.constant {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} dense<[[0], [1], [2], [3], [4], [5], [6], [7]]> : tensor<8x1xi64>
    %cst_14 = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_14, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<bf16>) -> tensor<256x3072xbf16>
    %1 = stablehlo.broadcast_in_dim %cst_10, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<bf16>) -> tensor<256x1xbf16>
    %2 = stablehlo.broadcast_in_dim %cst_9, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<256xbf16>
    %3 = stablehlo.broadcast_in_dim %cst_8, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %4 = stablehlo.broadcast_in_dim %cst_14, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %5 = stablehlo.broadcast_in_dim %cst_14, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %6 = stablehlo.broadcast_in_dim %cst_8, dims = [] : (tensor<bf16>) -> tensor<32x32xbf16>
    %7 = stablehlo.broadcast_in_dim %c_7, dims = [] : (tensor<i64>) -> tensor<32x32xi64>
    %8 = stablehlo.broadcast_in_dim %cst_6, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<8x32x768xbf16>
    %9 = stablehlo.broadcast_in_dim %cst_10, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<8x32x1xbf16>
    %10 = stablehlo.broadcast_in_dim %cst_9, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<bf16>) -> tensor<8x32xbf16>
    %11 = stablehlo.broadcast_in_dim %c_5, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<i64>) -> tensor<8x32xi64>
    %12 = stablehlo.broadcast_in_dim %c_7, dims = [] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<i64>) -> tensor<8x32xi64>
    %13 = stablehlo.reshape %arg18 : (tensor<50272x768xbf16>) -> tensor<1x50272x768xbf16>
    %14 = stablehlo.reshape %13 : (tensor<1x50272x768xbf16>) -> tensor<50272x768xbf16>
    %15 = stablehlo.reshape %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %16 = stablehlo.reshape %15 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %17 = stablehlo.reshape %15 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<1x8x32xi64>) -> tensor<256xi64>
    %18 = stablehlo.convert %17 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<256xi64>) -> tensor<256xui32>
    %19 = "stablehlo.gather"(%14, %18) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<50272x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %20 = stablehlo.reshape %19 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %21 = stablehlo.reshape %arg17 : (tensor<2050x768xbf16>) -> tensor<1x2050x768xbf16>
    %22 = stablehlo.reshape %21 : (tensor<1x2050x768xbf16>) -> tensor<2050x768xbf16>
    %23 = stablehlo.reshape %arg16 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %24 = stablehlo.reshape %23 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %25 = "stablehlo.reduce_window"(%24, %c) <{padding = dense<[[0, 0], [31, 0]]> : tensor<2x2xi64>, window_dimensions = array<i64: 1, 32>}> ({
    ^bb0(%arg23: tensor<i64>, %arg24: tensor<i64>):
      %221 = stablehlo.add %arg23, %arg24 : tensor<i64>
      stablehlo.return %221 : tensor<i64>
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi64>, tensor<i64>) -> tensor<8x32xi64>
    %26 = stablehlo.multiply %25, %24 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xi64>
    %27 = stablehlo.subtract %26, %12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xi64>
    %28 = stablehlo.add %27, %11 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xi64>
    %29 = stablehlo.reshape %28 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<8x32xi64>) -> tensor<256xi64>
    %30 = stablehlo.convert %29 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<256xi64>) -> tensor<256xui32>
    %31 = "stablehlo.gather"(%22, %30) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<2050x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %32 = stablehlo.reshape %31 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %33 = stablehlo.add %20, %32 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %34 = stablehlo.reduce(%33 init: %cst_14) applies stablehlo.add across dimensions = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %35 = stablehlo.multiply %34, %10 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xbf16>
    %36 = stablehlo.broadcast_in_dim %35, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %37 = stablehlo.subtract %33, %36 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %38 = stablehlo.multiply %37, %37 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %39 = stablehlo.reduce(%38 init: %cst_14) applies stablehlo.add across dimensions = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %40 = stablehlo.multiply %39, %10 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xbf16>
    %41 = stablehlo.reshape %40 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %42 = stablehlo.add %41, %9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x1xbf16>
    %43 = stablehlo.rsqrt %42 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x1xbf16>
    %44 = stablehlo.reshape %43 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %45 = stablehlo.broadcast_in_dim %44, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %46 = stablehlo.multiply %37, %45 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %47 = stablehlo.reshape %arg15 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %48 = stablehlo.reshape %47 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %49 = stablehlo.broadcast_in_dim %48, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %50 = stablehlo.multiply %46, %49 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %51 = stablehlo.reshape %arg14 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %52 = stablehlo.reshape %51 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %53 = stablehlo.broadcast_in_dim %52, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %54 = stablehlo.add %50, %53 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %55 = stablehlo.reshape %54 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %56 = stablehlo.reshape %arg22 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %57 = stablehlo.reshape %56 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %58 = stablehlo.transpose %57, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %59 = stablehlo.dot_general %55, %58, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %60 = stablehlo.reshape %59 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %61 = stablehlo.reshape %arg21 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %62 = stablehlo.reshape %61 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %63 = stablehlo.broadcast_in_dim %62, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %64 = stablehlo.add %60, %63 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %65 = stablehlo.multiply %64, %8 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %66 = stablehlo.reshape %65 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %67 = stablehlo.transpose %66, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %68 = stablehlo.reshape %67 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %69 = stablehlo.reshape %arg20 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %70 = stablehlo.reshape %69 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %71 = stablehlo.transpose %70, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %72 = stablehlo.dot_general %55, %71, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %73 = stablehlo.reshape %72 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %74 = stablehlo.reshape %arg19 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %75 = stablehlo.reshape %74 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %76 = stablehlo.broadcast_in_dim %75, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %77 = stablehlo.add %73, %76 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %78 = stablehlo.reshape %77 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %79 = stablehlo.transpose %78, dims = [0, 2, 3, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x64x32xbf16>
    %80 = stablehlo.reshape %79 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x64x32xbf16>) -> tensor<96x64x32xbf16>
    %81 = stablehlo.dot_general %68, %80, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<96x32x64xbf16>, tensor<96x64x32xbf16>) -> tensor<96x32x32xbf16>
    %82 = stablehlo.reshape %81 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<96x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %83 = stablehlo.broadcast_in_dim %c_1, dims = [1] : (tensor<32xi64>) -> tensor<32x32xi64>
    %84 = stablehlo.broadcast_in_dim %c_1, dims = [0] : (tensor<32xi64>) -> tensor<32x32xi64>
    %85 = stablehlo.subtract %83, %84 : tensor<32x32xi64>
    %86 = stablehlo.compare  GE, %85, %7 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %87 = stablehlo.select %86, %6, %5 : tensor<32x32xi1>, tensor<32x32xbf16>
    %88 = stablehlo.compare  GT, %83, %84 : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %89 = stablehlo.convert %88 : (tensor<32x32xi1>) -> tensor<32x32xbf16>
    %90 = stablehlo.multiply %87, %89 : tensor<32x32xbf16>
    %91 = stablehlo.reshape %90 : (tensor<32x32xbf16>) -> tensor<1x32x32xbf16>
    %92 = stablehlo.broadcast_in_dim %91, dims = [1, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x32x32xbf16>) -> tensor<8x1x32x32xbf16>
    %93 = stablehlo.reshape %23 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x32xi64>) -> tensor<8x1x1x32xi64>
    %94 = stablehlo.convert %93 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x1x1x32xi64>) -> tensor<8x1x1x32xbf16>
    %95 = stablehlo.reshape %94 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x1x1x32xbf16>) -> tensor<8x1x32xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 1, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x1x32xbf16>) -> tensor<8x1x32x32xbf16>
    %97 = stablehlo.add %92, %96 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<8x1x32x32xbf16>
    %98 = stablehlo.compare  EQ, %97, %4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x1x32x32xbf16>, tensor<8x1x32x32xbf16>) -> tensor<8x1x32x32xi1>
    %99 = stablehlo.select %98, %3, %92 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<8x1x32x32xi1>, tensor<8x1x32x32xbf16>
    %100 = stablehlo.reshape %99 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x1x32x32xbf16>) -> tensor<8x32x32xbf16>
    %101 = stablehlo.broadcast_in_dim %100, dims = [0, 2, 3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %102 = stablehlo.add %82, %101 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<8x12x32x32xbf16>
    %103 = stablehlo.convert %102 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x12x32x32xbf16>) -> tensor<8x12x32x32xf32>
    %104 = stablehlo.reduce(%103 init: %cst) applies stablehlo.maximum across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %105 = stablehlo.broadcast_in_dim %104, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %106 = stablehlo.subtract %103, %105 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<8x12x32x32xf32>
    %107 = stablehlo.exponential %106 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<8x12x32x32xf32>
    %108 = stablehlo.reduce(%107 init: %cst_2) applies stablehlo.add across dimensions = [3] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %110 = stablehlo.divide %107, %109 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : tensor<8x12x32x32xf32>
    %111 = stablehlo.convert %110 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x12x32x32xf32>) -> tensor<8x12x32x32xbf16>
    %112 = stablehlo.reshape %111 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x32x32xbf16>) -> tensor<96x32x32xbf16>
    %113 = stablehlo.reshape %arg13 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %114 = stablehlo.reshape %113 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %115 = stablehlo.transpose %114, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %116 = stablehlo.dot_general %55, %115, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %117 = stablehlo.reshape %116 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %118 = stablehlo.reshape %arg12 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %119 = stablehlo.reshape %118 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %120 = stablehlo.broadcast_in_dim %119, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %121 = stablehlo.add %117, %120 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %122 = stablehlo.reshape %121 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %123 = stablehlo.transpose %122, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %124 = stablehlo.reshape %123 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %125 = stablehlo.dot_general %112, %124, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<96x32x32xbf16>, tensor<96x32x64xbf16>) -> tensor<96x32x64xbf16>
    %126 = stablehlo.reshape %125 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<96x32x64xbf16>) -> tensor<8x12x32x64xbf16>
    %127 = stablehlo.transpose %126, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, xla_shape = "bf16[8,32,12,64]{3,1,2,0}"} : (tensor<8x12x32x64xbf16>) -> tensor<8x32x12x64xbf16>
    %128 = stablehlo.reshape %127 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x12x64xbf16>) -> tensor<256x768xbf16>
    %129 = stablehlo.reshape %arg11 : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %130 = stablehlo.reshape %129 : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %131 = stablehlo.transpose %130, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %132 = stablehlo.dot_general %128, %131, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %133 = stablehlo.reshape %132 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %134 = stablehlo.reshape %arg10 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %135 = stablehlo.reshape %134 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %136 = stablehlo.broadcast_in_dim %135, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %137 = stablehlo.add %133, %136 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %138 = stablehlo.add %33, %137 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %139 = stablehlo.reshape %138 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %140 = stablehlo.reduce(%139 init: %cst_14) applies stablehlo.add across dimensions = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %141 = stablehlo.multiply %140, %2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : tensor<256xbf16>
    %142 = stablehlo.broadcast_in_dim %141, dims = [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %143 = stablehlo.subtract %139, %142 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x768xbf16>
    %144 = stablehlo.multiply %143, %143 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x768xbf16>
    %145 = stablehlo.reduce(%144 init: %cst_14) applies stablehlo.add across dimensions = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %146 = stablehlo.multiply %145, %2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : tensor<256xbf16>
    %147 = stablehlo.reshape %146 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256xbf16>) -> tensor<256x1xbf16>
    %148 = stablehlo.add %147, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x1xbf16>
    %149 = stablehlo.rsqrt %148 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x1xbf16>
    %150 = stablehlo.reshape %149 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<256x1xbf16>) -> tensor<256xbf16>
    %151 = stablehlo.broadcast_in_dim %150, dims = [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %152 = stablehlo.multiply %143, %151 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x768xbf16>
    %153 = stablehlo.reshape %arg9 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %154 = stablehlo.reshape %153 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %155 = stablehlo.broadcast_in_dim %154, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %156 = stablehlo.multiply %152, %155 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x768xbf16>
    %157 = stablehlo.reshape %arg8 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %158 = stablehlo.reshape %157 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %159 = stablehlo.broadcast_in_dim %158, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %160 = stablehlo.add %156, %159 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x768xbf16>
    %161 = stablehlo.reshape %arg7 : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16>
    %162 = stablehlo.reshape %161 : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16>
    %163 = stablehlo.transpose %162, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16>
    %164 = stablehlo.dot_general %160, %163, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x3072xbf16>) -> tensor<256x3072xbf16>
    %165 = stablehlo.reshape %arg6 : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %166 = stablehlo.reshape %165 : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %167 = stablehlo.broadcast_in_dim %166, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<3072xbf16>) -> tensor<256x3072xbf16>
    %168 = stablehlo.add %164, %167 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x3072xbf16>
    %169 = stablehlo.maximum %168, %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x3072xbf16>
    %170 = stablehlo.reshape %arg5 : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16>
    %171 = stablehlo.reshape %170 : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16>
    %172 = stablehlo.transpose %171, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16>
    %173 = stablehlo.dot_general %169, %172, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x3072xbf16>, tensor<3072x768xbf16>) -> tensor<256x768xbf16>
    %174 = stablehlo.reshape %arg4 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %175 = stablehlo.reshape %174 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %176 = stablehlo.broadcast_in_dim %175, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %177 = stablehlo.add %173, %176 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x768xbf16>
    %178 = stablehlo.add %139, %177 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<256x768xbf16>
    %179 = stablehlo.reshape %178 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %180 = stablehlo.reduce(%179 init: %cst_14) applies stablehlo.add across dimensions = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %181 = stablehlo.multiply %180, %10 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xbf16>
    %182 = stablehlo.broadcast_in_dim %181, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %183 = stablehlo.subtract %179, %182 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %184 = stablehlo.multiply %183, %183 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %185 = stablehlo.reduce(%184 init: %cst_14) applies stablehlo.add across dimensions = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %186 = stablehlo.multiply %185, %10 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xbf16>
    %187 = stablehlo.reshape %186 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %188 = stablehlo.add %187, %9 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x1xbf16>
    %189 = stablehlo.rsqrt %188 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x1xbf16>
    %190 = stablehlo.reshape %189 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %191 = stablehlo.broadcast_in_dim %190, dims = [0, 1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %192 = stablehlo.multiply %183, %191 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %193 = stablehlo.reshape %arg3 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %194 = stablehlo.reshape %193 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %195 = stablehlo.broadcast_in_dim %194, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %196 = stablehlo.multiply %192, %195 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %197 = stablehlo.reshape %arg2 : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %198 = stablehlo.reshape %197 : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %199 = stablehlo.broadcast_in_dim %198, dims = [2] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %200 = stablehlo.add %196, %199 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : tensor<8x32x768xbf16>
    %201 = stablehlo.reshape %200 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %202 = stablehlo.reshape %arg1 : (tensor<2x768xbf16>) -> tensor<1x2x768xbf16>
    %203 = stablehlo.reshape %202 : (tensor<1x2x768xbf16>) -> tensor<2x768xbf16>
    %204 = stablehlo.transpose %203, dims = [1, 0] {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,2]{0,1}"} : (tensor<2x768xbf16>) -> tensor<768x2xbf16>
    %205 = stablehlo.dot_general %201, %204, contracting_dims = [1] x [0] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x2xbf16>) -> tensor<256x2xbf16>
    %206 = stablehlo.reshape %205 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x2xbf16>) -> tensor<8x32x2xbf16>
    %207 = stablehlo.broadcast_in_dim %c_3, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<32xi32>) -> tensor<8x32xi32>
    %208 = stablehlo.compare  NE, %16, %12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi64>, tensor<8x32xi64>) -> tensor<8x32xi1>
    %209 = stablehlo.convert %208 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi1>) -> tensor<8x32xi32>
    %210 = stablehlo.multiply %207, %209 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : tensor<8x32xi32>
    %211 = stablehlo.iota dim = 0 : tensor<32xi32>
    %212 = stablehlo.broadcast_in_dim %211, dims = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<32xi32>) -> tensor<8x32xi32>
    %213:2 = stablehlo.reduce(%210 init: %c_4), (%212 init: %c_0) across dimensions = [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}]>]>} : (tensor<8x32xi32>, tensor<8x32xi32>, tensor<i32>, tensor<i32>) -> (tensor<8xi32>, tensor<8xi32>)
     reducer(%arg23: tensor<i32>, %arg25: tensor<i32>) (%arg24: tensor<i32>, %arg26: tensor<i32>)  {
      %221 = stablehlo.compare  GE, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %222 = stablehlo.select %221, %arg23, %arg25 : tensor<i1>, tensor<i32>
      %223 = stablehlo.compare  EQ, %arg23, %arg25 : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %224 = stablehlo.minimum %arg24, %arg26 : tensor<i32>
      %225 = stablehlo.select %221, %arg24, %arg26 : tensor<i1>, tensor<i32>
      %226 = stablehlo.select %223, %224, %225 : tensor<i1>, tensor<i32>
      stablehlo.return %222, %226 : tensor<i32>, tensor<i32>
    }
    %214 = stablehlo.convert %213#1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<8xi32>) -> tensor<8xi64>
    %215 = stablehlo.compare  LT, %214, %c_12 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<8xi64>, tensor<8xi64>) -> tensor<8xi1>
    %216 = stablehlo.add %214, %c_11 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : tensor<8xi64>
    %217 = stablehlo.select %215, %216, %214 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : tensor<8xi1>, tensor<8xi64>
    %218 = stablehlo.reshape %217 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8xi64>) -> tensor<8x1xi64>
    %219 = stablehlo.concatenate %c_13, %218, dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x1xi64>, tensor<8x1xi64>) -> tensor<8x2xi64>
    %220 = "stablehlo.gather"(%206, %219) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1, 2>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x2xbf16>, tensor<8x2xi64>) -> tensor<8x2xbf16>
    return %220 : tensor<8x2xbf16>
  }
}


loc("gather.628"): error: 'sdy.all_reduce' op reduction axis "_axis_0" overlaps with operand dimension sharding or replicated axes
// -----// IR Dump After InsertExplicitReshardsPass Failed (insert-explicit-reshards) ('builtin.module' operation: @SyncTensorsGraph.630) //----- //
"builtin.module"() <{sym_name = "SyncTensorsGraph.630"}> ({
  "sdy.mesh"() <{mesh = #sdy.mesh<["_axis_0_updated"=1, "_axis_0"=8]>, sym_name = "mesh"}> : () -> ()
  "func.func"() <{arg_attrs = [{sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_0"}, {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___score_weight"}, {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_final_layer_norm_bias"}, {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_final_layer_norm_weight"}, {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc2_bias"}, {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc2_weight"}, {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc1_bias"}, {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_fc1_weight"}, {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_bias"}, {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_final_layer_norm_weight"}, {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_bias"}, {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_out_proj_weight"}, {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_bias"}, {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_v_proj_weight"}, {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_bias"}, {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_layer_norm_weight"}, {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "args_1"}, {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_embed_positions_weight"}, {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_embed_tokens_weight"}, {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_bias"}, {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_k_proj_weight"}, {sdy.sharding = #sdy.sharding<@mesh, [{}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_bias"}, {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>, ttcore.argument_type = #ttcore.argument_type<parameter>, ttcore.shard_status = #ttcore.shard_status<presharded>, ttir.name = "l__self___model_decoder_layers_0_self_attn_q_proj_weight"}], function_type = (tensor<8x32xi64>, tensor<2x768xbf16>, tensor<768xbf16>, tensor<768xbf16>, tensor<768xbf16>, tensor<768x3072xbf16>, tensor<3072xbf16>, tensor<3072x768xbf16>, tensor<768xbf16>, tensor<768xbf16>, tensor<768xbf16>, tensor<768x768xbf16>, tensor<768xbf16>, tensor<768x768xbf16>, tensor<768xbf16>, tensor<768xbf16>, tensor<8x32xi64>, tensor<2050x768xbf16>, tensor<50272x768xbf16>, tensor<768xbf16>, tensor<768x768xbf16>, tensor<768xbf16>, tensor<768x768xbf16>) -> tensor<8x2xbf16>, res_attrs = [{sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, ttcore.shard_status = #ttcore.shard_status<unsharded>}], sym_name = "main"}> ({
  ^bb0(%arg0: tensor<8x32xi64>, %arg1: tensor<2x768xbf16>, %arg2: tensor<768xbf16>, %arg3: tensor<768xbf16>, %arg4: tensor<768xbf16>, %arg5: tensor<768x3072xbf16>, %arg6: tensor<3072xbf16>, %arg7: tensor<3072x768xbf16>, %arg8: tensor<768xbf16>, %arg9: tensor<768xbf16>, %arg10: tensor<768xbf16>, %arg11: tensor<768x768xbf16>, %arg12: tensor<768xbf16>, %arg13: tensor<768x768xbf16>, %arg14: tensor<768xbf16>, %arg15: tensor<768xbf16>, %arg16: tensor<8x32xi64>, %arg17: tensor<2050x768xbf16>, %arg18: tensor<50272x768xbf16>, %arg19: tensor<768xbf16>, %arg20: tensor<768x768xbf16>, %arg21: tensor<768xbf16>, %arg22: tensor<768x768xbf16>):
    %0 = "stablehlo.constant"() <{value = dense<0> : tensor<i64>}> : () -> tensor<i64>
    %1 = "stablehlo.constant"() <{value = dense<0> : tensor<i32>}> : () -> tensor<i32>
    %2 = "stablehlo.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi64>}> : () -> tensor<32xi64>
    %3 = "stablehlo.constant"() <{value = dense<0xFF800000> : tensor<f32>}> : () -> tensor<f32>
    %4 = "stablehlo.constant"() <{value = dense<0.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %5 = "stablehlo.constant"() <{value = dense<[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]> : tensor<32xi32>}> : () -> tensor<32xi32>
    %6 = "stablehlo.constant"() <{value = dense<-2147483648> : tensor<i32>}> : () -> tensor<i32>
    %7 = "stablehlo.constant"() <{value = dense<2> : tensor<i64>}> : () -> tensor<i64>
    %8 = "stablehlo.constant"() <{value = dense<1.250000e-01> : tensor<bf16>}> : () -> tensor<bf16>
    %9 = "stablehlo.constant"() <{value = dense<1> : tensor<i64>}> : () -> tensor<i64>
    %10 = "stablehlo.constant"() <{value = dense<-3.389530e+38> : tensor<bf16>}> : () -> tensor<bf16>
    %11 = "stablehlo.constant"() <{value = dense<1.304630e-03> : tensor<bf16>}> : () -> tensor<bf16>
    %12 = "stablehlo.constant"() <{value = dense<1.001360e-05> : tensor<bf16>}> : () -> tensor<bf16>
    %13 = "stablehlo.constant"() <{value = dense<32> : tensor<8xi64>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : () -> tensor<8xi64>
    %14 = "stablehlo.constant"() <{value = dense<0> : tensor<8xi64>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : () -> tensor<8xi64>
    %15 = "stablehlo.constant"() <{value = dense<[[0], [1], [2], [3], [4], [5], [6], [7]]> : tensor<8x1xi64>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : () -> tensor<8x1xi64>
    %16 = "stablehlo.constant"() <{value = dense<0.000000e+00> : tensor<bf16>}> : () -> tensor<bf16>
    %17 = "stablehlo.broadcast_in_dim"(%16) <{broadcast_dimensions = array<i64>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<bf16>) -> tensor<256x3072xbf16>
    %18 = "stablehlo.broadcast_in_dim"(%12) <{broadcast_dimensions = array<i64>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<bf16>) -> tensor<256x1xbf16>
    %19 = "stablehlo.broadcast_in_dim"(%11) <{broadcast_dimensions = array<i64>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<bf16>) -> tensor<256xbf16>
    %20 = "stablehlo.broadcast_in_dim"(%10) <{broadcast_dimensions = array<i64>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %21 = "stablehlo.broadcast_in_dim"(%16) <{broadcast_dimensions = array<i64>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<8x1x32x32xbf16>
    %22 = "stablehlo.broadcast_in_dim"(%16) <{broadcast_dimensions = array<i64>}> : (tensor<bf16>) -> tensor<32x32xbf16>
    %23 = "stablehlo.broadcast_in_dim"(%10) <{broadcast_dimensions = array<i64>}> : (tensor<bf16>) -> tensor<32x32xbf16>
    %24 = "stablehlo.broadcast_in_dim"(%9) <{broadcast_dimensions = array<i64>}> : (tensor<i64>) -> tensor<32x32xi64>
    %25 = "stablehlo.broadcast_in_dim"(%8) <{broadcast_dimensions = array<i64>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<8x32x768xbf16>
    %26 = "stablehlo.broadcast_in_dim"(%12) <{broadcast_dimensions = array<i64>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<bf16>) -> tensor<8x32x1xbf16>
    %27 = "stablehlo.broadcast_in_dim"(%11) <{broadcast_dimensions = array<i64>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<bf16>) -> tensor<8x32xbf16>
    %28 = "stablehlo.broadcast_in_dim"(%7) <{broadcast_dimensions = array<i64>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<i64>) -> tensor<8x32xi64>
    %29 = "stablehlo.broadcast_in_dim"(%9) <{broadcast_dimensions = array<i64>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<i64>) -> tensor<8x32xi64>
    %30 = "stablehlo.reshape"(%arg18) : (tensor<50272x768xbf16>) -> tensor<1x50272x768xbf16>
    %31 = "stablehlo.reshape"(%30) : (tensor<1x50272x768xbf16>) -> tensor<50272x768xbf16>
    %32 = "stablehlo.reshape"(%arg0) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %33 = "stablehlo.reshape"(%32) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %34 = "stablehlo.reshape"(%32) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<1x8x32xi64>) -> tensor<256xi64>
    %35 = "stablehlo.convert"(%34) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<256xi64>) -> tensor<256xui32>
    %36 = "stablehlo.gather"(%31, %35) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<50272x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %37 = "stablehlo.reshape"(%36) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %38 = "stablehlo.reshape"(%arg17) : (tensor<2050x768xbf16>) -> tensor<1x2050x768xbf16>
    %39 = "stablehlo.reshape"(%38) : (tensor<1x2050x768xbf16>) -> tensor<2050x768xbf16>
    %40 = "stablehlo.reshape"(%arg16) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{?}, {"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi64>) -> tensor<1x8x32xi64>
    %41 = "stablehlo.reshape"(%40) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<1x8x32xi64>) -> tensor<8x32xi64>
    %42 = "stablehlo.reduce_window"(%41, %0) <{padding = dense<[[0, 0], [31, 0]]> : tensor<2x2xi64>, window_dimensions = array<i64: 1, 32>}> ({
    ^bb0(%arg43: tensor<i64>, %arg44: tensor<i64>):
      %253 = "stablehlo.add"(%arg43, %arg44) : (tensor<i64>, tensor<i64>) -> tensor<i64>
      "stablehlo.return"(%253) : (tensor<i64>) -> ()
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi64>, tensor<i64>) -> tensor<8x32xi64>
    %43 = "stablehlo.multiply"(%42, %41) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi64>, tensor<8x32xi64>) -> tensor<8x32xi64>
    %44 = "stablehlo.subtract"(%43, %29) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi64>, tensor<8x32xi64>) -> tensor<8x32xi64>
    %45 = "stablehlo.add"(%44, %28) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi64>, tensor<8x32xi64>) -> tensor<8x32xi64>
    %46 = "stablehlo.reshape"(%45) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<8x32xi64>) -> tensor<256xi64>
    %47 = "stablehlo.convert"(%46) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<256xi64>) -> tensor<256xui32>
    %48 = "stablehlo.gather"(%39, %47) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 768>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<2050x768xbf16>, tensor<256xui32>) -> tensor<256x768xbf16>
    %49 = "stablehlo.reshape"(%48) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %50 = "stablehlo.add"(%37, %49) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<8x32x768xbf16>) -> tensor<8x32x768xbf16>
    %51 = "stablehlo.reduce"(%50, %16) <{dimensions = array<i64: 2>}> ({
    ^bb0(%arg41: tensor<bf16>, %arg42: tensor<bf16>):
      %252 = "stablehlo.add"(%arg41, %arg42) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
      "stablehlo.return"(%252) : (tensor<bf16>) -> ()
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %52 = "stablehlo.multiply"(%51, %27) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32xbf16>, tensor<8x32xbf16>) -> tensor<8x32xbf16>
    %53 = "stablehlo.broadcast_in_dim"(%52) <{broadcast_dimensions = array<i64: 0, 1>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %54 = "stablehlo.subtract"(%50, %53) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<8x32x768xbf16>) -> tensor<8x32x768xbf16>
    %55 = "stablehlo.multiply"(%54, %54) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<8x32x768xbf16>) -> tensor<8x32x768xbf16>
    %56 = "stablehlo.reduce"(%55, %16) <{dimensions = array<i64: 2>}> ({
    ^bb0(%arg39: tensor<bf16>, %arg40: tensor<bf16>):
      %251 = "stablehlo.add"(%arg39, %arg40) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
      "stablehlo.return"(%251) : (tensor<bf16>) -> ()
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %57 = "stablehlo.multiply"(%56, %27) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32xbf16>, tensor<8x32xbf16>) -> tensor<8x32xbf16>
    %58 = "stablehlo.reshape"(%57) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %59 = "stablehlo.add"(%58, %26) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32x1xbf16>, tensor<8x32x1xbf16>) -> tensor<8x32x1xbf16>
    %60 = "stablehlo.rsqrt"(%59) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32x1xbf16>) -> tensor<8x32x1xbf16>
    %61 = "stablehlo.reshape"(%60) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %62 = "stablehlo.broadcast_in_dim"(%61) <{broadcast_dimensions = array<i64: 0, 1>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %63 = "stablehlo.multiply"(%54, %62) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<8x32x768xbf16>) -> tensor<8x32x768xbf16>
    %64 = "stablehlo.reshape"(%arg15) : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %65 = "stablehlo.reshape"(%64) : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %66 = "stablehlo.broadcast_in_dim"(%65) <{broadcast_dimensions = array<i64: 2>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %67 = "stablehlo.multiply"(%63, %66) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<8x32x768xbf16>) -> tensor<8x32x768xbf16>
    %68 = "stablehlo.reshape"(%arg14) : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %69 = "stablehlo.reshape"(%68) : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %70 = "stablehlo.broadcast_in_dim"(%69) <{broadcast_dimensions = array<i64: 2>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %71 = "stablehlo.add"(%67, %70) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<8x32x768xbf16>) -> tensor<8x32x768xbf16>
    %72 = "stablehlo.reshape"(%71) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %73 = "stablehlo.reshape"(%arg22) : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %74 = "stablehlo.reshape"(%73) : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %75 = "stablehlo.transpose"(%74) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %76 = "stablehlo.dot_general"(%72, %75) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %77 = "stablehlo.reshape"(%76) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %78 = "stablehlo.reshape"(%arg21) : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %79 = "stablehlo.reshape"(%78) : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %80 = "stablehlo.broadcast_in_dim"(%79) <{broadcast_dimensions = array<i64: 2>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %81 = "stablehlo.add"(%77, %80) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<8x32x768xbf16>) -> tensor<8x32x768xbf16>
    %82 = "stablehlo.multiply"(%81, %25) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<8x32x768xbf16>) -> tensor<8x32x768xbf16>
    %83 = "stablehlo.reshape"(%82) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %84 = "stablehlo.transpose"(%83) <{permutation = array<i64: 0, 2, 1, 3>}> {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %85 = "stablehlo.reshape"(%84) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %86 = "stablehlo.reshape"(%arg20) : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %87 = "stablehlo.reshape"(%86) : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %88 = "stablehlo.transpose"(%87) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %89 = "stablehlo.dot_general"(%72, %88) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %90 = "stablehlo.reshape"(%89) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %91 = "stablehlo.reshape"(%arg19) : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %92 = "stablehlo.reshape"(%91) : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %93 = "stablehlo.broadcast_in_dim"(%92) <{broadcast_dimensions = array<i64: 2>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %94 = "stablehlo.add"(%90, %93) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<8x32x768xbf16>) -> tensor<8x32x768xbf16>
    %95 = "stablehlo.reshape"(%94) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %96 = "stablehlo.transpose"(%95) <{permutation = array<i64: 0, 2, 3, 1>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x64x32xbf16>
    %97 = "stablehlo.reshape"(%96) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x64x32xbf16>) -> tensor<96x64x32xbf16>
    %98 = "stablehlo.dot_general"(%85, %97) <{dot_dimension_numbers = #stablehlo.dot<lhs_batching_dimensions = [0], rhs_batching_dimensions = [0], lhs_contracting_dimensions = [2], rhs_contracting_dimensions = [1]>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<96x32x64xbf16>, tensor<96x64x32xbf16>) -> tensor<96x32x32xbf16>
    %99 = "stablehlo.reshape"(%98) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<96x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %100 = "stablehlo.broadcast_in_dim"(%2) <{broadcast_dimensions = array<i64: 1>}> : (tensor<32xi64>) -> tensor<32x32xi64>
    %101 = "stablehlo.broadcast_in_dim"(%2) <{broadcast_dimensions = array<i64: 0>}> : (tensor<32xi64>) -> tensor<32x32xi64>
    %102 = "stablehlo.subtract"(%100, %101) : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi64>
    %103 = "stablehlo.compare"(%102, %24) <{comparison_direction = #stablehlo<comparison_direction GE>}> : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %104 = "stablehlo.select"(%103, %23, %22) : (tensor<32x32xi1>, tensor<32x32xbf16>, tensor<32x32xbf16>) -> tensor<32x32xbf16>
    %105 = "stablehlo.compare"(%100, %101) <{comparison_direction = #stablehlo<comparison_direction GT>}> : (tensor<32x32xi64>, tensor<32x32xi64>) -> tensor<32x32xi1>
    %106 = "stablehlo.convert"(%105) : (tensor<32x32xi1>) -> tensor<32x32xbf16>
    %107 = "stablehlo.multiply"(%104, %106) : (tensor<32x32xbf16>, tensor<32x32xbf16>) -> tensor<32x32xbf16>
    %108 = "stablehlo.reshape"(%107) : (tensor<32x32xbf16>) -> tensor<1x32x32xbf16>
    %109 = "stablehlo.broadcast_in_dim"(%108) <{broadcast_dimensions = array<i64: 1, 2, 3>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x32x32xbf16>) -> tensor<8x1x32x32xbf16>
    %110 = "stablehlo.reshape"(%40) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<1x8x32xi64>) -> tensor<8x1x1x32xi64>
    %111 = "stablehlo.convert"(%110) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x1x1x32xi64>) -> tensor<8x1x1x32xbf16>
    %112 = "stablehlo.reshape"(%111) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x1x1x32xbf16>) -> tensor<8x1x32xbf16>
    %113 = "stablehlo.broadcast_in_dim"(%112) <{broadcast_dimensions = array<i64: 0, 1, 3>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x1x32xbf16>) -> tensor<8x1x32x32xbf16>
    %114 = "stablehlo.add"(%109, %113) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x1x32x32xbf16>, tensor<8x1x32x32xbf16>) -> tensor<8x1x32x32xbf16>
    %115 = "stablehlo.compare"(%114, %21) <{comparison_direction = #stablehlo<comparison_direction EQ>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x1x32x32xbf16>, tensor<8x1x32x32xbf16>) -> tensor<8x1x32x32xi1>
    %116 = "stablehlo.select"(%115, %20, %109) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x1x32x32xi1>, tensor<8x1x32x32xbf16>, tensor<8x1x32x32xbf16>) -> tensor<8x1x32x32xbf16>
    %117 = "stablehlo.reshape"(%116) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x1x32x32xbf16>) -> tensor<8x32x32xbf16>
    %118 = "stablehlo.broadcast_in_dim"(%117) <{broadcast_dimensions = array<i64: 0, 2, 3>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %119 = "stablehlo.add"(%99, %118) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x12x32x32xbf16>, tensor<8x12x32x32xbf16>) -> tensor<8x12x32x32xbf16>
    %120 = "stablehlo.convert"(%119) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x12x32x32xbf16>) -> tensor<8x12x32x32xf32>
    %121 = "stablehlo.reduce"(%120, %3) <{dimensions = array<i64: 3>}> ({
    ^bb0(%arg37: tensor<f32>, %arg38: tensor<f32>):
      %250 = "stablehlo.maximum"(%arg37, %arg38) : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "stablehlo.return"(%250) : (tensor<f32>) -> ()
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %122 = "stablehlo.broadcast_in_dim"(%121) <{broadcast_dimensions = array<i64: 0, 1, 2>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %123 = "stablehlo.subtract"(%120, %122) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x12x32x32xf32>, tensor<8x12x32x32xf32>) -> tensor<8x12x32x32xf32>
    %124 = "stablehlo.exponential"(%123) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x12x32x32xf32>) -> tensor<8x12x32x32xf32>
    %125 = "stablehlo.reduce"(%124, %4) <{dimensions = array<i64: 3>}> ({
    ^bb0(%arg35: tensor<f32>, %arg36: tensor<f32>):
      %249 = "stablehlo.add"(%arg35, %arg36) : (tensor<f32>, tensor<f32>) -> tensor<f32>
      "stablehlo.return"(%249) : (tensor<f32>) -> ()
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x32x32xf32>, tensor<f32>) -> tensor<8x12x32xf32>
    %126 = "stablehlo.broadcast_in_dim"(%125) <{broadcast_dimensions = array<i64: 0, 1, 2>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x12x32xf32>) -> tensor<8x12x32x32xf32>
    %127 = "stablehlo.divide"(%124, %126) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x12x32x32xf32>, tensor<8x12x32x32xf32>) -> tensor<8x12x32x32xf32>
    %128 = "stablehlo.convert"(%127) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x12x32x32xf32>) -> tensor<8x12x32x32xbf16>
    %129 = "stablehlo.reshape"(%128) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x32x32xbf16>) -> tensor<96x32x32xbf16>
    %130 = "stablehlo.reshape"(%arg13) : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %131 = "stablehlo.reshape"(%130) : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %132 = "stablehlo.transpose"(%131) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %133 = "stablehlo.dot_general"(%72, %132) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %134 = "stablehlo.reshape"(%133) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %135 = "stablehlo.reshape"(%arg12) : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %136 = "stablehlo.reshape"(%135) : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %137 = "stablehlo.broadcast_in_dim"(%136) <{broadcast_dimensions = array<i64: 2>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %138 = "stablehlo.add"(%134, %137) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<8x32x768xbf16>) -> tensor<8x32x768xbf16>
    %139 = "stablehlo.reshape"(%138) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<8x32x12x64xbf16>
    %140 = "stablehlo.transpose"(%139) <{permutation = array<i64: 0, 2, 1, 3>}> {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, xla_shape = "bf16[8,12,32,64]{3,1,2,0}"} : (tensor<8x32x12x64xbf16>) -> tensor<8x12x32x64xbf16>
    %141 = "stablehlo.reshape"(%140) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x12x32x64xbf16>) -> tensor<96x32x64xbf16>
    %142 = "stablehlo.dot_general"(%129, %141) <{dot_dimension_numbers = #stablehlo.dot<lhs_batching_dimensions = [0], rhs_batching_dimensions = [0], lhs_contracting_dimensions = [2], rhs_contracting_dimensions = [1]>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<96x32x32xbf16>, tensor<96x32x64xbf16>) -> tensor<96x32x64xbf16>
    %143 = "stablehlo.reshape"(%142) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>} : (tensor<96x32x64xbf16>) -> tensor<8x12x32x64xbf16>
    %144 = "stablehlo.transpose"(%143) <{permutation = array<i64: 0, 2, 1, 3>}> {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}, {?}]>]>, xla_shape = "bf16[8,32,12,64]{3,1,2,0}"} : (tensor<8x12x32x64xbf16>) -> tensor<8x32x12x64xbf16>
    %145 = "stablehlo.reshape"(%144) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x12x64xbf16>) -> tensor<256x768xbf16>
    %146 = "stablehlo.reshape"(%arg11) : (tensor<768x768xbf16>) -> tensor<1x768x768xbf16>
    %147 = "stablehlo.reshape"(%146) : (tensor<1x768x768xbf16>) -> tensor<768x768xbf16>
    %148 = "stablehlo.transpose"(%147) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,768]{0,1}"} : (tensor<768x768xbf16>) -> tensor<768x768xbf16>
    %149 = "stablehlo.dot_general"(%145, %148) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x768xbf16>) -> tensor<256x768xbf16>
    %150 = "stablehlo.reshape"(%149) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %151 = "stablehlo.reshape"(%arg10) : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %152 = "stablehlo.reshape"(%151) : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %153 = "stablehlo.broadcast_in_dim"(%152) <{broadcast_dimensions = array<i64: 2>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %154 = "stablehlo.add"(%150, %153) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<8x32x768xbf16>) -> tensor<8x32x768xbf16>
    %155 = "stablehlo.add"(%50, %154) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<8x32x768xbf16>) -> tensor<8x32x768xbf16>
    %156 = "stablehlo.reshape"(%155) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %157 = "stablehlo.reduce"(%156, %16) <{dimensions = array<i64: 1>}> ({
    ^bb0(%arg33: tensor<bf16>, %arg34: tensor<bf16>):
      %248 = "stablehlo.add"(%arg33, %arg34) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
      "stablehlo.return"(%248) : (tensor<bf16>) -> ()
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %158 = "stablehlo.multiply"(%157, %19) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<256xbf16>, tensor<256xbf16>) -> tensor<256xbf16>
    %159 = "stablehlo.broadcast_in_dim"(%158) <{broadcast_dimensions = array<i64: 0>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %160 = "stablehlo.subtract"(%156, %159) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<256x768xbf16>) -> tensor<256x768xbf16>
    %161 = "stablehlo.multiply"(%160, %160) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<256x768xbf16>) -> tensor<256x768xbf16>
    %162 = "stablehlo.reduce"(%161, %16) <{dimensions = array<i64: 1>}> ({
    ^bb0(%arg31: tensor<bf16>, %arg32: tensor<bf16>):
      %247 = "stablehlo.add"(%arg31, %arg32) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
      "stablehlo.return"(%247) : (tensor<bf16>) -> ()
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<256x768xbf16>, tensor<bf16>) -> tensor<256xbf16>
    %163 = "stablehlo.multiply"(%162, %19) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<256xbf16>, tensor<256xbf16>) -> tensor<256xbf16>
    %164 = "stablehlo.reshape"(%163) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256xbf16>) -> tensor<256x1xbf16>
    %165 = "stablehlo.add"(%164, %18) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x1xbf16>, tensor<256x1xbf16>) -> tensor<256x1xbf16>
    %166 = "stablehlo.rsqrt"(%165) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x1xbf16>) -> tensor<256x1xbf16>
    %167 = "stablehlo.reshape"(%166) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<256x1xbf16>) -> tensor<256xbf16>
    %168 = "stablehlo.broadcast_in_dim"(%167) <{broadcast_dimensions = array<i64: 0>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256xbf16>) -> tensor<256x768xbf16>
    %169 = "stablehlo.multiply"(%160, %168) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<256x768xbf16>) -> tensor<256x768xbf16>
    %170 = "stablehlo.reshape"(%arg9) : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %171 = "stablehlo.reshape"(%170) : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %172 = "stablehlo.broadcast_in_dim"(%171) <{broadcast_dimensions = array<i64: 1>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %173 = "stablehlo.multiply"(%169, %172) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<256x768xbf16>) -> tensor<256x768xbf16>
    %174 = "stablehlo.reshape"(%arg8) : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %175 = "stablehlo.reshape"(%174) : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %176 = "stablehlo.broadcast_in_dim"(%175) <{broadcast_dimensions = array<i64: 1>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %177 = "stablehlo.add"(%173, %176) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<256x768xbf16>) -> tensor<256x768xbf16>
    %178 = "stablehlo.reshape"(%arg7) : (tensor<3072x768xbf16>) -> tensor<1x3072x768xbf16>
    %179 = "stablehlo.reshape"(%178) : (tensor<1x3072x768xbf16>) -> tensor<3072x768xbf16>
    %180 = "stablehlo.transpose"(%179) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,3072]{0,1}"} : (tensor<3072x768xbf16>) -> tensor<768x3072xbf16>
    %181 = "stablehlo.dot_general"(%177, %180) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x3072xbf16>) -> tensor<256x3072xbf16>
    %182 = "stablehlo.reshape"(%arg6) : (tensor<3072xbf16>) -> tensor<1x1x3072xbf16>
    %183 = "stablehlo.reshape"(%182) : (tensor<1x1x3072xbf16>) -> tensor<3072xbf16>
    %184 = "stablehlo.broadcast_in_dim"(%183) <{broadcast_dimensions = array<i64: 1>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<3072xbf16>) -> tensor<256x3072xbf16>
    %185 = "stablehlo.add"(%181, %184) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x3072xbf16>, tensor<256x3072xbf16>) -> tensor<256x3072xbf16>
    %186 = "stablehlo.maximum"(%185, %17) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x3072xbf16>, tensor<256x3072xbf16>) -> tensor<256x3072xbf16>
    %187 = "stablehlo.reshape"(%arg5) : (tensor<768x3072xbf16>) -> tensor<1x768x3072xbf16>
    %188 = "stablehlo.reshape"(%187) : (tensor<1x768x3072xbf16>) -> tensor<768x3072xbf16>
    %189 = "stablehlo.transpose"(%188) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[3072,768]{0,1}"} : (tensor<768x3072xbf16>) -> tensor<3072x768xbf16>
    %190 = "stablehlo.dot_general"(%186, %189) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x3072xbf16>, tensor<3072x768xbf16>) -> tensor<256x768xbf16>
    %191 = "stablehlo.reshape"(%arg4) : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %192 = "stablehlo.reshape"(%191) : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %193 = "stablehlo.broadcast_in_dim"(%192) <{broadcast_dimensions = array<i64: 1>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<256x768xbf16>
    %194 = "stablehlo.add"(%190, %193) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<256x768xbf16>) -> tensor<256x768xbf16>
    %195 = "stablehlo.add"(%156, %194) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<256x768xbf16>) -> tensor<256x768xbf16>
    %196 = "stablehlo.reshape"(%195) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x768xbf16>) -> tensor<8x32x768xbf16>
    %197 = "stablehlo.reduce"(%196, %16) <{dimensions = array<i64: 2>}> ({
    ^bb0(%arg29: tensor<bf16>, %arg30: tensor<bf16>):
      %246 = "stablehlo.add"(%arg29, %arg30) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
      "stablehlo.return"(%246) : (tensor<bf16>) -> ()
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %198 = "stablehlo.multiply"(%197, %27) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32xbf16>, tensor<8x32xbf16>) -> tensor<8x32xbf16>
    %199 = "stablehlo.broadcast_in_dim"(%198) <{broadcast_dimensions = array<i64: 0, 1>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %200 = "stablehlo.subtract"(%196, %199) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<8x32x768xbf16>) -> tensor<8x32x768xbf16>
    %201 = "stablehlo.multiply"(%200, %200) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<8x32x768xbf16>) -> tensor<8x32x768xbf16>
    %202 = "stablehlo.reduce"(%201, %16) <{dimensions = array<i64: 2>}> ({
    ^bb0(%arg27: tensor<bf16>, %arg28: tensor<bf16>):
      %245 = "stablehlo.add"(%arg27, %arg28) : (tensor<bf16>, tensor<bf16>) -> tensor<bf16>
      "stablehlo.return"(%245) : (tensor<bf16>) -> ()
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<bf16>) -> tensor<8x32xbf16>
    %203 = "stablehlo.multiply"(%202, %27) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32xbf16>, tensor<8x32xbf16>) -> tensor<8x32xbf16>
    %204 = "stablehlo.reshape"(%203) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x1xbf16>
    %205 = "stablehlo.add"(%204, %26) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32x1xbf16>, tensor<8x32x1xbf16>) -> tensor<8x32x1xbf16>
    %206 = "stablehlo.rsqrt"(%205) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32x1xbf16>) -> tensor<8x32x1xbf16>
    %207 = "stablehlo.reshape"(%206) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x1xbf16>) -> tensor<8x32xbf16>
    %208 = "stablehlo.broadcast_in_dim"(%207) <{broadcast_dimensions = array<i64: 0, 1>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32xbf16>) -> tensor<8x32x768xbf16>
    %209 = "stablehlo.multiply"(%200, %208) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<8x32x768xbf16>) -> tensor<8x32x768xbf16>
    %210 = "stablehlo.reshape"(%arg3) : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %211 = "stablehlo.reshape"(%210) : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %212 = "stablehlo.broadcast_in_dim"(%211) <{broadcast_dimensions = array<i64: 2>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %213 = "stablehlo.multiply"(%209, %212) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<8x32x768xbf16>) -> tensor<8x32x768xbf16>
    %214 = "stablehlo.reshape"(%arg2) : (tensor<768xbf16>) -> tensor<1x1x768xbf16>
    %215 = "stablehlo.reshape"(%214) : (tensor<1x1x768xbf16>) -> tensor<768xbf16>
    %216 = "stablehlo.broadcast_in_dim"(%215) <{broadcast_dimensions = array<i64: 2>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<768xbf16>) -> tensor<8x32x768xbf16>
    %217 = "stablehlo.add"(%213, %216) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<8x32x768xbf16>, tensor<8x32x768xbf16>) -> tensor<8x32x768xbf16>
    %218 = "stablehlo.reshape"(%217) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x768xbf16>) -> tensor<256x768xbf16>
    %219 = "stablehlo.reshape"(%arg1) : (tensor<2x768xbf16>) -> tensor<1x2x768xbf16>
    %220 = "stablehlo.reshape"(%219) : (tensor<1x2x768xbf16>) -> tensor<2x768xbf16>
    %221 = "stablehlo.transpose"(%220) <{permutation = array<i64: 1, 0>}> {result_layout = dense<[0, 1]> : tensor<2xindex>, xla_shape = "bf16[768,2]{0,1}"} : (tensor<2x768xbf16>) -> tensor<768x2xbf16>
    %222 = "stablehlo.dot_general"(%218, %221) <{dot_dimension_numbers = #stablehlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<256x768xbf16>, tensor<768x2xbf16>) -> tensor<256x2xbf16>
    %223 = "stablehlo.reshape"(%222) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}, {?}]>]>} : (tensor<256x2xbf16>) -> tensor<8x32x2xbf16>
    %224 = "stablehlo.broadcast_in_dim"(%5) <{broadcast_dimensions = array<i64: 1>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<32xi32>) -> tensor<8x32xi32>
    %225 = "stablehlo.compare"(%33, %29) <{comparison_direction = #stablehlo<comparison_direction NE>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi64>, tensor<8x32xi64>) -> tensor<8x32xi1>
    %226 = "stablehlo.convert"(%225) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi1>) -> tensor<8x32xi32>
    %227 = "stablehlo.multiply"(%224, %226) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32xi32>, tensor<8x32xi32>) -> tensor<8x32xi32>
    %228 = "stablehlo.iota"() <{iota_dimension = 0 : i64}> : () -> tensor<32xi32>
    %229 = "stablehlo.broadcast_in_dim"(%228) <{broadcast_dimensions = array<i64: 1>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<32xi32>) -> tensor<8x32xi32>
    %230:2 = "stablehlo.reduce"(%227, %229, %6, %1) <{dimensions = array<i64: 1>}> ({
    ^bb0(%arg23: tensor<i32>, %arg24: tensor<i32>, %arg25: tensor<i32>, %arg26: tensor<i32>):
      %239 = "stablehlo.compare"(%arg23, %arg25) <{comparison_direction = #stablehlo<comparison_direction GE>}> : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %240 = "stablehlo.select"(%239, %arg23, %arg25) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
      %241 = "stablehlo.compare"(%arg23, %arg25) <{comparison_direction = #stablehlo<comparison_direction EQ>}> : (tensor<i32>, tensor<i32>) -> tensor<i1>
      %242 = "stablehlo.minimum"(%arg24, %arg26) : (tensor<i32>, tensor<i32>) -> tensor<i32>
      %243 = "stablehlo.select"(%239, %arg24, %arg26) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
      %244 = "stablehlo.select"(%241, %242, %243) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
      "stablehlo.return"(%240, %244) : (tensor<i32>, tensor<i32>) -> ()
    }) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>, <@mesh, [{"_axis_0", ?}]>]>} : (tensor<8x32xi32>, tensor<8x32xi32>, tensor<i32>, tensor<i32>) -> (tensor<8xi32>, tensor<8xi32>)
    %231 = "stablehlo.convert"(%230#1) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<8xi32>) -> tensor<8xi64>
    %232 = "stablehlo.compare"(%231, %14) <{comparison_direction = #stablehlo<comparison_direction LT>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<8xi64>, tensor<8xi64>) -> tensor<8xi1>
    %233 = "stablehlo.add"(%231, %13) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<8xi64>, tensor<8xi64>) -> tensor<8xi64>
    %234 = "stablehlo.select"(%232, %233, %231) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}]>]>} : (tensor<8xi1>, tensor<8xi64>, tensor<8xi64>) -> tensor<8xi64>
    %235 = "stablehlo.reshape"(%234) {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8xi64>) -> tensor<8x1xi64>
    %236 = "stablehlo.concatenate"(%15, %235) <{dimension = 1 : i64}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x1xi64>, tensor<8x1xi64>) -> tensor<8x2xi64>
    %237 = "stablehlo.gather"(%223, %236) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, slice_sizes = array<i64: 1, 1, 2>}> {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"_axis_0", ?}, {?}]>]>} : (tensor<8x32x2xbf16>, tensor<8x2xi64>) -> tensor<8x2xbf16>
    %238 = "sdy.all_reduce"(%237) <{out_sharding = #sdy.sharding<@mesh, [{"_axis_0", ?}, {?}]>, reduction_axes = #sdy<axis_ref_list{"_axis_0"}>}> : (tensor<8x2xbf16>) -> tensor<8x2xbf16>
    "func.return"(%238) : (tensor<8x2xbf16>) -> ()
  }) : () -> ()
}) {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} : () -> ()


2025-10-28 19:04:57.963 (  17.587s) [        2AFB8480]      module_builder.cc:681    ERR| Failed to run stablehlo pipeline
2025-10-28 19:04:57.963 (  17.587s) [        2AFB8480]      error_instance.cc:52       1| ErrorInstance::PJRT_Error_Message
2025-10-28 19:04:57.963 (  17.587s) [        2AFB8480]      error_instance.cc:61       1| ErrorInstance::PJRT_Error_GetCode
2025-10-28 19:04:57.963 (  17.587s) [        2AFB8480]      error_instance.cc:46       1| ErrorInstance::PJRT_Error_Destroy
2025-10-28 19:04:58.059 (  17.683s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:58.059 (  17.683s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:58.059 (  17.683s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:58.059 (  17.683s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:58.059 (  17.683s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:58.059 (  17.683s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:58.059 (  17.683s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:58.059 (  17.683s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:58.059 (  17.683s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:58.063 (  17.687s) [        2AFB8480]     client_instance.cc:616      1| ClientInstance::PJRT_Client_Compile
2025-10-28 19:04:58.063 (  17.687s) [        2AFB8480]      module_builder.cc:220      1| ModuleBuilder::buildModule
2025-10-28 19:04:58.063 (  17.687s) [        2AFB8480]      module_builder.cc:963      1| MLIR Module vhlo:
#loc1 = loc("p0.1")
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<8x32x!vhlo.i64_v1> loc("p0.1")) -> (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) {
    %0 = "vhlo.custom_call_v1"(%arg0) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"xla.sdy.FuncResultSharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<true>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>">}>} : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1> loc(#loc1)
    "vhlo.return_v1"(%0) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>]>, res_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, sym_visibility = #vhlo.string_v1<"">} loc(#loc)
} loc(#loc)
#loc = loc(unknown)
------------------ END OF MLIR MODULE ------------------
// -----// IR Dump Before VhloToVersionPass (vhlo-to-version) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) {
    %0 = "vhlo.custom_call_v1"(%arg0) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"xla.sdy.FuncResultSharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<true>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>">}>} : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1>
    "vhlo.return_v1"(%0) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>]>, res_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump Before VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) {
    %0 = "vhlo.custom_call_v1"(%arg0) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"xla.sdy.FuncResultSharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<true>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>">}>} : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1>
    "vhlo.return_v1"(%0) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>]>, res_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump After VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}"}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


2025-10-28 19:04:58.065 (  17.689s) [        2AFB8480]      module_builder.cc:963      1| MLIR Module shlo:
#loc1 = loc("p0.1")
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"} loc("p0.1")) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}"}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64> loc(#loc1)
    return %0 : tensor<8x32xi64> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
------------------ END OF MLIR MODULE ------------------
2025-10-28 19:04:58.065 (  17.689s) [        2AFB8480]      module_builder.cc:963      1| MLIR Module shlo_frontend:
#loc1 = loc("p0.1")
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>} loc("p0.1")) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}"}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64> loc(#loc1)
    return %0 : tensor<8x32xi64> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
------------------ END OF MLIR MODULE ------------------
// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}"}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}"}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before TTPopulateArgumentTypes (tt-populate-argument-types) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}"}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}"}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump After ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before ConvertXlaSdyToSdyPass (convert-xla-sdy-to-sdy) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump After ConvertXlaSdyToSdyPass (convert-xla-sdy-to-sdy) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before AnalyzeMeshPass (analyze-mesh) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before ApplyShardingConstraintsPass (sdy-apply-sharding-constraints) ('func.func' operation: @main) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before ShardingConstraintToReshardPass (sdy-sharding-constraint-to-reshard) ('func.func' operation: @main) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before InsertExplicitReshardsPass (insert-explicit-reshards) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before WrapUnderManualComputationPass (wrap-under-manual-computation) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before ReshardToCollectivesPass (sdy-reshard-to-collectives) ('func.func' operation: @main) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before UpdateGlobalToLocalShapesPass (update-global-to-local-shapes) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before CloseShardingsPass (sdy-close-shardings) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


2025-10-28 19:04:58.071 (  17.695s) [        2AFB8480]      module_builder.cc:963      1| MLIR Module shlo_compiler:
#loc1 = loc("p0.1")
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]> loc(#loc)
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p0.1")) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64> loc(#loc1)
    return %0 : tensor<8x32xi64> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
------------------ END OF MLIR MODULE ------------------
// -----// IR Dump Before ConvertArithToStableHLO (convert-arith-to-stablehlo) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before LegalizeStableHLOCompositeToTTIR (legalize-stablehlo-composite-to-ttir) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before StablehloLegalizeCompositeToCallPass (stablehlo-legalize-composite-to-call) ('func.func' operation: @main) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before ConvertStableHLOToTTIR (convert-stablehlo-to-ttir) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


error: 'func.func' op arg 0 - unknown mesh: @mesh
// -----// IR Dump After ConvertStableHLOToTTIR Failed (convert-stablehlo-to-ttir) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
"builtin.module"() <{sym_name = "ReplicateShardedData.6"}> ({
  "func.func"() <{arg_attrs = [{sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}], function_type = (tensor<8x32xi64>) -> tensor<8x32xi64>, res_attrs = [{mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}], sym_name = "main"}> ({
  ^bb0(%arg0: tensor<8x32xi64>):
    %0 = "stablehlo.custom_call"(%arg0) <{call_target_name = "xla.sdy.FuncResultSharding", has_side_effect = true}> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    "func.return"(%0) : (tensor<8x32xi64>) -> ()
  }) : () -> ()
}) {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} : () -> ()


2025-10-28 19:04:58.074 (  17.698s) [        2AFB8480]      module_builder.cc:712    ERR| Failed to convert from SHLO to TTIR module
2025-10-28 19:04:58.074 (  17.698s) [        2AFB8480]      error_instance.cc:52       1| ErrorInstance::PJRT_Error_Message
2025-10-28 19:04:58.074 (  17.698s) [        2AFB8480]      error_instance.cc:61       1| ErrorInstance::PJRT_Error_GetCode
2025-10-28 19:04:58.074 (  17.698s) [        2AFB8480]      error_instance.cc:46       1| ErrorInstance::PJRT_Error_Destroy
2025-10-28 19:04:58.075 (  17.699s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:58.075 (  17.699s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:58.075 (  17.699s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:58.075 (  17.699s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:58.075 (  17.699s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:58.075 (  17.699s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:58.075 (  17.699s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:58.075 (  17.699s) [        2AFB8480]     buffer_instance.cc:484      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-10-28 19:04:58.075 (  17.699s) [        2AFB8480]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-10-28 19:04:58.079 (  17.703s) [        2AFB8480]     client_instance.cc:616      1| ClientInstance::PJRT_Client_Compile
2025-10-28 19:04:58.079 (  17.703s) [        2AFB8480]      module_builder.cc:220      1| ModuleBuilder::buildModule
2025-10-28 19:04:58.079 (  17.703s) [        2AFB8480]      module_builder.cc:963      1| MLIR Module vhlo:
#loc1 = loc("p0.1")
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<8x32x!vhlo.i64_v1> loc("p0.1")) -> (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) {
    %0 = "vhlo.custom_call_v1"(%arg0) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"xla.sdy.FuncResultSharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<true>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>">}>} : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1> loc(#loc1)
    "vhlo.return_v1"(%0) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> () loc(#loc)
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>]>, res_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, sym_visibility = #vhlo.string_v1<"">} loc(#loc)
} loc(#loc)
#loc = loc(unknown)
------------------ END OF MLIR MODULE ------------------
// -----// IR Dump Before VhloToVersionPass (vhlo-to-version) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) {
    %0 = "vhlo.custom_call_v1"(%arg0) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"xla.sdy.FuncResultSharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<true>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>">}>} : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1>
    "vhlo.return_v1"(%0) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>]>, res_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump Before VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) {
    %0 = "vhlo.custom_call_v1"(%arg0) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"xla.sdy.FuncResultSharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<true>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>">}>} : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> !vhlo.tensor_v1<8x32x!vhlo.i64_v1>
    "vhlo.return_v1"(%0) : (!vhlo.tensor_v1<8x32x!vhlo.i64_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[8,1]<=[8]}">}>]>, res_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, sym_visibility = #vhlo.string_v1<"">}
}


// -----// IR Dump After VhloLegalizeToStablehloPass (vhlo-legalize-to-stablehlo) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}"}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


2025-10-28 19:04:58.080 (  17.704s) [        2AFB8480]      module_builder.cc:963      1| MLIR Module shlo:
#loc1 = loc("p0.1")
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}"} loc("p0.1")) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}"}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64> loc(#loc1)
    return %0 : tensor<8x32xi64> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
------------------ END OF MLIR MODULE ------------------
2025-10-28 19:04:58.081 (  17.705s) [        2AFB8480]      module_builder.cc:963      1| MLIR Module shlo_frontend:
#loc1 = loc("p0.1")
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>} loc("p0.1")) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}"}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64> loc(#loc1)
    return %0 : tensor<8x32xi64> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
------------------ END OF MLIR MODULE ------------------
// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}"}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}"}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before TTPopulateArgumentTypes (tt-populate-argument-types) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}"}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}"}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump After ApplyArgumentShardStatusPass (apply-argument-shard-status) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before ConvertXlaSdyToSdyPass (convert-xla-sdy-to-sdy) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=8]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<8x32xi64> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>"}, mhlo.sharding = "{devices=[8,1]<=[8]}", ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump After ConvertXlaSdyToSdyPass (convert-xla-sdy-to-sdy) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before AnalyzeMeshPass (analyze-mesh) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before ApplyShardingConstraintsPass (sdy-apply-sharding-constraints) ('func.func' operation: @main) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before AggressivePropagationPass (sdy-aggressive-propagate) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before ShardingConstraintToReshardPass (sdy-sharding-constraint-to-reshard) ('func.func' operation: @main) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before InsertExplicitReshardsPass (insert-explicit-reshards) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before WrapUnderManualComputationPass (wrap-under-manual-computation) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before ReshardToCollectivesPass (sdy-reshard-to-collectives) ('func.func' operation: @main) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before UpdateGlobalToLocalShapesPass (update-global-to-local-shapes) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before CloseShardingsPass (sdy-close-shardings) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


2025-10-28 19:04:58.086 (  17.710s) [        2AFB8480]      module_builder.cc:963      1| MLIR Module shlo_compiler:
#loc1 = loc("p0.1")
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]> loc(#loc)
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>} loc("p0.1")) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64> loc(#loc1)
    return %0 : tensor<8x32xi64> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc = loc(unknown)
------------------ END OF MLIR MODULE ------------------
// -----// IR Dump Before ConvertArithToStableHLO (convert-arith-to-stablehlo) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before LegalizeStableHLOCompositeToTTIR (legalize-stablehlo-composite-to-ttir) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before StablehloLegalizeCompositeToCallPass (stablehlo-legalize-composite-to-call) ('func.func' operation: @main) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before Inliner (inline) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('func.func' operation: @main) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


// -----// IR Dump Before ConvertStableHLOToTTIR (convert-stablehlo-to-ttir) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
module @ReplicateShardedData.6 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=8]>
  func.func @main(%arg0: tensor<8x32xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<8x32xi64> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) {
    %0 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    return %0 : tensor<8x32xi64>
  }
}


error: 'func.func' op arg 0 - unknown mesh: @mesh
// -----// IR Dump After ConvertStableHLOToTTIR Failed (convert-stablehlo-to-ttir) ('builtin.module' operation: @ReplicateShardedData.6) //----- //
"builtin.module"() <{sym_name = "ReplicateShardedData.6"}> ({
  "func.func"() <{arg_attrs = [{sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>, ttcore.argument_type = #ttcore.argument_type<input>, ttcore.shard_status = #ttcore.shard_status<presharded>}], function_type = (tensor<8x32xi64>) -> tensor<8x32xi64>, res_attrs = [{mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}], sym_name = "main"}> ({
  ^bb0(%arg0: tensor<8x32xi64>):
    %0 = "stablehlo.custom_call"(%arg0) <{call_target_name = "xla.sdy.FuncResultSharding", has_side_effect = true}> {mhlo.frontend_attributes = {xla.sdy.sharding = "#sdy.sharding_per_value<[<@mesh, [{}, {}]>]>"}} : (tensor<8x32xi64>) -> tensor<8x32xi64>
    "func.return"(%0) : (tensor<8x32xi64>) -> ()
  }) : () -> ()
}) {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x8>]>} : () -> ()


2025-10-28 19:04:58.089 (  17.713s) [        2AFB8480]      module_builder.cc:712    ERR| Failed to convert from SHLO to TTIR module
2025-10-28 19:04:58.089 (  17.713s) [        2AFB8480]      error_instance.cc:52       1| ErrorInstance::PJRT_Error_Message
2025-10-28 19:04:58.089 (  17.713s) [        2AFB8480]      error_instance.cc:61       1| ErrorInstance::PJRT_Error_GetCode
2025-10-28 19:04:58.089 (  17.713s) [        2AFB8480]      error_instance.cc:46       1| ErrorInstance::PJRT_Error_Destroy
Running tests/runner/test_models.py::test_all_models[opt/sequence_classification/pytorch-facebook/opt-125m-data_parallel-full-inference] - pytorch_opt_facebook/opt-125m_nlp_text_cls_huggingface
Created device mesh: (1, 8) with 8 devices.
FAILED

=================================== FAILURES ===================================
_ test_all_models[opt/sequence_classification/pytorch-facebook/opt-125m-data_parallel-full-inference] _

test_entry = ModelTestEntry(path='/localdev/hshah/tt-xla/third_party/tt_forge_models/opt/sequence_classification/pytorch/loader.py'...iant.OPT_125M: 'facebook/opt-125m'>, <class 'tt-forge-models.opt.sequence_classification.pytorch.loader.ModelLoader'>))
run_mode = <RunMode.INFERENCE: 'inference'>, op_by_op = None
parallelism = <Parallelism.DATA_PARALLEL: 'data_parallel'>
record_property = <function record_property.<locals>.append_property at 0x7faabee04540>
test_metadata = <tests.runner.test_utils.ModelTestConfig object at 0x7faabfb29b10>
request = <FixtureRequest for <Function test_all_models[opt/sequence_classification/pytorch-facebook/opt-125m-data_parallel-full-inference]>>
capteesys = <_pytest.capture.CaptureFixture object at 0x7faabedf6410>

    @pytest.mark.model_test
    @pytest.mark.no_auto_properties
    @pytest.mark.parametrize(
        "run_mode",
        [
            pytest.param(RunMode.INFERENCE, id="inference", marks=pytest.mark.inference),
            pytest.param(RunMode.TRAINING, id="training", marks=pytest.mark.training),
        ],
    )
    @pytest.mark.parametrize(
        "op_by_op",
        [None],
        ids=["full"],  # When op-by-op flow is required/supported, add here.
    )
    @pytest.mark.parametrize(
        "parallelism",
        [
            pytest.param(
                Parallelism.SINGLE_DEVICE,
                id="single_device",
                marks=pytest.mark.single_device,
            ),
            pytest.param(
                Parallelism.DATA_PARALLEL,
                id="data_parallel",
                marks=pytest.mark.data_parallel,
            ),
            pytest.param(
                Parallelism.TENSOR_PARALLEL,
                id="tensor_parallel",
                marks=pytest.mark.tensor_parallel,
            ),
        ],
    )
    @pytest.mark.parametrize(
        "test_entry",
        test_entries,
        ids=create_test_id_generator(MODELS_ROOT),
    )
    def test_all_models(
        test_entry,
        run_mode,
        op_by_op,
        parallelism,
        record_property,
        test_metadata,
        request,
        capteesys,
    ):
    
        loader_path = test_entry.path
        variant, ModelLoader = test_entry.variant_info
    
        # Ensure per-model requirements are installed, and roll back after the test
        with RequirementsManager.for_loader(loader_path):
    
            # Get the model loader and model info from desired model, variant.
            loader = ModelLoader(variant=variant)
            model_info = ModelLoader.get_model_info(variant=variant)
            print(f"Running {request.node.nodeid} - {model_info.name}", flush=True)
    
            succeeded = False
            comparison_result = None
            tester = None
    
            try:
                # Only run the actual model test if not marked for skip. The record properties
                # function in finally block will always be called and handles the pytest.skip.
                if test_metadata.status != ModelTestStatus.NOT_SUPPORTED_SKIP:
                    tester = DynamicTorchModelTester(
                        run_mode,
                        loader=loader,
                        comparison_config=test_metadata.to_comparison_config(),
                        parallelism=parallelism,
                    )
    
>                   comparison_result = tester.test()
                                        ^^^^^^^^^^^^^

tests/runner/test_models.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/infra/testers/single_chip/model/model_tester.py:145: in test
    return self._test_inference()
           ^^^^^^^^^^^^^^^^^^^^^^
tests/infra/testers/single_chip/model/model_tester.py:158: in _test_inference
    tt_res = self._run_on_tt_device(self._workload)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/infra/testers/single_chip/model/model_tester.py:168: in _run_on_tt_device
    return self._device_runner.run_on_tt_device(compiled_workload)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/infra/runners/device_runner.py:30: in run_on_tt_device
    return self.run_on_device(workload, DeviceType.TT, device_num)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/infra/runners/device_runner.py:40: in run_on_device
    return self._run_on_device(device_workload, device)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/infra/runners/torch_device_runner.py:79: in _run_on_device
    return workload.execute()
           ^^^^^^^^^^^^^^^^^^
tests/infra/workloads/workload.py:68: in execute
    return self.model(*self.args, **self.kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1749: in _wrapped_call_impl
    return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:655: in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762: in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/local/lib/python3.11/dist-packages/transformers/models/opt/modeling_opt.py:947: in forward
    @auto_docstring
/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:838: in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tt_torch.backend.backend.XLAExecutor object at 0x7faabe442910>
args = (<[RuntimeError('Error code: 13') raised in repr()] Tensor object at 0x7faa7c1f1070>, <[RuntimeError('Error code: 13') raised in repr()] Tensor object at 0x7faa7c1f1130>)
output = (<[RuntimeError('Check failed: data()->tensor_data: ') raised in repr()] Tensor object at 0x7faa6c703ef0>,)
gm_has_functional_output_kind = True
el = OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='index'), target=None)

    def __call__(self, *args):
    
        if self.inject_metadata:
            # MetadataDispatchMode intercepts tensor operations via TorchDispatchMode and
            # attaches FX metadata (module hierarchy, file, line) to XLA tensors.
            with MetadataDispatchMode(self.node_info):
                output = self.module(*args)
        else:
            output = self.module(*args)
    
        gm_has_functional_output_kind: bool = True
    
        for el in self.signature.output_specs:
            if el.kind is not OutputKind.USER_OUTPUT:
                gm_has_functional_output_kind = False
                break
    
        if gm_has_functional_output_kind:
            # This tells torch-xla to cut the graph at only what is required to
            # compute all tensors in the `output` list.
>           torch_xla._XLAC._xla_sync_multi(list(output), self.devices, wait=False)
E           ValueError: Error code: 13

python_package/tt_torch/backend/backend.py:117: ValueError
=============================== warnings summary ===============================
<frozen importlib._bootstrap>:241
  <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute

<frozen importlib._bootstrap>:241
  <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/runner/test_models.py::test_all_models[opt/sequence_classification/pytorch-facebook/opt-125m-data_parallel-full-inference] - ValueError: Error code: 13
======================== 1 failed, 2 warnings in 21.30s ========================
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
2025-10-28 19:05:02.041 (  21.665s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.041 (  21.665s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.041 (  21.665s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.041 (  21.665s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.041 (  21.665s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.041 (  21.665s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.041 (  21.665s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.041 (  21.665s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.041 (  21.665s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.041 (  21.665s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.041 (  21.665s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.041 (  21.665s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.041 (  21.665s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.041 (  21.665s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.041 (  21.665s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.041 (  21.665s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.898 (  22.522s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.898 (  22.522s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.898 (  22.522s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.898 (  22.522s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.898 (  22.522s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.898 (  22.522s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.898 (  22.522s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.898 (  22.522s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.899 (  22.523s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.905 (  22.529s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.910 (  22.534s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.915 (  22.539s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.919 (  22.543s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.923 (  22.547s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.928 (  22.552s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.933 (  22.557s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.937 (  22.561s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.937 (  22.561s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.937 (  22.561s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.937 (  22.561s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.937 (  22.561s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.937 (  22.561s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.937 (  22.561s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.937 (  22.561s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.937 (  22.561s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.937 (  22.561s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.937 (  22.561s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.937 (  22.561s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.937 (  22.561s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.937 (  22.561s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.937 (  22.561s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.937 (  22.561s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.937 (  22.561s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.937 (  22.561s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.937 (  22.561s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.937 (  22.561s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.937 (  22.561s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.937 (  22.562s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.938 (  22.562s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.938 (  22.562s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.938 (  22.562s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.938 (  22.562s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.938 (  22.562s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.938 (  22.562s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.938 (  22.562s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.938 (  22.562s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.938 (  22.562s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.938 (  22.562s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.938 (  22.562s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.938 (  22.562s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.938 (  22.562s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.938 (  22.562s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.938 (  22.562s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.938 (  22.562s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.938 (  22.562s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.938 (  22.562s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.938 (  22.562s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.938 (  22.562s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.938 (  22.562s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.938 (  22.563s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.939 (  22.563s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.939 (  22.563s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.939 (  22.563s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.939 (  22.563s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.939 (  22.563s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.939 (  22.563s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.939 (  22.563s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.939 (  22.563s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.939 (  22.563s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.939 (  22.563s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.939 (  22.563s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.939 (  22.563s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.939 (  22.563s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.939 (  22.563s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.939 (  22.563s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.939 (  22.563s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.939 (  22.563s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.939 (  22.563s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.939 (  22.563s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.939 (  22.563s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.939 (  22.563s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.939 (  22.564s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.940 (  22.564s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.940 (  22.564s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.940 (  22.564s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.940 (  22.564s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.940 (  22.564s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.940 (  22.564s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.940 (  22.564s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.940 (  22.564s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.940 (  22.564s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.940 (  22.564s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.940 (  22.564s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.940 (  22.564s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.940 (  22.564s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.940 (  22.564s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.940 (  22.564s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.940 (  22.564s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.940 (  22.564s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.940 (  22.564s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.940 (  22.564s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.940 (  22.564s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.940 (  22.564s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.940 (  22.565s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.940 (  22.565s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.941 (  22.565s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.941 (  22.565s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.941 (  22.565s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.941 (  22.565s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.941 (  22.565s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.941 (  22.565s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.941 (  22.565s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.941 (  22.565s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.941 (  22.565s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.941 (  22.565s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.941 (  22.565s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.941 (  22.565s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.941 (  22.565s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.941 (  22.565s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.941 (  22.565s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.941 (  22.565s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.941 (  22.565s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.941 (  22.565s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.941 (  22.566s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.942 (  22.566s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.942 (  22.566s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.942 (  22.566s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.942 (  22.566s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.942 (  22.566s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.942 (  22.566s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.942 (  22.566s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.942 (  22.566s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.942 (  22.566s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.942 (  22.566s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.942 (  22.566s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.942 (  22.566s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.942 (  22.566s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.942 (  22.566s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.942 (  22.566s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.942 (  22.566s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.942 (  22.566s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.942 (  22.566s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.942 (  22.566s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.942 (  22.566s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.942 (  22.567s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.943 (  22.567s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.943 (  22.567s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.943 (  22.567s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.943 (  22.567s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.943 (  22.567s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.943 (  22.567s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.943 (  22.567s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.943 (  22.567s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.943 (  22.567s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.943 (  22.567s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.948 (  22.572s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.948 (  22.572s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.948 (  22.572s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.948 (  22.572s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.948 (  22.572s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.948 (  22.572s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.948 (  22.572s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.948 (  22.572s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.948 (  22.572s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.948 (  22.572s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.948 (  22.572s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.948 (  22.572s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
2025-10-28 19:05:02.948 (  22.573s) [        2AFB8480]     buffer_instance.cc:402      1| BufferInstance::PJRT_Buffer_Destroy
